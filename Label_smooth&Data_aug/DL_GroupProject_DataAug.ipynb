{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfGrai_Qt7Ny"
      },
      "outputs": [],
      "source": [
        "# import all libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "21a8b3095f71462fa150dbda57845c26",
            "cbdc166f546545c18b74e4be74807802",
            "ca5d8d313be841b180832c8adebed4e4",
            "0c94233cf11047299a7804e4e0d73fa7",
            "c10bd99090f84f8988c7fee82f6c81ce",
            "33f6c9c5ea0a485c9c6a9e92e51ab00a",
            "1a9181f148c9430883b16efea94e8c47",
            "78cde31aef4a4816bedf5c3fe0c5e5ae",
            "5bd017a1a8bf44fc997eae7dda56c33e",
            "26d2a7ff54a04f58bfc9c508a12496bd",
            "5afcaa535195476185b666706120cb78"
          ]
        },
        "id": "VgAiImV0uURP",
        "outputId": "039110f8-d05f-4694-ec31-d531256bf5db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/169001437 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "21a8b3095f71462fa150dbda57845c26"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# base setting for cifar100\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5074,0.4867,0.4411),(0.2011,0.1987,0.2025)),\n",
        "    transforms.RandomErasing(value = (0.5074,0.4867,0.4411))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5074,0.4867,0.4411),(0.2011,0.1987,0.2025)),\n",
        "])\n",
        "\n",
        "#download CIFAR100 dataset\n",
        "trainset = torchvision.datasets.CIFAR100(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR100(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "# we can use a larger batch size during test, because we do not save \n",
        "# intermediate variables for gradient computation, which leaves more memory\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n",
        "\n",
        "#classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "#           'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for image,label in trainset:\n",
        "    print(\"Image shape: \",image.shape)\n",
        "    print(\"Image tensor: \", image)\n",
        "    print(\"Label: \", label)\n",
        "    break"
      ],
      "metadata": {
        "id": "r9w0GtgVNgIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(trainset.classes)"
      ],
      "metadata": {
        "id": "gvORam6KNxvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_classes_items = dict()\n",
        "\n",
        "for train_item in trainset:\n",
        "    label = trainset.classes[train_item[1]]\n",
        "    if label not in train_classes_items:\n",
        "        train_classes_items[label] = 1\n",
        "    else:\n",
        "        train_classes_items[label] += 1\n",
        "\n",
        "train_classes_items"
      ],
      "metadata": {
        "id": "B_YV8xb0N-NA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_classes_items = dict()\n",
        "for test_item in testset:\n",
        "    label = testset.classes[test_item[1]]\n",
        "    if label not in test_classes_items:\n",
        "        test_classes_items[label] = 1\n",
        "    else:\n",
        "        test_classes_items[label] += 1\n",
        "\n",
        "test_classes_items"
      ],
      "metadata": {
        "id": "yNET6eDxOKm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iP2YfD1gEUC-"
      },
      "source": [
        "See more examples at: https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9e4QhB5B89H"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hldipDVsv-Jt"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "def train(epoch, net, criterion, trainloader, scheduler):\n",
        "    device = 'cuda'\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        if (batch_idx+1) % 50 == 0:\n",
        "          print(\"iteration : %3d, loss : %0.4f, accuracy : %2.2f\" % (batch_idx+1, train_loss/(batch_idx+1), 100.*correct/total))\n",
        "\n",
        "    scheduler.step()\n",
        "    return train_loss/(batch_idx+1), 100.*correct/total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgyCI0U08i2h"
      },
      "source": [
        "Test performance on the test set. Note the use of `torch.inference_mode()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkooK-hQu4a6"
      },
      "outputs": [],
      "source": [
        "def test(epoch, net, criterion, testloader):\n",
        "    device = 'cuda'\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.inference_mode():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    return test_loss/(batch_idx+1), 100.*correct/total\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEj8J7xqwAxD"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(net, acc, epoch):\n",
        "    # Save checkpoint.\n",
        "    print('Saving..')\n",
        "    state = {\n",
        "        'net': net.state_dict(),\n",
        "        'acc': acc,\n",
        "        'epoch': epoch,\n",
        "    }\n",
        "    if not os.path.isdir('checkpoint'):\n",
        "        os.mkdir('checkpoint')\n",
        "    torch.save(state, './checkpoint/ckpt.pth')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlCAjBEWwXNo"
      },
      "outputs": [],
      "source": [
        "# defining resnet models\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=100):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        # This is the \"stem\"\n",
        "        # For CIFAR (32x32 images), it does not perform downsampling\n",
        "        # It should downsample for ImageNet\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        # four stages with three downsampling\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
        "\n",
        "\n",
        "def test_resnet18():\n",
        "    net = ResNet18()\n",
        "    y = net(torch.randn(1, 3, 32, 32))\n",
        "    print(y.size())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9J1Pp8RUVEF",
        "outputId": "bae77b9f-02f3-4afd-849f-7f2db7076bbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "iteration :  50, loss : 4.5166, accuracy : 3.23\n",
            "iteration : 100, loss : 4.3302, accuracy : 5.02\n",
            "iteration : 150, loss : 4.2161, accuracy : 6.15\n",
            "iteration : 200, loss : 4.1345, accuracy : 7.08\n",
            "iteration : 250, loss : 4.0642, accuracy : 7.88\n",
            "iteration : 300, loss : 4.0134, accuracy : 8.48\n",
            "iteration : 350, loss : 3.9625, accuracy : 9.10\n",
            "Epoch :   1, training loss : 3.9227, training accuracy : 9.63, test loss : 3.5510, test accuracy : 15.37\n",
            "\n",
            "Epoch: 2\n",
            "iteration :  50, loss : 3.4741, accuracy : 16.48\n",
            "iteration : 100, loss : 3.4642, accuracy : 16.72\n",
            "iteration : 150, loss : 3.4350, accuracy : 17.18\n",
            "iteration : 200, loss : 3.3982, accuracy : 17.68\n",
            "iteration : 250, loss : 3.3692, accuracy : 18.09\n",
            "iteration : 300, loss : 3.3480, accuracy : 18.46\n",
            "iteration : 350, loss : 3.3241, accuracy : 18.94\n",
            "Epoch :   2, training loss : 3.3038, training accuracy : 19.36, test loss : 3.0613, test accuracy : 24.56\n",
            "\n",
            "Epoch: 3\n",
            "iteration :  50, loss : 3.0480, accuracy : 23.56\n",
            "iteration : 100, loss : 3.0251, accuracy : 24.25\n",
            "iteration : 150, loss : 3.0025, accuracy : 24.84\n",
            "iteration : 200, loss : 2.9861, accuracy : 25.08\n",
            "iteration : 250, loss : 2.9642, accuracy : 25.49\n",
            "iteration : 300, loss : 2.9482, accuracy : 25.71\n",
            "iteration : 350, loss : 2.9318, accuracy : 26.06\n",
            "Epoch :   3, training loss : 2.9115, training accuracy : 26.46, test loss : 3.0290, test accuracy : 25.50\n",
            "\n",
            "Epoch: 4\n",
            "iteration :  50, loss : 2.6694, accuracy : 31.05\n",
            "iteration : 100, loss : 2.6554, accuracy : 31.33\n",
            "iteration : 150, loss : 2.6350, accuracy : 31.73\n",
            "iteration : 200, loss : 2.6250, accuracy : 32.04\n",
            "iteration : 250, loss : 2.6158, accuracy : 32.14\n",
            "iteration : 300, loss : 2.5932, accuracy : 32.68\n",
            "iteration : 350, loss : 2.5808, accuracy : 32.98\n",
            "Epoch :   4, training loss : 2.5654, training accuracy : 33.28, test loss : 2.5525, test accuracy : 35.01\n",
            "\n",
            "Epoch: 5\n",
            "iteration :  50, loss : 2.3938, accuracy : 36.44\n",
            "iteration : 100, loss : 2.3582, accuracy : 37.43\n",
            "iteration : 150, loss : 2.3534, accuracy : 37.76\n",
            "iteration : 200, loss : 2.3489, accuracy : 37.72\n",
            "iteration : 250, loss : 2.3346, accuracy : 38.02\n",
            "iteration : 300, loss : 2.3289, accuracy : 38.27\n",
            "iteration : 350, loss : 2.3112, accuracy : 38.72\n",
            "Epoch :   5, training loss : 2.2986, training accuracy : 39.05, test loss : 2.3259, test accuracy : 40.00\n",
            "\n",
            "Epoch: 6\n",
            "iteration :  50, loss : 2.1700, accuracy : 41.80\n",
            "iteration : 100, loss : 2.1498, accuracy : 42.02\n",
            "iteration : 150, loss : 2.1394, accuracy : 42.26\n",
            "iteration : 200, loss : 2.1324, accuracy : 42.46\n",
            "iteration : 250, loss : 2.1243, accuracy : 42.60\n",
            "iteration : 300, loss : 2.1160, accuracy : 42.84\n",
            "iteration : 350, loss : 2.1061, accuracy : 43.02\n",
            "Epoch :   6, training loss : 2.0963, training accuracy : 43.29, test loss : 2.0455, test accuracy : 44.99\n",
            "\n",
            "Epoch: 7\n",
            "iteration :  50, loss : 1.9751, accuracy : 46.08\n",
            "iteration : 100, loss : 1.9532, accuracy : 46.62\n",
            "iteration : 150, loss : 1.9584, accuracy : 46.55\n",
            "iteration : 200, loss : 1.9476, accuracy : 46.67\n",
            "iteration : 250, loss : 1.9395, accuracy : 46.90\n",
            "iteration : 300, loss : 1.9343, accuracy : 47.03\n",
            "iteration : 350, loss : 1.9296, accuracy : 47.19\n",
            "Epoch :   7, training loss : 1.9274, training accuracy : 47.25, test loss : 1.8780, test accuracy : 48.63\n",
            "\n",
            "Epoch: 8\n",
            "iteration :  50, loss : 1.7613, accuracy : 51.02\n",
            "iteration : 100, loss : 1.7719, accuracy : 50.92\n",
            "iteration : 150, loss : 1.7922, accuracy : 50.62\n",
            "iteration : 200, loss : 1.7886, accuracy : 50.53\n",
            "iteration : 250, loss : 1.7893, accuracy : 50.54\n",
            "iteration : 300, loss : 1.7915, accuracy : 50.47\n",
            "iteration : 350, loss : 1.7837, accuracy : 50.53\n",
            "Epoch :   8, training loss : 1.7803, training accuracy : 50.60, test loss : 1.7457, test accuracy : 52.25\n",
            "\n",
            "Epoch: 9\n",
            "iteration :  50, loss : 1.6618, accuracy : 53.34\n",
            "iteration : 100, loss : 1.6731, accuracy : 53.09\n",
            "iteration : 150, loss : 1.6698, accuracy : 53.00\n",
            "iteration : 200, loss : 1.6681, accuracy : 53.25\n",
            "iteration : 250, loss : 1.6777, accuracy : 52.95\n",
            "iteration : 300, loss : 1.6629, accuracy : 53.36\n",
            "iteration : 350, loss : 1.6666, accuracy : 53.19\n",
            "Epoch :   9, training loss : 1.6673, training accuracy : 53.16, test loss : 1.7774, test accuracy : 52.32\n",
            "\n",
            "Epoch: 10\n",
            "iteration :  50, loss : 1.5869, accuracy : 55.34\n",
            "iteration : 100, loss : 1.5675, accuracy : 55.88\n",
            "iteration : 150, loss : 1.5575, accuracy : 55.86\n",
            "iteration : 200, loss : 1.5596, accuracy : 55.58\n",
            "iteration : 250, loss : 1.5610, accuracy : 55.62\n",
            "iteration : 300, loss : 1.5601, accuracy : 55.75\n",
            "iteration : 350, loss : 1.5586, accuracy : 55.78\n",
            "Epoch :  10, training loss : 1.5554, training accuracy : 55.90, test loss : 1.9450, test accuracy : 49.13\n",
            "\n",
            "Epoch: 11\n",
            "iteration :  50, loss : 1.4570, accuracy : 57.56\n",
            "iteration : 100, loss : 1.4717, accuracy : 57.77\n",
            "iteration : 150, loss : 1.4697, accuracy : 57.74\n",
            "iteration : 200, loss : 1.4757, accuracy : 57.80\n",
            "iteration : 250, loss : 1.4683, accuracy : 58.04\n",
            "iteration : 300, loss : 1.4635, accuracy : 58.20\n",
            "iteration : 350, loss : 1.4662, accuracy : 58.10\n",
            "Epoch :  11, training loss : 1.4699, training accuracy : 58.08, test loss : 1.5866, test accuracy : 56.89\n",
            "\n",
            "Epoch: 12\n",
            "iteration :  50, loss : 1.3728, accuracy : 60.95\n",
            "iteration : 100, loss : 1.3696, accuracy : 61.14\n",
            "iteration : 150, loss : 1.3727, accuracy : 60.82\n",
            "iteration : 200, loss : 1.3751, accuracy : 60.87\n",
            "iteration : 250, loss : 1.3758, accuracy : 60.84\n",
            "iteration : 300, loss : 1.3836, accuracy : 60.59\n",
            "iteration : 350, loss : 1.3859, accuracy : 60.44\n",
            "Epoch :  12, training loss : 1.3856, training accuracy : 60.51, test loss : 1.6765, test accuracy : 55.84\n",
            "\n",
            "Epoch: 13\n",
            "iteration :  50, loss : 1.3042, accuracy : 62.03\n",
            "iteration : 100, loss : 1.3046, accuracy : 61.83\n",
            "iteration : 150, loss : 1.3138, accuracy : 61.85\n",
            "iteration : 200, loss : 1.3174, accuracy : 61.79\n",
            "iteration : 250, loss : 1.3230, accuracy : 61.56\n",
            "iteration : 300, loss : 1.3239, accuracy : 61.51\n",
            "iteration : 350, loss : 1.3211, accuracy : 61.56\n",
            "Epoch :  13, training loss : 1.3150, training accuracy : 61.85, test loss : 1.6602, test accuracy : 55.86\n",
            "\n",
            "Epoch: 14\n",
            "iteration :  50, loss : 1.2364, accuracy : 63.89\n",
            "iteration : 100, loss : 1.2419, accuracy : 63.52\n",
            "iteration : 150, loss : 1.2392, accuracy : 63.73\n",
            "iteration : 200, loss : 1.2437, accuracy : 63.78\n",
            "iteration : 250, loss : 1.2590, accuracy : 63.43\n",
            "iteration : 300, loss : 1.2588, accuracy : 63.35\n",
            "iteration : 350, loss : 1.2610, accuracy : 63.39\n",
            "Epoch :  14, training loss : 1.2596, training accuracy : 63.40, test loss : 1.5282, test accuracy : 59.41\n",
            "\n",
            "Epoch: 15\n",
            "iteration :  50, loss : 1.1206, accuracy : 66.84\n",
            "iteration : 100, loss : 1.1430, accuracy : 66.44\n",
            "iteration : 150, loss : 1.1685, accuracy : 65.91\n",
            "iteration : 200, loss : 1.1635, accuracy : 65.95\n",
            "iteration : 250, loss : 1.1740, accuracy : 65.71\n",
            "iteration : 300, loss : 1.1817, accuracy : 65.41\n",
            "iteration : 350, loss : 1.1863, accuracy : 65.28\n",
            "Epoch :  15, training loss : 1.1856, training accuracy : 65.30, test loss : 1.5159, test accuracy : 59.60\n",
            "\n",
            "Epoch: 16\n",
            "iteration :  50, loss : 1.1043, accuracy : 66.75\n",
            "iteration : 100, loss : 1.1129, accuracy : 66.80\n",
            "iteration : 150, loss : 1.1097, accuracy : 67.07\n",
            "iteration : 200, loss : 1.1196, accuracy : 66.83\n",
            "iteration : 250, loss : 1.1200, accuracy : 66.90\n",
            "iteration : 300, loss : 1.1218, accuracy : 66.95\n",
            "iteration : 350, loss : 1.1218, accuracy : 67.00\n",
            "Epoch :  16, training loss : 1.1247, training accuracy : 66.92, test loss : 1.4401, test accuracy : 61.01\n",
            "\n",
            "Epoch: 17\n",
            "iteration :  50, loss : 1.0590, accuracy : 68.64\n",
            "iteration : 100, loss : 1.0478, accuracy : 68.84\n",
            "iteration : 150, loss : 1.0543, accuracy : 68.84\n",
            "iteration : 200, loss : 1.0576, accuracy : 68.75\n",
            "iteration : 250, loss : 1.0545, accuracy : 68.70\n",
            "iteration : 300, loss : 1.0629, accuracy : 68.52\n",
            "iteration : 350, loss : 1.0671, accuracy : 68.42\n",
            "Epoch :  17, training loss : 1.0684, training accuracy : 68.45, test loss : 1.4478, test accuracy : 61.46\n",
            "\n",
            "Epoch: 18\n",
            "iteration :  50, loss : 0.9968, accuracy : 69.69\n",
            "iteration : 100, loss : 0.9867, accuracy : 70.02\n",
            "iteration : 150, loss : 0.9992, accuracy : 69.96\n",
            "iteration : 200, loss : 1.0045, accuracy : 69.86\n",
            "iteration : 250, loss : 1.0087, accuracy : 69.87\n",
            "iteration : 300, loss : 1.0081, accuracy : 69.85\n",
            "iteration : 350, loss : 1.0108, accuracy : 69.79\n",
            "Epoch :  18, training loss : 1.0170, training accuracy : 69.68, test loss : 1.4194, test accuracy : 61.61\n",
            "\n",
            "Epoch: 19\n",
            "iteration :  50, loss : 0.9185, accuracy : 72.19\n",
            "iteration : 100, loss : 0.9525, accuracy : 71.55\n",
            "iteration : 150, loss : 0.9554, accuracy : 71.30\n",
            "iteration : 200, loss : 0.9632, accuracy : 71.23\n",
            "iteration : 250, loss : 0.9677, accuracy : 71.08\n",
            "iteration : 300, loss : 0.9729, accuracy : 70.91\n",
            "iteration : 350, loss : 0.9775, accuracy : 70.81\n",
            "Epoch :  19, training loss : 0.9833, training accuracy : 70.69, test loss : 1.4465, test accuracy : 61.77\n",
            "\n",
            "Epoch: 20\n",
            "iteration :  50, loss : 0.9150, accuracy : 72.17\n",
            "iteration : 100, loss : 0.9042, accuracy : 72.54\n",
            "iteration : 150, loss : 0.9106, accuracy : 72.43\n",
            "iteration : 200, loss : 0.9085, accuracy : 72.58\n",
            "iteration : 250, loss : 0.9154, accuracy : 72.38\n",
            "iteration : 300, loss : 0.9241, accuracy : 72.20\n",
            "iteration : 350, loss : 0.9291, accuracy : 71.98\n",
            "Epoch :  20, training loss : 0.9300, training accuracy : 72.01, test loss : 1.3879, test accuracy : 63.37\n",
            "\n",
            "Epoch: 21\n",
            "iteration :  50, loss : 0.8245, accuracy : 74.34\n",
            "iteration : 100, loss : 0.8430, accuracy : 74.14\n",
            "iteration : 150, loss : 0.8687, accuracy : 73.55\n",
            "iteration : 200, loss : 0.8693, accuracy : 73.52\n",
            "iteration : 250, loss : 0.8788, accuracy : 73.29\n",
            "iteration : 300, loss : 0.8793, accuracy : 73.22\n",
            "iteration : 350, loss : 0.8803, accuracy : 73.22\n",
            "Epoch :  21, training loss : 0.8878, training accuracy : 73.00, test loss : 1.3562, test accuracy : 64.55\n",
            "\n",
            "Epoch: 22\n",
            "iteration :  50, loss : 0.7940, accuracy : 75.56\n",
            "iteration : 100, loss : 0.7931, accuracy : 75.81\n",
            "iteration : 150, loss : 0.8133, accuracy : 75.12\n",
            "iteration : 200, loss : 0.8222, accuracy : 74.82\n",
            "iteration : 250, loss : 0.8245, accuracy : 74.77\n",
            "iteration : 300, loss : 0.8293, accuracy : 74.73\n",
            "iteration : 350, loss : 0.8362, accuracy : 74.59\n",
            "Epoch :  22, training loss : 0.8399, training accuracy : 74.48, test loss : 1.3936, test accuracy : 64.32\n",
            "\n",
            "Epoch: 23\n",
            "iteration :  50, loss : 0.7407, accuracy : 76.73\n",
            "iteration : 100, loss : 0.7563, accuracy : 76.50\n",
            "iteration : 150, loss : 0.7690, accuracy : 76.04\n",
            "iteration : 200, loss : 0.7851, accuracy : 75.62\n",
            "iteration : 250, loss : 0.7928, accuracy : 75.51\n",
            "iteration : 300, loss : 0.7970, accuracy : 75.45\n",
            "iteration : 350, loss : 0.8058, accuracy : 75.22\n",
            "Epoch :  23, training loss : 0.8108, training accuracy : 75.18, test loss : 1.2863, test accuracy : 66.32\n",
            "\n",
            "Epoch: 24\n",
            "iteration :  50, loss : 0.7554, accuracy : 76.89\n",
            "iteration : 100, loss : 0.7543, accuracy : 77.18\n",
            "iteration : 150, loss : 0.7475, accuracy : 77.27\n",
            "iteration : 200, loss : 0.7575, accuracy : 76.89\n",
            "iteration : 250, loss : 0.7618, accuracy : 76.77\n",
            "iteration : 300, loss : 0.7662, accuracy : 76.57\n",
            "iteration : 350, loss : 0.7715, accuracy : 76.42\n",
            "Epoch :  24, training loss : 0.7776, training accuracy : 76.22, test loss : 1.4271, test accuracy : 64.01\n",
            "\n",
            "Epoch: 25\n",
            "iteration :  50, loss : 0.7149, accuracy : 78.03\n",
            "iteration : 100, loss : 0.7127, accuracy : 78.11\n",
            "iteration : 150, loss : 0.7138, accuracy : 77.94\n",
            "iteration : 200, loss : 0.7173, accuracy : 77.89\n",
            "iteration : 250, loss : 0.7169, accuracy : 77.87\n",
            "iteration : 300, loss : 0.7216, accuracy : 77.81\n",
            "iteration : 350, loss : 0.7245, accuracy : 77.63\n",
            "Epoch :  25, training loss : 0.7330, training accuracy : 77.39, test loss : 1.3932, test accuracy : 65.24\n",
            "\n",
            "Epoch: 26\n",
            "iteration :  50, loss : 0.6554, accuracy : 79.50\n",
            "iteration : 100, loss : 0.6676, accuracy : 79.55\n",
            "iteration : 150, loss : 0.6859, accuracy : 78.94\n",
            "iteration : 200, loss : 0.6860, accuracy : 78.85\n",
            "iteration : 250, loss : 0.6924, accuracy : 78.58\n",
            "iteration : 300, loss : 0.7011, accuracy : 78.34\n",
            "iteration : 350, loss : 0.7074, accuracy : 78.18\n",
            "Epoch :  26, training loss : 0.7100, training accuracy : 78.12, test loss : 1.4136, test accuracy : 64.80\n",
            "\n",
            "Epoch: 27\n",
            "iteration :  50, loss : 0.6017, accuracy : 81.31\n",
            "iteration : 100, loss : 0.6169, accuracy : 80.95\n",
            "iteration : 150, loss : 0.6328, accuracy : 80.52\n",
            "iteration : 200, loss : 0.6448, accuracy : 80.18\n",
            "iteration : 250, loss : 0.6509, accuracy : 80.05\n",
            "iteration : 300, loss : 0.6610, accuracy : 79.77\n",
            "iteration : 350, loss : 0.6697, accuracy : 79.47\n",
            "Epoch :  27, training loss : 0.6743, training accuracy : 79.30, test loss : 1.4220, test accuracy : 65.35\n",
            "\n",
            "Epoch: 28\n",
            "iteration :  50, loss : 0.6389, accuracy : 80.23\n",
            "iteration : 100, loss : 0.6363, accuracy : 80.49\n",
            "iteration : 150, loss : 0.6401, accuracy : 80.30\n",
            "iteration : 200, loss : 0.6386, accuracy : 80.38\n",
            "iteration : 250, loss : 0.6416, accuracy : 80.20\n",
            "iteration : 300, loss : 0.6414, accuracy : 80.14\n",
            "iteration : 350, loss : 0.6470, accuracy : 79.92\n",
            "Epoch :  28, training loss : 0.6515, training accuracy : 79.80, test loss : 1.3109, test accuracy : 67.42\n",
            "\n",
            "Epoch: 29\n",
            "iteration :  50, loss : 0.5722, accuracy : 82.23\n",
            "iteration : 100, loss : 0.5653, accuracy : 82.41\n",
            "iteration : 150, loss : 0.5755, accuracy : 82.05\n",
            "iteration : 200, loss : 0.5861, accuracy : 81.78\n",
            "iteration : 250, loss : 0.5967, accuracy : 81.40\n",
            "iteration : 300, loss : 0.5999, accuracy : 81.35\n",
            "iteration : 350, loss : 0.6056, accuracy : 81.21\n",
            "Epoch :  29, training loss : 0.6085, training accuracy : 81.08, test loss : 1.3460, test accuracy : 67.04\n",
            "\n",
            "Epoch: 30\n",
            "iteration :  50, loss : 0.5594, accuracy : 82.38\n",
            "iteration : 100, loss : 0.5587, accuracy : 82.52\n",
            "iteration : 150, loss : 0.5660, accuracy : 82.19\n",
            "iteration : 200, loss : 0.5721, accuracy : 82.03\n",
            "iteration : 250, loss : 0.5759, accuracy : 81.94\n",
            "iteration : 300, loss : 0.5793, accuracy : 81.82\n",
            "iteration : 350, loss : 0.5868, accuracy : 81.62\n",
            "Epoch :  30, training loss : 0.5905, training accuracy : 81.56, test loss : 1.3067, test accuracy : 67.87\n",
            "\n",
            "Epoch: 31\n",
            "iteration :  50, loss : 0.5305, accuracy : 83.92\n",
            "iteration : 100, loss : 0.5443, accuracy : 83.33\n",
            "iteration : 150, loss : 0.5515, accuracy : 83.01\n",
            "iteration : 200, loss : 0.5501, accuracy : 83.00\n",
            "iteration : 250, loss : 0.5513, accuracy : 82.92\n",
            "iteration : 300, loss : 0.5531, accuracy : 82.83\n",
            "iteration : 350, loss : 0.5582, accuracy : 82.67\n",
            "Epoch :  31, training loss : 0.5586, training accuracy : 82.63, test loss : 1.3828, test accuracy : 66.58\n",
            "\n",
            "Epoch: 32\n",
            "iteration :  50, loss : 0.4925, accuracy : 84.42\n",
            "iteration : 100, loss : 0.4869, accuracy : 84.79\n",
            "iteration : 150, loss : 0.4954, accuracy : 84.59\n",
            "iteration : 200, loss : 0.5049, accuracy : 84.23\n",
            "iteration : 250, loss : 0.5092, accuracy : 84.04\n",
            "iteration : 300, loss : 0.5141, accuracy : 83.89\n",
            "iteration : 350, loss : 0.5187, accuracy : 83.69\n",
            "Epoch :  32, training loss : 0.5243, training accuracy : 83.50, test loss : 1.3370, test accuracy : 67.84\n",
            "\n",
            "Epoch: 33\n",
            "iteration :  50, loss : 0.4792, accuracy : 84.86\n",
            "iteration : 100, loss : 0.4668, accuracy : 85.09\n",
            "iteration : 150, loss : 0.4842, accuracy : 84.60\n",
            "iteration : 200, loss : 0.4845, accuracy : 84.64\n",
            "iteration : 250, loss : 0.4893, accuracy : 84.58\n",
            "iteration : 300, loss : 0.4982, accuracy : 84.33\n",
            "iteration : 350, loss : 0.5049, accuracy : 84.09\n",
            "Epoch :  33, training loss : 0.5100, training accuracy : 83.93, test loss : 1.3208, test accuracy : 68.45\n",
            "\n",
            "Epoch: 34\n",
            "iteration :  50, loss : 0.4707, accuracy : 85.64\n",
            "iteration : 100, loss : 0.4800, accuracy : 85.02\n",
            "iteration : 150, loss : 0.4832, accuracy : 84.86\n",
            "iteration : 200, loss : 0.4775, accuracy : 85.02\n",
            "iteration : 250, loss : 0.4764, accuracy : 85.00\n",
            "iteration : 300, loss : 0.4796, accuracy : 84.84\n",
            "iteration : 350, loss : 0.4861, accuracy : 84.65\n",
            "Epoch :  34, training loss : 0.4931, training accuracy : 84.48, test loss : 1.4429, test accuracy : 67.05\n",
            "\n",
            "Epoch: 35\n",
            "iteration :  50, loss : 0.4453, accuracy : 85.62\n",
            "iteration : 100, loss : 0.4468, accuracy : 85.83\n",
            "iteration : 150, loss : 0.4569, accuracy : 85.53\n",
            "iteration : 200, loss : 0.4602, accuracy : 85.47\n",
            "iteration : 250, loss : 0.4643, accuracy : 85.36\n",
            "iteration : 300, loss : 0.4647, accuracy : 85.33\n",
            "iteration : 350, loss : 0.4642, accuracy : 85.29\n",
            "Epoch :  35, training loss : 0.4678, training accuracy : 85.15, test loss : 1.4094, test accuracy : 68.21\n",
            "\n",
            "Epoch: 36\n",
            "iteration :  50, loss : 0.4178, accuracy : 86.91\n",
            "iteration : 100, loss : 0.4115, accuracy : 87.05\n",
            "iteration : 150, loss : 0.4285, accuracy : 86.49\n",
            "iteration : 200, loss : 0.4333, accuracy : 86.37\n",
            "iteration : 250, loss : 0.4388, accuracy : 86.21\n",
            "iteration : 300, loss : 0.4442, accuracy : 86.04\n",
            "iteration : 350, loss : 0.4483, accuracy : 85.88\n",
            "Epoch :  36, training loss : 0.4480, training accuracy : 85.94, test loss : 1.3521, test accuracy : 68.34\n",
            "\n",
            "Epoch: 37\n",
            "iteration :  50, loss : 0.3912, accuracy : 88.06\n",
            "iteration : 100, loss : 0.4038, accuracy : 87.78\n",
            "iteration : 150, loss : 0.4094, accuracy : 87.51\n",
            "iteration : 200, loss : 0.4189, accuracy : 87.08\n",
            "iteration : 250, loss : 0.4209, accuracy : 86.97\n",
            "iteration : 300, loss : 0.4235, accuracy : 86.83\n",
            "iteration : 350, loss : 0.4270, accuracy : 86.69\n",
            "Epoch :  37, training loss : 0.4305, training accuracy : 86.56, test loss : 1.3635, test accuracy : 68.76\n",
            "\n",
            "Epoch: 38\n",
            "iteration :  50, loss : 0.3820, accuracy : 88.31\n",
            "iteration : 100, loss : 0.3867, accuracy : 87.82\n",
            "iteration : 150, loss : 0.3950, accuracy : 87.49\n",
            "iteration : 200, loss : 0.3982, accuracy : 87.35\n",
            "iteration : 250, loss : 0.4043, accuracy : 87.11\n",
            "iteration : 300, loss : 0.4071, accuracy : 87.09\n",
            "iteration : 350, loss : 0.4143, accuracy : 86.88\n",
            "Epoch :  38, training loss : 0.4137, training accuracy : 86.88, test loss : 1.3491, test accuracy : 68.93\n",
            "\n",
            "Epoch: 39\n",
            "iteration :  50, loss : 0.3531, accuracy : 89.02\n",
            "iteration : 100, loss : 0.3596, accuracy : 88.59\n",
            "iteration : 150, loss : 0.3688, accuracy : 88.35\n",
            "iteration : 200, loss : 0.3778, accuracy : 88.11\n",
            "iteration : 250, loss : 0.3882, accuracy : 87.71\n",
            "iteration : 300, loss : 0.3926, accuracy : 87.64\n",
            "iteration : 350, loss : 0.3921, accuracy : 87.66\n",
            "Epoch :  39, training loss : 0.3932, training accuracy : 87.62, test loss : 1.3993, test accuracy : 68.45\n",
            "\n",
            "Epoch: 40\n",
            "iteration :  50, loss : 0.3453, accuracy : 89.27\n",
            "iteration : 100, loss : 0.3431, accuracy : 89.29\n",
            "iteration : 150, loss : 0.3517, accuracy : 88.88\n",
            "iteration : 200, loss : 0.3553, accuracy : 88.74\n",
            "iteration : 250, loss : 0.3571, accuracy : 88.68\n",
            "iteration : 300, loss : 0.3647, accuracy : 88.43\n",
            "iteration : 350, loss : 0.3682, accuracy : 88.32\n",
            "Epoch :  40, training loss : 0.3711, training accuracy : 88.19, test loss : 1.4640, test accuracy : 68.87\n",
            "\n",
            "Epoch: 41\n",
            "iteration :  50, loss : 0.3441, accuracy : 89.11\n",
            "iteration : 100, loss : 0.3389, accuracy : 89.31\n",
            "iteration : 150, loss : 0.3470, accuracy : 88.96\n",
            "iteration : 200, loss : 0.3509, accuracy : 88.89\n",
            "iteration : 250, loss : 0.3566, accuracy : 88.70\n",
            "iteration : 300, loss : 0.3573, accuracy : 88.65\n",
            "iteration : 350, loss : 0.3642, accuracy : 88.39\n",
            "Epoch :  41, training loss : 0.3677, training accuracy : 88.28, test loss : 1.3851, test accuracy : 69.46\n",
            "\n",
            "Epoch: 42\n",
            "iteration :  50, loss : 0.3409, accuracy : 89.41\n",
            "iteration : 100, loss : 0.3376, accuracy : 89.45\n",
            "iteration : 150, loss : 0.3411, accuracy : 89.27\n",
            "iteration : 200, loss : 0.3415, accuracy : 89.31\n",
            "iteration : 250, loss : 0.3456, accuracy : 89.15\n",
            "iteration : 300, loss : 0.3489, accuracy : 89.00\n",
            "iteration : 350, loss : 0.3515, accuracy : 88.91\n",
            "Epoch :  42, training loss : 0.3557, training accuracy : 88.75, test loss : 1.4165, test accuracy : 68.73\n",
            "\n",
            "Epoch: 43\n",
            "iteration :  50, loss : 0.3307, accuracy : 89.27\n",
            "iteration : 100, loss : 0.3328, accuracy : 89.46\n",
            "iteration : 150, loss : 0.3305, accuracy : 89.57\n",
            "iteration : 200, loss : 0.3292, accuracy : 89.65\n",
            "iteration : 250, loss : 0.3311, accuracy : 89.57\n",
            "iteration : 300, loss : 0.3292, accuracy : 89.58\n",
            "iteration : 350, loss : 0.3345, accuracy : 89.37\n",
            "Epoch :  43, training loss : 0.3399, training accuracy : 89.28, test loss : 1.4473, test accuracy : 68.51\n",
            "\n",
            "Epoch: 44\n",
            "iteration :  50, loss : 0.3091, accuracy : 90.44\n",
            "iteration : 100, loss : 0.3143, accuracy : 90.06\n",
            "iteration : 150, loss : 0.3145, accuracy : 90.01\n",
            "iteration : 200, loss : 0.3181, accuracy : 89.98\n",
            "iteration : 250, loss : 0.3178, accuracy : 89.99\n",
            "iteration : 300, loss : 0.3236, accuracy : 89.83\n",
            "iteration : 350, loss : 0.3284, accuracy : 89.65\n",
            "Epoch :  44, training loss : 0.3299, training accuracy : 89.60, test loss : 1.3973, test accuracy : 70.41\n",
            "\n",
            "Epoch: 45\n",
            "iteration :  50, loss : 0.2881, accuracy : 90.75\n",
            "iteration : 100, loss : 0.2926, accuracy : 90.83\n",
            "iteration : 150, loss : 0.2990, accuracy : 90.64\n",
            "iteration : 200, loss : 0.2993, accuracy : 90.61\n",
            "iteration : 250, loss : 0.3064, accuracy : 90.39\n",
            "iteration : 300, loss : 0.3108, accuracy : 90.19\n",
            "iteration : 350, loss : 0.3131, accuracy : 90.15\n",
            "Epoch :  45, training loss : 0.3153, training accuracy : 90.03, test loss : 1.5170, test accuracy : 69.16\n",
            "\n",
            "Epoch: 46\n",
            "iteration :  50, loss : 0.2851, accuracy : 91.16\n",
            "iteration : 100, loss : 0.2851, accuracy : 91.02\n",
            "iteration : 150, loss : 0.2841, accuracy : 91.19\n",
            "iteration : 200, loss : 0.2848, accuracy : 91.09\n",
            "iteration : 250, loss : 0.2886, accuracy : 90.96\n",
            "iteration : 300, loss : 0.2952, accuracy : 90.72\n",
            "iteration : 350, loss : 0.3005, accuracy : 90.52\n",
            "Epoch :  46, training loss : 0.3007, training accuracy : 90.48, test loss : 1.4066, test accuracy : 69.59\n",
            "\n",
            "Epoch: 47\n",
            "iteration :  50, loss : 0.2724, accuracy : 91.44\n",
            "iteration : 100, loss : 0.2721, accuracy : 91.39\n",
            "iteration : 150, loss : 0.2714, accuracy : 91.47\n",
            "iteration : 200, loss : 0.2763, accuracy : 91.30\n",
            "iteration : 250, loss : 0.2798, accuracy : 91.22\n",
            "iteration : 300, loss : 0.2852, accuracy : 91.07\n",
            "iteration : 350, loss : 0.2887, accuracy : 90.93\n",
            "Epoch :  47, training loss : 0.2897, training accuracy : 90.87, test loss : 1.4165, test accuracy : 70.23\n",
            "\n",
            "Epoch: 48\n",
            "iteration :  50, loss : 0.2783, accuracy : 91.33\n",
            "iteration : 100, loss : 0.2694, accuracy : 91.82\n",
            "iteration : 150, loss : 0.2688, accuracy : 91.76\n",
            "iteration : 200, loss : 0.2763, accuracy : 91.49\n",
            "iteration : 250, loss : 0.2782, accuracy : 91.48\n",
            "iteration : 300, loss : 0.2797, accuracy : 91.43\n",
            "iteration : 350, loss : 0.2787, accuracy : 91.45\n",
            "Epoch :  48, training loss : 0.2799, training accuracy : 91.37, test loss : 1.4224, test accuracy : 70.05\n",
            "\n",
            "Epoch: 49\n",
            "iteration :  50, loss : 0.2390, accuracy : 92.62\n",
            "iteration : 100, loss : 0.2358, accuracy : 92.77\n",
            "iteration : 150, loss : 0.2442, accuracy : 92.45\n",
            "iteration : 200, loss : 0.2501, accuracy : 92.28\n",
            "iteration : 250, loss : 0.2563, accuracy : 92.11\n",
            "iteration : 300, loss : 0.2600, accuracy : 91.98\n",
            "iteration : 350, loss : 0.2623, accuracy : 91.88\n",
            "Epoch :  49, training loss : 0.2666, training accuracy : 91.76, test loss : 1.4402, test accuracy : 71.05\n",
            "\n",
            "Epoch: 50\n",
            "iteration :  50, loss : 0.2957, accuracy : 90.92\n",
            "iteration : 100, loss : 0.2826, accuracy : 91.30\n",
            "iteration : 150, loss : 0.2805, accuracy : 91.30\n",
            "iteration : 200, loss : 0.2776, accuracy : 91.38\n",
            "iteration : 250, loss : 0.2778, accuracy : 91.32\n",
            "iteration : 300, loss : 0.2772, accuracy : 91.34\n",
            "iteration : 350, loss : 0.2780, accuracy : 91.26\n",
            "Epoch :  50, training loss : 0.2809, training accuracy : 91.15, test loss : 1.4197, test accuracy : 70.52\n",
            "\n",
            "Epoch: 51\n",
            "iteration :  50, loss : 0.2460, accuracy : 92.14\n",
            "iteration : 100, loss : 0.2374, accuracy : 92.55\n",
            "iteration : 150, loss : 0.2407, accuracy : 92.52\n",
            "iteration : 200, loss : 0.2529, accuracy : 92.11\n",
            "iteration : 250, loss : 0.2568, accuracy : 91.93\n",
            "iteration : 300, loss : 0.2584, accuracy : 91.81\n",
            "iteration : 350, loss : 0.2634, accuracy : 91.62\n",
            "Epoch :  51, training loss : 0.2648, training accuracy : 91.62, test loss : 1.4754, test accuracy : 70.08\n",
            "\n",
            "Epoch: 52\n",
            "iteration :  50, loss : 0.2358, accuracy : 92.97\n",
            "iteration : 100, loss : 0.2406, accuracy : 92.65\n",
            "iteration : 150, loss : 0.2432, accuracy : 92.54\n",
            "iteration : 200, loss : 0.2465, accuracy : 92.36\n",
            "iteration : 250, loss : 0.2494, accuracy : 92.21\n",
            "iteration : 300, loss : 0.2497, accuracy : 92.16\n",
            "iteration : 350, loss : 0.2540, accuracy : 91.99\n",
            "Epoch :  52, training loss : 0.2564, training accuracy : 91.94, test loss : 1.4037, test accuracy : 70.58\n",
            "\n",
            "Epoch: 53\n",
            "iteration :  50, loss : 0.2468, accuracy : 92.19\n",
            "iteration : 100, loss : 0.2440, accuracy : 92.37\n",
            "iteration : 150, loss : 0.2417, accuracy : 92.45\n",
            "iteration : 200, loss : 0.2441, accuracy : 92.50\n",
            "iteration : 250, loss : 0.2447, accuracy : 92.43\n",
            "iteration : 300, loss : 0.2452, accuracy : 92.45\n",
            "iteration : 350, loss : 0.2467, accuracy : 92.39\n",
            "Epoch :  53, training loss : 0.2459, training accuracy : 92.39, test loss : 1.4761, test accuracy : 70.50\n",
            "\n",
            "Epoch: 54\n",
            "iteration :  50, loss : 0.2227, accuracy : 92.83\n",
            "iteration : 100, loss : 0.2153, accuracy : 93.17\n",
            "iteration : 150, loss : 0.2134, accuracy : 93.21\n",
            "iteration : 200, loss : 0.2174, accuracy : 93.05\n",
            "iteration : 250, loss : 0.2226, accuracy : 92.92\n",
            "iteration : 300, loss : 0.2228, accuracy : 92.94\n",
            "iteration : 350, loss : 0.2252, accuracy : 92.87\n",
            "Epoch :  54, training loss : 0.2289, training accuracy : 92.74, test loss : 1.4493, test accuracy : 71.10\n",
            "\n",
            "Epoch: 55\n",
            "iteration :  50, loss : 0.2282, accuracy : 92.77\n",
            "iteration : 100, loss : 0.2264, accuracy : 92.77\n",
            "iteration : 150, loss : 0.2244, accuracy : 92.85\n",
            "iteration : 200, loss : 0.2255, accuracy : 92.89\n",
            "iteration : 250, loss : 0.2300, accuracy : 92.85\n",
            "iteration : 300, loss : 0.2315, accuracy : 92.87\n",
            "iteration : 350, loss : 0.2307, accuracy : 92.93\n",
            "Epoch :  55, training loss : 0.2308, training accuracy : 92.88, test loss : 1.5256, test accuracy : 70.25\n",
            "\n",
            "Epoch: 56\n",
            "iteration :  50, loss : 0.2087, accuracy : 93.27\n",
            "iteration : 100, loss : 0.1999, accuracy : 93.84\n",
            "iteration : 150, loss : 0.2043, accuracy : 93.77\n",
            "iteration : 200, loss : 0.2038, accuracy : 93.74\n",
            "iteration : 250, loss : 0.2055, accuracy : 93.71\n",
            "iteration : 300, loss : 0.2097, accuracy : 93.55\n",
            "iteration : 350, loss : 0.2156, accuracy : 93.34\n",
            "Epoch :  56, training loss : 0.2176, training accuracy : 93.26, test loss : 1.4649, test accuracy : 70.13\n",
            "\n",
            "Epoch: 57\n",
            "iteration :  50, loss : 0.2162, accuracy : 93.23\n",
            "iteration : 100, loss : 0.2103, accuracy : 93.52\n",
            "iteration : 150, loss : 0.2120, accuracy : 93.46\n",
            "iteration : 200, loss : 0.2140, accuracy : 93.32\n",
            "iteration : 250, loss : 0.2143, accuracy : 93.29\n",
            "iteration : 300, loss : 0.2155, accuracy : 93.28\n",
            "iteration : 350, loss : 0.2175, accuracy : 93.26\n",
            "Epoch :  57, training loss : 0.2186, training accuracy : 93.21, test loss : 1.4799, test accuracy : 70.83\n",
            "\n",
            "Epoch: 58\n",
            "iteration :  50, loss : 0.2101, accuracy : 93.42\n",
            "iteration : 100, loss : 0.2130, accuracy : 93.36\n",
            "iteration : 150, loss : 0.2061, accuracy : 93.60\n",
            "iteration : 200, loss : 0.2108, accuracy : 93.49\n",
            "iteration : 250, loss : 0.2141, accuracy : 93.37\n",
            "iteration : 300, loss : 0.2157, accuracy : 93.32\n",
            "iteration : 350, loss : 0.2149, accuracy : 93.33\n",
            "Epoch :  58, training loss : 0.2150, training accuracy : 93.37, test loss : 1.4807, test accuracy : 70.68\n",
            "\n",
            "Epoch: 59\n",
            "iteration :  50, loss : 0.2009, accuracy : 93.89\n",
            "iteration : 100, loss : 0.1956, accuracy : 94.16\n",
            "iteration : 150, loss : 0.1953, accuracy : 94.16\n",
            "iteration : 200, loss : 0.1979, accuracy : 94.04\n",
            "iteration : 250, loss : 0.2003, accuracy : 93.93\n",
            "iteration : 300, loss : 0.2048, accuracy : 93.80\n",
            "iteration : 350, loss : 0.2067, accuracy : 93.74\n",
            "Epoch :  59, training loss : 0.2065, training accuracy : 93.73, test loss : 1.4490, test accuracy : 70.72\n",
            "\n",
            "Epoch: 60\n",
            "iteration :  50, loss : 0.1847, accuracy : 94.16\n",
            "iteration : 100, loss : 0.1882, accuracy : 94.02\n",
            "iteration : 150, loss : 0.1965, accuracy : 93.90\n",
            "iteration : 200, loss : 0.1991, accuracy : 93.83\n",
            "iteration : 250, loss : 0.1961, accuracy : 93.85\n",
            "iteration : 300, loss : 0.1974, accuracy : 93.79\n",
            "iteration : 350, loss : 0.1992, accuracy : 93.75\n",
            "Epoch :  60, training loss : 0.2009, training accuracy : 93.70, test loss : 1.4860, test accuracy : 71.03\n",
            "\n",
            "Epoch: 61\n",
            "iteration :  50, loss : 0.1910, accuracy : 94.14\n",
            "iteration : 100, loss : 0.1863, accuracy : 94.10\n",
            "iteration : 150, loss : 0.1888, accuracy : 94.11\n",
            "iteration : 200, loss : 0.1879, accuracy : 94.18\n",
            "iteration : 250, loss : 0.1903, accuracy : 94.13\n",
            "iteration : 300, loss : 0.1924, accuracy : 94.06\n",
            "iteration : 350, loss : 0.1944, accuracy : 93.98\n",
            "Epoch :  61, training loss : 0.1951, training accuracy : 93.94, test loss : 1.4946, test accuracy : 70.52\n",
            "\n",
            "Epoch: 62\n",
            "iteration :  50, loss : 0.1846, accuracy : 94.61\n",
            "iteration : 100, loss : 0.1884, accuracy : 94.48\n",
            "iteration : 150, loss : 0.1865, accuracy : 94.34\n",
            "iteration : 200, loss : 0.1845, accuracy : 94.42\n",
            "iteration : 250, loss : 0.1876, accuracy : 94.31\n",
            "iteration : 300, loss : 0.1881, accuracy : 94.28\n",
            "iteration : 350, loss : 0.1874, accuracy : 94.33\n",
            "Epoch :  62, training loss : 0.1887, training accuracy : 94.27, test loss : 1.5300, test accuracy : 71.07\n",
            "\n",
            "Epoch: 63\n",
            "iteration :  50, loss : 0.1776, accuracy : 94.33\n",
            "iteration : 100, loss : 0.1757, accuracy : 94.61\n",
            "iteration : 150, loss : 0.1822, accuracy : 94.43\n",
            "iteration : 200, loss : 0.1797, accuracy : 94.54\n",
            "iteration : 250, loss : 0.1822, accuracy : 94.50\n",
            "iteration : 300, loss : 0.1835, accuracy : 94.44\n",
            "iteration : 350, loss : 0.1846, accuracy : 94.37\n",
            "Epoch :  63, training loss : 0.1857, training accuracy : 94.30, test loss : 1.4267, test accuracy : 72.01\n",
            "\n",
            "Epoch: 64\n",
            "iteration :  50, loss : 0.1856, accuracy : 94.53\n",
            "iteration : 100, loss : 0.1774, accuracy : 94.71\n",
            "iteration : 150, loss : 0.1751, accuracy : 94.72\n",
            "iteration : 200, loss : 0.1768, accuracy : 94.62\n",
            "iteration : 250, loss : 0.1762, accuracy : 94.65\n",
            "iteration : 300, loss : 0.1779, accuracy : 94.58\n",
            "iteration : 350, loss : 0.1800, accuracy : 94.54\n",
            "Epoch :  64, training loss : 0.1798, training accuracy : 94.53, test loss : 1.4429, test accuracy : 71.24\n",
            "\n",
            "Epoch: 65\n",
            "iteration :  50, loss : 0.1518, accuracy : 95.44\n",
            "iteration : 100, loss : 0.1581, accuracy : 95.11\n",
            "iteration : 150, loss : 0.1624, accuracy : 95.02\n",
            "iteration : 200, loss : 0.1676, accuracy : 94.82\n",
            "iteration : 250, loss : 0.1698, accuracy : 94.78\n",
            "iteration : 300, loss : 0.1717, accuracy : 94.71\n",
            "iteration : 350, loss : 0.1740, accuracy : 94.60\n",
            "Epoch :  65, training loss : 0.1751, training accuracy : 94.56, test loss : 1.4760, test accuracy : 71.76\n",
            "\n",
            "Epoch: 66\n",
            "iteration :  50, loss : 0.1531, accuracy : 95.31\n",
            "iteration : 100, loss : 0.1581, accuracy : 95.30\n",
            "iteration : 150, loss : 0.1637, accuracy : 95.07\n",
            "iteration : 200, loss : 0.1641, accuracy : 95.01\n",
            "iteration : 250, loss : 0.1647, accuracy : 94.98\n",
            "iteration : 300, loss : 0.1693, accuracy : 94.89\n",
            "iteration : 350, loss : 0.1712, accuracy : 94.78\n",
            "Epoch :  66, training loss : 0.1723, training accuracy : 94.70, test loss : 1.4880, test accuracy : 71.82\n",
            "\n",
            "Epoch: 67\n",
            "iteration :  50, loss : 0.1664, accuracy : 94.91\n",
            "iteration : 100, loss : 0.1675, accuracy : 94.97\n",
            "iteration : 150, loss : 0.1687, accuracy : 94.98\n",
            "iteration : 200, loss : 0.1698, accuracy : 94.89\n",
            "iteration : 250, loss : 0.1703, accuracy : 94.78\n",
            "iteration : 300, loss : 0.1725, accuracy : 94.72\n",
            "iteration : 350, loss : 0.1728, accuracy : 94.69\n",
            "Epoch :  67, training loss : 0.1748, training accuracy : 94.64, test loss : 1.4999, test accuracy : 71.20\n",
            "\n",
            "Epoch: 68\n",
            "iteration :  50, loss : 0.1726, accuracy : 94.47\n",
            "iteration : 100, loss : 0.1725, accuracy : 94.63\n",
            "iteration : 150, loss : 0.1720, accuracy : 94.74\n",
            "iteration : 200, loss : 0.1661, accuracy : 94.87\n",
            "iteration : 250, loss : 0.1661, accuracy : 94.84\n",
            "iteration : 300, loss : 0.1684, accuracy : 94.79\n",
            "iteration : 350, loss : 0.1722, accuracy : 94.68\n",
            "Epoch :  68, training loss : 0.1738, training accuracy : 94.64, test loss : 1.5234, test accuracy : 70.48\n",
            "\n",
            "Epoch: 69\n",
            "iteration :  50, loss : 0.1545, accuracy : 95.45\n",
            "iteration : 100, loss : 0.1585, accuracy : 95.23\n",
            "iteration : 150, loss : 0.1627, accuracy : 95.06\n",
            "iteration : 200, loss : 0.1622, accuracy : 95.09\n",
            "iteration : 250, loss : 0.1631, accuracy : 95.12\n",
            "iteration : 300, loss : 0.1633, accuracy : 95.10\n",
            "iteration : 350, loss : 0.1628, accuracy : 95.12\n",
            "Epoch :  69, training loss : 0.1626, training accuracy : 95.11, test loss : 1.4570, test accuracy : 71.78\n",
            "\n",
            "Epoch: 70\n",
            "iteration :  50, loss : 0.1459, accuracy : 95.66\n",
            "iteration : 100, loss : 0.1447, accuracy : 95.56\n",
            "iteration : 150, loss : 0.1472, accuracy : 95.50\n",
            "iteration : 200, loss : 0.1528, accuracy : 95.30\n",
            "iteration : 250, loss : 0.1556, accuracy : 95.16\n",
            "iteration : 300, loss : 0.1562, accuracy : 95.20\n",
            "iteration : 350, loss : 0.1566, accuracy : 95.18\n",
            "Epoch :  70, training loss : 0.1564, training accuracy : 95.17, test loss : 1.4312, test accuracy : 71.71\n",
            "\n",
            "Epoch: 71\n",
            "iteration :  50, loss : 0.1391, accuracy : 95.84\n",
            "iteration : 100, loss : 0.1421, accuracy : 95.67\n",
            "iteration : 150, loss : 0.1464, accuracy : 95.56\n",
            "iteration : 200, loss : 0.1464, accuracy : 95.51\n",
            "iteration : 250, loss : 0.1467, accuracy : 95.54\n",
            "iteration : 300, loss : 0.1478, accuracy : 95.47\n",
            "iteration : 350, loss : 0.1510, accuracy : 95.32\n",
            "Epoch :  71, training loss : 0.1532, training accuracy : 95.27, test loss : 1.5444, test accuracy : 71.74\n",
            "\n",
            "Epoch: 72\n",
            "iteration :  50, loss : 0.1433, accuracy : 95.53\n",
            "iteration : 100, loss : 0.1486, accuracy : 95.31\n",
            "iteration : 150, loss : 0.1505, accuracy : 95.36\n",
            "iteration : 200, loss : 0.1508, accuracy : 95.43\n",
            "iteration : 250, loss : 0.1494, accuracy : 95.46\n",
            "iteration : 300, loss : 0.1486, accuracy : 95.43\n",
            "iteration : 350, loss : 0.1511, accuracy : 95.35\n",
            "Epoch :  72, training loss : 0.1524, training accuracy : 95.34, test loss : 1.5207, test accuracy : 71.54\n",
            "\n",
            "Epoch: 73\n",
            "iteration :  50, loss : 0.1637, accuracy : 95.20\n",
            "iteration : 100, loss : 0.1552, accuracy : 95.32\n",
            "iteration : 150, loss : 0.1493, accuracy : 95.44\n",
            "iteration : 200, loss : 0.1509, accuracy : 95.44\n",
            "iteration : 250, loss : 0.1520, accuracy : 95.42\n",
            "iteration : 300, loss : 0.1529, accuracy : 95.35\n",
            "iteration : 350, loss : 0.1527, accuracy : 95.35\n",
            "Epoch :  73, training loss : 0.1528, training accuracy : 95.35, test loss : 1.5366, test accuracy : 71.53\n",
            "\n",
            "Epoch: 74\n",
            "iteration :  50, loss : 0.1334, accuracy : 96.16\n",
            "iteration : 100, loss : 0.1339, accuracy : 95.97\n",
            "iteration : 150, loss : 0.1380, accuracy : 95.85\n",
            "iteration : 200, loss : 0.1381, accuracy : 95.87\n",
            "iteration : 250, loss : 0.1428, accuracy : 95.75\n",
            "iteration : 300, loss : 0.1440, accuracy : 95.65\n",
            "iteration : 350, loss : 0.1460, accuracy : 95.58\n",
            "Epoch :  74, training loss : 0.1476, training accuracy : 95.55, test loss : 1.5244, test accuracy : 71.63\n",
            "\n",
            "Epoch: 75\n",
            "iteration :  50, loss : 0.1324, accuracy : 96.16\n",
            "iteration : 100, loss : 0.1337, accuracy : 95.95\n",
            "iteration : 150, loss : 0.1409, accuracy : 95.69\n",
            "iteration : 200, loss : 0.1412, accuracy : 95.70\n",
            "iteration : 250, loss : 0.1442, accuracy : 95.57\n",
            "iteration : 300, loss : 0.1441, accuracy : 95.55\n",
            "iteration : 350, loss : 0.1463, accuracy : 95.51\n",
            "Epoch :  75, training loss : 0.1472, training accuracy : 95.50, test loss : 1.5389, test accuracy : 71.15\n",
            "\n",
            "Epoch: 76\n",
            "iteration :  50, loss : 0.1305, accuracy : 95.73\n",
            "iteration : 100, loss : 0.1317, accuracy : 95.95\n",
            "iteration : 150, loss : 0.1336, accuracy : 95.86\n",
            "iteration : 200, loss : 0.1360, accuracy : 95.76\n",
            "iteration : 250, loss : 0.1394, accuracy : 95.70\n",
            "iteration : 300, loss : 0.1407, accuracy : 95.67\n",
            "iteration : 350, loss : 0.1407, accuracy : 95.66\n",
            "Epoch :  76, training loss : 0.1410, training accuracy : 95.67, test loss : 1.4902, test accuracy : 71.94\n",
            "\n",
            "Epoch: 77\n",
            "iteration :  50, loss : 0.1394, accuracy : 95.61\n",
            "iteration : 100, loss : 0.1359, accuracy : 95.66\n",
            "iteration : 150, loss : 0.1390, accuracy : 95.58\n",
            "iteration : 200, loss : 0.1393, accuracy : 95.55\n",
            "iteration : 250, loss : 0.1405, accuracy : 95.61\n",
            "iteration : 300, loss : 0.1411, accuracy : 95.61\n",
            "iteration : 350, loss : 0.1402, accuracy : 95.65\n",
            "Epoch :  77, training loss : 0.1409, training accuracy : 95.64, test loss : 1.4888, test accuracy : 71.93\n",
            "\n",
            "Epoch: 78\n",
            "iteration :  50, loss : 0.1298, accuracy : 96.06\n",
            "iteration : 100, loss : 0.1323, accuracy : 95.83\n",
            "iteration : 150, loss : 0.1360, accuracy : 95.75\n",
            "iteration : 200, loss : 0.1340, accuracy : 95.77\n",
            "iteration : 250, loss : 0.1355, accuracy : 95.77\n",
            "iteration : 300, loss : 0.1345, accuracy : 95.79\n",
            "iteration : 350, loss : 0.1329, accuracy : 95.85\n",
            "Epoch :  78, training loss : 0.1322, training accuracy : 95.86, test loss : 1.4874, test accuracy : 71.80\n",
            "\n",
            "Epoch: 79\n",
            "iteration :  50, loss : 0.1202, accuracy : 96.42\n",
            "iteration : 100, loss : 0.1320, accuracy : 96.13\n",
            "iteration : 150, loss : 0.1270, accuracy : 96.28\n",
            "iteration : 200, loss : 0.1292, accuracy : 96.15\n",
            "iteration : 250, loss : 0.1316, accuracy : 96.07\n",
            "iteration : 300, loss : 0.1332, accuracy : 96.01\n",
            "iteration : 350, loss : 0.1329, accuracy : 96.03\n",
            "Epoch :  79, training loss : 0.1330, training accuracy : 96.02, test loss : 1.4962, test accuracy : 71.93\n",
            "\n",
            "Epoch: 80\n",
            "iteration :  50, loss : 0.1196, accuracy : 96.47\n",
            "iteration : 100, loss : 0.1229, accuracy : 96.32\n",
            "iteration : 150, loss : 0.1279, accuracy : 96.13\n",
            "iteration : 200, loss : 0.1286, accuracy : 96.12\n",
            "iteration : 250, loss : 0.1297, accuracy : 96.10\n",
            "iteration : 300, loss : 0.1320, accuracy : 95.98\n",
            "iteration : 350, loss : 0.1308, accuracy : 96.03\n",
            "Epoch :  80, training loss : 0.1304, training accuracy : 96.04, test loss : 1.5036, test accuracy : 71.47\n",
            "\n",
            "Epoch: 81\n",
            "iteration :  50, loss : 0.1218, accuracy : 96.38\n",
            "iteration : 100, loss : 0.1277, accuracy : 96.00\n",
            "iteration : 150, loss : 0.1307, accuracy : 95.86\n",
            "iteration : 200, loss : 0.1310, accuracy : 95.89\n",
            "iteration : 250, loss : 0.1323, accuracy : 95.82\n",
            "iteration : 300, loss : 0.1346, accuracy : 95.77\n",
            "iteration : 350, loss : 0.1363, accuracy : 95.73\n",
            "Epoch :  81, training loss : 0.1360, training accuracy : 95.75, test loss : 1.5322, test accuracy : 72.02\n",
            "\n",
            "Epoch: 82\n",
            "iteration :  50, loss : 0.1356, accuracy : 96.06\n",
            "iteration : 100, loss : 0.1271, accuracy : 96.16\n",
            "iteration : 150, loss : 0.1285, accuracy : 96.15\n",
            "iteration : 200, loss : 0.1287, accuracy : 96.12\n",
            "iteration : 250, loss : 0.1282, accuracy : 96.17\n",
            "iteration : 300, loss : 0.1291, accuracy : 96.06\n",
            "iteration : 350, loss : 0.1301, accuracy : 96.03\n",
            "Epoch :  82, training loss : 0.1286, training accuracy : 96.12, test loss : 1.5149, test accuracy : 71.91\n",
            "\n",
            "Epoch: 83\n",
            "iteration :  50, loss : 0.1171, accuracy : 96.38\n",
            "iteration : 100, loss : 0.1176, accuracy : 96.43\n",
            "iteration : 150, loss : 0.1177, accuracy : 96.46\n",
            "iteration : 200, loss : 0.1219, accuracy : 96.30\n",
            "iteration : 250, loss : 0.1237, accuracy : 96.22\n",
            "iteration : 300, loss : 0.1238, accuracy : 96.21\n",
            "iteration : 350, loss : 0.1250, accuracy : 96.17\n",
            "Epoch :  83, training loss : 0.1242, training accuracy : 96.20, test loss : 1.4851, test accuracy : 71.87\n",
            "\n",
            "Epoch: 84\n",
            "iteration :  50, loss : 0.1161, accuracy : 96.59\n",
            "iteration : 100, loss : 0.1190, accuracy : 96.37\n",
            "iteration : 150, loss : 0.1177, accuracy : 96.35\n",
            "iteration : 200, loss : 0.1190, accuracy : 96.37\n",
            "iteration : 250, loss : 0.1218, accuracy : 96.28\n",
            "iteration : 300, loss : 0.1223, accuracy : 96.25\n",
            "iteration : 350, loss : 0.1224, accuracy : 96.26\n",
            "Epoch :  84, training loss : 0.1215, training accuracy : 96.27, test loss : 1.5619, test accuracy : 71.60\n",
            "\n",
            "Epoch: 85\n",
            "iteration :  50, loss : 0.1190, accuracy : 96.48\n",
            "iteration : 100, loss : 0.1206, accuracy : 96.38\n",
            "iteration : 150, loss : 0.1229, accuracy : 96.29\n",
            "iteration : 200, loss : 0.1217, accuracy : 96.31\n",
            "iteration : 250, loss : 0.1204, accuracy : 96.37\n",
            "iteration : 300, loss : 0.1216, accuracy : 96.30\n",
            "iteration : 350, loss : 0.1234, accuracy : 96.24\n",
            "Epoch :  85, training loss : 0.1235, training accuracy : 96.24, test loss : 1.4963, test accuracy : 71.73\n",
            "\n",
            "Epoch: 86\n",
            "iteration :  50, loss : 0.1160, accuracy : 96.72\n",
            "iteration : 100, loss : 0.1149, accuracy : 96.64\n",
            "iteration : 150, loss : 0.1141, accuracy : 96.67\n",
            "iteration : 200, loss : 0.1116, accuracy : 96.69\n",
            "iteration : 250, loss : 0.1140, accuracy : 96.56\n",
            "iteration : 300, loss : 0.1157, accuracy : 96.46\n",
            "iteration : 350, loss : 0.1173, accuracy : 96.42\n",
            "Epoch :  86, training loss : 0.1174, training accuracy : 96.42, test loss : 1.5214, test accuracy : 72.37\n",
            "\n",
            "Epoch: 87\n",
            "iteration :  50, loss : 0.1046, accuracy : 96.94\n",
            "iteration : 100, loss : 0.1072, accuracy : 96.67\n",
            "iteration : 150, loss : 0.1096, accuracy : 96.63\n",
            "iteration : 200, loss : 0.1107, accuracy : 96.64\n",
            "iteration : 250, loss : 0.1104, accuracy : 96.61\n",
            "iteration : 300, loss : 0.1130, accuracy : 96.53\n",
            "iteration : 350, loss : 0.1147, accuracy : 96.48\n",
            "Epoch :  87, training loss : 0.1159, training accuracy : 96.45, test loss : 1.5453, test accuracy : 72.11\n",
            "\n",
            "Epoch: 88\n",
            "iteration :  50, loss : 0.1189, accuracy : 96.02\n",
            "iteration : 100, loss : 0.1200, accuracy : 96.05\n",
            "iteration : 150, loss : 0.1187, accuracy : 96.19\n",
            "iteration : 200, loss : 0.1201, accuracy : 96.25\n",
            "iteration : 250, loss : 0.1188, accuracy : 96.34\n",
            "iteration : 300, loss : 0.1181, accuracy : 96.35\n",
            "iteration : 350, loss : 0.1185, accuracy : 96.34\n",
            "Epoch :  88, training loss : 0.1180, training accuracy : 96.35, test loss : 1.5498, test accuracy : 72.27\n",
            "\n",
            "Epoch: 89\n",
            "iteration :  50, loss : 0.1124, accuracy : 96.38\n",
            "iteration : 100, loss : 0.1200, accuracy : 96.36\n",
            "iteration : 150, loss : 0.1143, accuracy : 96.53\n",
            "iteration : 200, loss : 0.1126, accuracy : 96.62\n",
            "iteration : 250, loss : 0.1132, accuracy : 96.56\n",
            "iteration : 300, loss : 0.1128, accuracy : 96.53\n",
            "iteration : 350, loss : 0.1123, accuracy : 96.54\n",
            "Epoch :  89, training loss : 0.1119, training accuracy : 96.56, test loss : 1.5339, test accuracy : 71.65\n",
            "\n",
            "Epoch: 90\n",
            "iteration :  50, loss : 0.1083, accuracy : 96.58\n",
            "iteration : 100, loss : 0.1083, accuracy : 96.59\n",
            "iteration : 150, loss : 0.1091, accuracy : 96.53\n",
            "iteration : 200, loss : 0.1112, accuracy : 96.52\n",
            "iteration : 250, loss : 0.1117, accuracy : 96.51\n",
            "iteration : 300, loss : 0.1112, accuracy : 96.58\n",
            "iteration : 350, loss : 0.1119, accuracy : 96.56\n",
            "Epoch :  90, training loss : 0.1127, training accuracy : 96.54, test loss : 1.5529, test accuracy : 72.14\n",
            "\n",
            "Epoch: 91\n",
            "iteration :  50, loss : 0.1085, accuracy : 96.77\n",
            "iteration : 100, loss : 0.1098, accuracy : 96.71\n",
            "iteration : 150, loss : 0.1060, accuracy : 96.80\n",
            "iteration : 200, loss : 0.1083, accuracy : 96.70\n",
            "iteration : 250, loss : 0.1088, accuracy : 96.67\n",
            "iteration : 300, loss : 0.1096, accuracy : 96.67\n",
            "iteration : 350, loss : 0.1096, accuracy : 96.68\n",
            "Epoch :  91, training loss : 0.1089, training accuracy : 96.69, test loss : 1.5115, test accuracy : 72.42\n",
            "\n",
            "Epoch: 92\n",
            "iteration :  50, loss : 0.0952, accuracy : 96.97\n",
            "iteration : 100, loss : 0.0983, accuracy : 97.08\n",
            "iteration : 150, loss : 0.1020, accuracy : 96.99\n",
            "iteration : 200, loss : 0.1045, accuracy : 96.86\n",
            "iteration : 250, loss : 0.1042, accuracy : 96.85\n",
            "iteration : 300, loss : 0.1069, accuracy : 96.78\n",
            "iteration : 350, loss : 0.1096, accuracy : 96.71\n",
            "Epoch :  92, training loss : 0.1085, training accuracy : 96.76, test loss : 1.5527, test accuracy : 72.02\n",
            "\n",
            "Epoch: 93\n",
            "iteration :  50, loss : 0.0980, accuracy : 97.08\n",
            "iteration : 100, loss : 0.0957, accuracy : 97.13\n",
            "iteration : 150, loss : 0.0941, accuracy : 97.21\n",
            "iteration : 200, loss : 0.0963, accuracy : 97.10\n",
            "iteration : 250, loss : 0.0988, accuracy : 97.02\n",
            "iteration : 300, loss : 0.1005, accuracy : 96.98\n",
            "iteration : 350, loss : 0.1008, accuracy : 96.96\n",
            "Epoch :  93, training loss : 0.1029, training accuracy : 96.91, test loss : 1.5615, test accuracy : 72.12\n",
            "\n",
            "Epoch: 94\n",
            "iteration :  50, loss : 0.1007, accuracy : 96.92\n",
            "iteration : 100, loss : 0.0999, accuracy : 96.88\n",
            "iteration : 150, loss : 0.0994, accuracy : 96.81\n",
            "iteration : 200, loss : 0.1004, accuracy : 96.84\n",
            "iteration : 250, loss : 0.1020, accuracy : 96.83\n",
            "iteration : 300, loss : 0.1035, accuracy : 96.80\n",
            "iteration : 350, loss : 0.1043, accuracy : 96.81\n",
            "Epoch :  94, training loss : 0.1047, training accuracy : 96.77, test loss : 1.5642, test accuracy : 72.20\n",
            "\n",
            "Epoch: 95\n",
            "iteration :  50, loss : 0.1051, accuracy : 96.94\n",
            "iteration : 100, loss : 0.1003, accuracy : 96.95\n",
            "iteration : 150, loss : 0.1011, accuracy : 96.84\n",
            "iteration : 200, loss : 0.1008, accuracy : 96.86\n",
            "iteration : 250, loss : 0.1021, accuracy : 96.80\n",
            "iteration : 300, loss : 0.1022, accuracy : 96.81\n",
            "iteration : 350, loss : 0.1033, accuracy : 96.78\n",
            "Epoch :  95, training loss : 0.1050, training accuracy : 96.72, test loss : 1.5818, test accuracy : 71.98\n",
            "\n",
            "Epoch: 96\n",
            "iteration :  50, loss : 0.1091, accuracy : 96.50\n",
            "iteration : 100, loss : 0.1016, accuracy : 96.85\n",
            "iteration : 150, loss : 0.0995, accuracy : 96.90\n",
            "iteration : 200, loss : 0.1002, accuracy : 96.87\n",
            "iteration : 250, loss : 0.1000, accuracy : 96.90\n",
            "iteration : 300, loss : 0.0988, accuracy : 96.93\n",
            "iteration : 350, loss : 0.0999, accuracy : 96.90\n",
            "Epoch :  96, training loss : 0.1002, training accuracy : 96.89, test loss : 1.5452, test accuracy : 72.11\n",
            "\n",
            "Epoch: 97\n",
            "iteration :  50, loss : 0.0930, accuracy : 97.27\n",
            "iteration : 100, loss : 0.0903, accuracy : 97.27\n",
            "iteration : 150, loss : 0.0928, accuracy : 97.19\n",
            "iteration : 200, loss : 0.0956, accuracy : 97.06\n",
            "iteration : 250, loss : 0.0939, accuracy : 97.12\n",
            "iteration : 300, loss : 0.0950, accuracy : 97.12\n",
            "iteration : 350, loss : 0.0965, accuracy : 97.06\n",
            "Epoch :  97, training loss : 0.0962, training accuracy : 97.06, test loss : 1.5509, test accuracy : 71.89\n",
            "\n",
            "Epoch: 98\n",
            "iteration :  50, loss : 0.0956, accuracy : 96.95\n",
            "iteration : 100, loss : 0.0915, accuracy : 97.17\n",
            "iteration : 150, loss : 0.0943, accuracy : 97.11\n",
            "iteration : 200, loss : 0.0940, accuracy : 97.18\n",
            "iteration : 250, loss : 0.0941, accuracy : 97.14\n",
            "iteration : 300, loss : 0.0936, accuracy : 97.14\n",
            "iteration : 350, loss : 0.0944, accuracy : 97.12\n",
            "Epoch :  98, training loss : 0.0945, training accuracy : 97.10, test loss : 1.5327, test accuracy : 72.58\n",
            "\n",
            "Epoch: 99\n",
            "iteration :  50, loss : 0.0974, accuracy : 97.12\n",
            "iteration : 100, loss : 0.0959, accuracy : 97.14\n",
            "iteration : 150, loss : 0.0965, accuracy : 97.17\n",
            "iteration : 200, loss : 0.0971, accuracy : 97.14\n",
            "iteration : 250, loss : 0.0969, accuracy : 97.12\n",
            "iteration : 300, loss : 0.0954, accuracy : 97.14\n",
            "iteration : 350, loss : 0.0960, accuracy : 97.08\n",
            "Epoch :  99, training loss : 0.0976, training accuracy : 97.02, test loss : 1.5206, test accuracy : 72.61\n",
            "\n",
            "Epoch: 100\n",
            "iteration :  50, loss : 0.0848, accuracy : 97.64\n",
            "iteration : 100, loss : 0.0896, accuracy : 97.46\n",
            "iteration : 150, loss : 0.0884, accuracy : 97.42\n",
            "iteration : 200, loss : 0.0883, accuracy : 97.39\n",
            "iteration : 250, loss : 0.0883, accuracy : 97.38\n",
            "iteration : 300, loss : 0.0898, accuracy : 97.28\n",
            "iteration : 350, loss : 0.0910, accuracy : 97.25\n",
            "Epoch : 100, training loss : 0.0921, training accuracy : 97.21, test loss : 1.5518, test accuracy : 72.87\n",
            "\n",
            "Epoch: 101\n",
            "iteration :  50, loss : 0.0992, accuracy : 97.14\n",
            "iteration : 100, loss : 0.0927, accuracy : 97.20\n",
            "iteration : 150, loss : 0.0938, accuracy : 97.17\n",
            "iteration : 200, loss : 0.0914, accuracy : 97.21\n",
            "iteration : 250, loss : 0.0910, accuracy : 97.24\n",
            "iteration : 300, loss : 0.0916, accuracy : 97.24\n",
            "iteration : 350, loss : 0.0926, accuracy : 97.22\n",
            "Epoch : 101, training loss : 0.0923, training accuracy : 97.21, test loss : 1.5405, test accuracy : 72.16\n",
            "\n",
            "Epoch: 102\n",
            "iteration :  50, loss : 0.0939, accuracy : 97.27\n",
            "iteration : 100, loss : 0.0932, accuracy : 97.20\n",
            "iteration : 150, loss : 0.0902, accuracy : 97.24\n",
            "iteration : 200, loss : 0.0919, accuracy : 97.26\n",
            "iteration : 250, loss : 0.0915, accuracy : 97.29\n",
            "iteration : 300, loss : 0.0922, accuracy : 97.24\n",
            "iteration : 350, loss : 0.0903, accuracy : 97.29\n",
            "Epoch : 102, training loss : 0.0915, training accuracy : 97.27, test loss : 1.5810, test accuracy : 72.10\n",
            "\n",
            "Epoch: 103\n",
            "iteration :  50, loss : 0.0791, accuracy : 97.73\n",
            "iteration : 100, loss : 0.0759, accuracy : 97.68\n",
            "iteration : 150, loss : 0.0769, accuracy : 97.65\n",
            "iteration : 200, loss : 0.0779, accuracy : 97.58\n",
            "iteration : 250, loss : 0.0794, accuracy : 97.58\n",
            "iteration : 300, loss : 0.0820, accuracy : 97.51\n",
            "iteration : 350, loss : 0.0826, accuracy : 97.50\n",
            "Epoch : 103, training loss : 0.0829, training accuracy : 97.51, test loss : 1.5506, test accuracy : 72.37\n",
            "\n",
            "Epoch: 104\n",
            "iteration :  50, loss : 0.0811, accuracy : 97.72\n",
            "iteration : 100, loss : 0.0852, accuracy : 97.54\n",
            "iteration : 150, loss : 0.0871, accuracy : 97.46\n",
            "iteration : 200, loss : 0.0853, accuracy : 97.54\n",
            "iteration : 250, loss : 0.0858, accuracy : 97.49\n",
            "iteration : 300, loss : 0.0871, accuracy : 97.45\n",
            "iteration : 350, loss : 0.0880, accuracy : 97.41\n",
            "Epoch : 104, training loss : 0.0886, training accuracy : 97.36, test loss : 1.5698, test accuracy : 72.36\n",
            "\n",
            "Epoch: 105\n",
            "iteration :  50, loss : 0.0872, accuracy : 97.34\n",
            "iteration : 100, loss : 0.0808, accuracy : 97.47\n",
            "iteration : 150, loss : 0.0861, accuracy : 97.33\n",
            "iteration : 200, loss : 0.0900, accuracy : 97.18\n",
            "iteration : 250, loss : 0.0903, accuracy : 97.22\n",
            "iteration : 300, loss : 0.0912, accuracy : 97.22\n",
            "iteration : 350, loss : 0.0919, accuracy : 97.19\n",
            "Epoch : 105, training loss : 0.0915, training accuracy : 97.21, test loss : 1.5304, test accuracy : 72.61\n",
            "\n",
            "Epoch: 106\n",
            "iteration :  50, loss : 0.0717, accuracy : 97.83\n",
            "iteration : 100, loss : 0.0765, accuracy : 97.76\n",
            "iteration : 150, loss : 0.0764, accuracy : 97.73\n",
            "iteration : 200, loss : 0.0787, accuracy : 97.64\n",
            "iteration : 250, loss : 0.0790, accuracy : 97.66\n",
            "iteration : 300, loss : 0.0799, accuracy : 97.61\n",
            "iteration : 350, loss : 0.0830, accuracy : 97.52\n",
            "Epoch : 106, training loss : 0.0832, training accuracy : 97.51, test loss : 1.5548, test accuracy : 72.50\n",
            "\n",
            "Epoch: 107\n",
            "iteration :  50, loss : 0.0731, accuracy : 97.75\n",
            "iteration : 100, loss : 0.0757, accuracy : 97.59\n",
            "iteration : 150, loss : 0.0777, accuracy : 97.57\n",
            "iteration : 200, loss : 0.0803, accuracy : 97.53\n",
            "iteration : 250, loss : 0.0820, accuracy : 97.49\n",
            "iteration : 300, loss : 0.0824, accuracy : 97.49\n",
            "iteration : 350, loss : 0.0838, accuracy : 97.46\n",
            "Epoch : 107, training loss : 0.0835, training accuracy : 97.47, test loss : 1.5449, test accuracy : 72.87\n",
            "\n",
            "Epoch: 108\n",
            "iteration :  50, loss : 0.0823, accuracy : 97.55\n",
            "iteration : 100, loss : 0.0796, accuracy : 97.67\n",
            "iteration : 150, loss : 0.0831, accuracy : 97.49\n",
            "iteration : 200, loss : 0.0846, accuracy : 97.48\n",
            "iteration : 250, loss : 0.0848, accuracy : 97.49\n",
            "iteration : 300, loss : 0.0839, accuracy : 97.49\n",
            "iteration : 350, loss : 0.0826, accuracy : 97.54\n",
            "Epoch : 108, training loss : 0.0823, training accuracy : 97.53, test loss : 1.5198, test accuracy : 72.91\n",
            "\n",
            "Epoch: 109\n",
            "iteration :  50, loss : 0.0715, accuracy : 97.94\n",
            "iteration : 100, loss : 0.0774, accuracy : 97.66\n",
            "iteration : 150, loss : 0.0783, accuracy : 97.54\n",
            "iteration : 200, loss : 0.0814, accuracy : 97.48\n",
            "iteration : 250, loss : 0.0817, accuracy : 97.50\n",
            "iteration : 300, loss : 0.0825, accuracy : 97.49\n",
            "iteration : 350, loss : 0.0833, accuracy : 97.47\n",
            "Epoch : 109, training loss : 0.0838, training accuracy : 97.47, test loss : 1.5829, test accuracy : 72.51\n",
            "\n",
            "Epoch: 110\n",
            "iteration :  50, loss : 0.0713, accuracy : 97.98\n",
            "iteration : 100, loss : 0.0747, accuracy : 97.77\n",
            "iteration : 150, loss : 0.0797, accuracy : 97.66\n",
            "iteration : 200, loss : 0.0832, accuracy : 97.50\n",
            "iteration : 250, loss : 0.0848, accuracy : 97.45\n",
            "iteration : 300, loss : 0.0837, accuracy : 97.47\n",
            "iteration : 350, loss : 0.0829, accuracy : 97.50\n",
            "Epoch : 110, training loss : 0.0842, training accuracy : 97.44, test loss : 1.5805, test accuracy : 72.37\n",
            "\n",
            "Epoch: 111\n",
            "iteration :  50, loss : 0.0877, accuracy : 97.28\n",
            "iteration : 100, loss : 0.0820, accuracy : 97.52\n",
            "iteration : 150, loss : 0.0827, accuracy : 97.49\n",
            "iteration : 200, loss : 0.0849, accuracy : 97.41\n",
            "iteration : 250, loss : 0.0856, accuracy : 97.37\n",
            "iteration : 300, loss : 0.0852, accuracy : 97.40\n",
            "iteration : 350, loss : 0.0866, accuracy : 97.32\n",
            "Epoch : 111, training loss : 0.0863, training accuracy : 97.37, test loss : 1.5548, test accuracy : 72.11\n",
            "\n",
            "Epoch: 112\n",
            "iteration :  50, loss : 0.0819, accuracy : 97.66\n",
            "iteration : 100, loss : 0.0807, accuracy : 97.71\n",
            "iteration : 150, loss : 0.0792, accuracy : 97.63\n",
            "iteration : 200, loss : 0.0802, accuracy : 97.61\n",
            "iteration : 250, loss : 0.0789, accuracy : 97.64\n",
            "iteration : 300, loss : 0.0776, accuracy : 97.69\n",
            "iteration : 350, loss : 0.0760, accuracy : 97.72\n",
            "Epoch : 112, training loss : 0.0756, training accuracy : 97.75, test loss : 1.5384, test accuracy : 72.37\n",
            "\n",
            "Epoch: 113\n",
            "iteration :  50, loss : 0.0774, accuracy : 97.69\n",
            "iteration : 100, loss : 0.0780, accuracy : 97.71\n",
            "iteration : 150, loss : 0.0811, accuracy : 97.66\n",
            "iteration : 200, loss : 0.0793, accuracy : 97.69\n",
            "iteration : 250, loss : 0.0816, accuracy : 97.58\n",
            "iteration : 300, loss : 0.0827, accuracy : 97.57\n",
            "iteration : 350, loss : 0.0819, accuracy : 97.57\n",
            "Epoch : 113, training loss : 0.0817, training accuracy : 97.55, test loss : 1.5451, test accuracy : 72.40\n",
            "\n",
            "Epoch: 114\n",
            "iteration :  50, loss : 0.0685, accuracy : 98.00\n",
            "iteration : 100, loss : 0.0686, accuracy : 97.92\n",
            "iteration : 150, loss : 0.0753, accuracy : 97.71\n",
            "iteration : 200, loss : 0.0767, accuracy : 97.65\n",
            "iteration : 250, loss : 0.0778, accuracy : 97.61\n",
            "iteration : 300, loss : 0.0776, accuracy : 97.63\n",
            "iteration : 350, loss : 0.0789, accuracy : 97.61\n",
            "Epoch : 114, training loss : 0.0786, training accuracy : 97.61, test loss : 1.5743, test accuracy : 72.42\n",
            "\n",
            "Epoch: 115\n",
            "iteration :  50, loss : 0.0666, accuracy : 97.98\n",
            "iteration : 100, loss : 0.0695, accuracy : 97.98\n",
            "iteration : 150, loss : 0.0727, accuracy : 97.87\n",
            "iteration : 200, loss : 0.0728, accuracy : 97.87\n",
            "iteration : 250, loss : 0.0726, accuracy : 97.85\n",
            "iteration : 300, loss : 0.0736, accuracy : 97.80\n",
            "iteration : 350, loss : 0.0743, accuracy : 97.78\n",
            "Epoch : 115, training loss : 0.0743, training accuracy : 97.79, test loss : 1.5717, test accuracy : 72.42\n",
            "\n",
            "Epoch: 116\n",
            "iteration :  50, loss : 0.0834, accuracy : 97.56\n",
            "iteration : 100, loss : 0.0724, accuracy : 97.74\n",
            "iteration : 150, loss : 0.0703, accuracy : 97.83\n",
            "iteration : 200, loss : 0.0713, accuracy : 97.82\n",
            "iteration : 250, loss : 0.0716, accuracy : 97.77\n",
            "iteration : 300, loss : 0.0712, accuracy : 97.79\n",
            "iteration : 350, loss : 0.0702, accuracy : 97.82\n",
            "Epoch : 116, training loss : 0.0709, training accuracy : 97.81, test loss : 1.5537, test accuracy : 72.28\n",
            "\n",
            "Epoch: 117\n",
            "iteration :  50, loss : 0.0748, accuracy : 97.83\n",
            "iteration : 100, loss : 0.0799, accuracy : 97.66\n",
            "iteration : 150, loss : 0.0815, accuracy : 97.55\n",
            "iteration : 200, loss : 0.0817, accuracy : 97.54\n",
            "iteration : 250, loss : 0.0803, accuracy : 97.57\n",
            "iteration : 300, loss : 0.0793, accuracy : 97.58\n",
            "iteration : 350, loss : 0.0797, accuracy : 97.57\n",
            "Epoch : 117, training loss : 0.0794, training accuracy : 97.59, test loss : 1.5713, test accuracy : 72.94\n",
            "\n",
            "Epoch: 118\n",
            "iteration :  50, loss : 0.0709, accuracy : 97.77\n",
            "iteration : 100, loss : 0.0699, accuracy : 97.84\n",
            "iteration : 150, loss : 0.0715, accuracy : 97.86\n",
            "iteration : 200, loss : 0.0742, accuracy : 97.80\n",
            "iteration : 250, loss : 0.0761, accuracy : 97.72\n",
            "iteration : 300, loss : 0.0755, accuracy : 97.73\n",
            "iteration : 350, loss : 0.0741, accuracy : 97.77\n",
            "Epoch : 118, training loss : 0.0736, training accuracy : 97.76, test loss : 1.5382, test accuracy : 73.06\n",
            "\n",
            "Epoch: 119\n",
            "iteration :  50, loss : 0.0759, accuracy : 97.69\n",
            "iteration : 100, loss : 0.0736, accuracy : 97.74\n",
            "iteration : 150, loss : 0.0740, accuracy : 97.73\n",
            "iteration : 200, loss : 0.0729, accuracy : 97.73\n",
            "iteration : 250, loss : 0.0716, accuracy : 97.79\n",
            "iteration : 300, loss : 0.0725, accuracy : 97.76\n",
            "iteration : 350, loss : 0.0740, accuracy : 97.70\n",
            "Epoch : 119, training loss : 0.0735, training accuracy : 97.72, test loss : 1.5522, test accuracy : 72.75\n",
            "\n",
            "Epoch: 120\n",
            "iteration :  50, loss : 0.0698, accuracy : 97.95\n",
            "iteration : 100, loss : 0.0713, accuracy : 97.93\n",
            "iteration : 150, loss : 0.0714, accuracy : 97.90\n",
            "iteration : 200, loss : 0.0729, accuracy : 97.77\n",
            "iteration : 250, loss : 0.0740, accuracy : 97.74\n",
            "iteration : 300, loss : 0.0742, accuracy : 97.74\n",
            "iteration : 350, loss : 0.0741, accuracy : 97.75\n",
            "Epoch : 120, training loss : 0.0735, training accuracy : 97.76, test loss : 1.5787, test accuracy : 72.94\n",
            "\n",
            "Epoch: 121\n",
            "iteration :  50, loss : 0.0606, accuracy : 97.91\n",
            "iteration : 100, loss : 0.0630, accuracy : 97.91\n",
            "iteration : 150, loss : 0.0630, accuracy : 97.95\n",
            "iteration : 200, loss : 0.0627, accuracy : 97.99\n",
            "iteration : 250, loss : 0.0652, accuracy : 97.95\n",
            "iteration : 300, loss : 0.0656, accuracy : 97.92\n",
            "iteration : 350, loss : 0.0660, accuracy : 97.92\n",
            "Epoch : 121, training loss : 0.0662, training accuracy : 97.92, test loss : 1.5448, test accuracy : 72.62\n",
            "\n",
            "Epoch: 122\n",
            "iteration :  50, loss : 0.0762, accuracy : 97.52\n",
            "iteration : 100, loss : 0.0699, accuracy : 97.79\n",
            "iteration : 150, loss : 0.0699, accuracy : 97.80\n",
            "iteration : 200, loss : 0.0688, accuracy : 97.90\n",
            "iteration : 250, loss : 0.0690, accuracy : 97.89\n",
            "iteration : 300, loss : 0.0706, accuracy : 97.80\n",
            "iteration : 350, loss : 0.0710, accuracy : 97.77\n",
            "Epoch : 122, training loss : 0.0718, training accuracy : 97.74, test loss : 1.5549, test accuracy : 73.13\n",
            "\n",
            "Epoch: 123\n",
            "iteration :  50, loss : 0.0594, accuracy : 98.20\n",
            "iteration : 100, loss : 0.0627, accuracy : 98.11\n",
            "iteration : 150, loss : 0.0647, accuracy : 98.11\n",
            "iteration : 200, loss : 0.0652, accuracy : 98.04\n",
            "iteration : 250, loss : 0.0665, accuracy : 98.00\n",
            "iteration : 300, loss : 0.0665, accuracy : 97.99\n",
            "iteration : 350, loss : 0.0664, accuracy : 97.99\n",
            "Epoch : 123, training loss : 0.0657, training accuracy : 97.99, test loss : 1.5602, test accuracy : 72.83\n",
            "\n",
            "Epoch: 124\n",
            "iteration :  50, loss : 0.0662, accuracy : 97.94\n",
            "iteration : 100, loss : 0.0695, accuracy : 97.84\n",
            "iteration : 150, loss : 0.0721, accuracy : 97.73\n",
            "iteration : 200, loss : 0.0715, accuracy : 97.73\n",
            "iteration : 250, loss : 0.0723, accuracy : 97.74\n",
            "iteration : 300, loss : 0.0724, accuracy : 97.73\n",
            "iteration : 350, loss : 0.0717, accuracy : 97.75\n",
            "Epoch : 124, training loss : 0.0716, training accuracy : 97.74, test loss : 1.5579, test accuracy : 72.97\n",
            "\n",
            "Epoch: 125\n",
            "iteration :  50, loss : 0.0575, accuracy : 98.38\n",
            "iteration : 100, loss : 0.0607, accuracy : 98.21\n",
            "iteration : 150, loss : 0.0599, accuracy : 98.24\n",
            "iteration : 200, loss : 0.0620, accuracy : 98.14\n",
            "iteration : 250, loss : 0.0636, accuracy : 98.03\n",
            "iteration : 300, loss : 0.0637, accuracy : 98.06\n",
            "iteration : 350, loss : 0.0635, accuracy : 98.06\n",
            "Epoch : 125, training loss : 0.0640, training accuracy : 98.04, test loss : 1.5585, test accuracy : 72.90\n",
            "\n",
            "Epoch: 126\n",
            "iteration :  50, loss : 0.0658, accuracy : 97.88\n",
            "iteration : 100, loss : 0.0643, accuracy : 98.02\n",
            "iteration : 150, loss : 0.0630, accuracy : 98.10\n",
            "iteration : 200, loss : 0.0613, accuracy : 98.12\n",
            "iteration : 250, loss : 0.0625, accuracy : 98.09\n",
            "iteration : 300, loss : 0.0639, accuracy : 98.07\n",
            "iteration : 350, loss : 0.0637, accuracy : 98.07\n",
            "Epoch : 126, training loss : 0.0641, training accuracy : 98.05, test loss : 1.5653, test accuracy : 73.11\n",
            "\n",
            "Epoch: 127\n",
            "iteration :  50, loss : 0.0606, accuracy : 98.11\n",
            "iteration : 100, loss : 0.0598, accuracy : 98.20\n",
            "iteration : 150, loss : 0.0618, accuracy : 98.16\n",
            "iteration : 200, loss : 0.0633, accuracy : 98.12\n",
            "iteration : 250, loss : 0.0626, accuracy : 98.12\n",
            "iteration : 300, loss : 0.0632, accuracy : 98.10\n",
            "iteration : 350, loss : 0.0645, accuracy : 98.05\n",
            "Epoch : 127, training loss : 0.0650, training accuracy : 98.05, test loss : 1.5646, test accuracy : 72.58\n",
            "\n",
            "Epoch: 128\n",
            "iteration :  50, loss : 0.0646, accuracy : 97.91\n",
            "iteration : 100, loss : 0.0651, accuracy : 97.91\n",
            "iteration : 150, loss : 0.0630, accuracy : 98.02\n",
            "iteration : 200, loss : 0.0628, accuracy : 98.01\n",
            "iteration : 250, loss : 0.0629, accuracy : 97.98\n",
            "iteration : 300, loss : 0.0631, accuracy : 98.01\n",
            "iteration : 350, loss : 0.0623, accuracy : 98.02\n",
            "Epoch : 128, training loss : 0.0633, training accuracy : 98.01, test loss : 1.5744, test accuracy : 72.83\n",
            "\n",
            "Epoch: 129\n",
            "iteration :  50, loss : 0.0670, accuracy : 98.00\n",
            "iteration : 100, loss : 0.0639, accuracy : 98.12\n",
            "iteration : 150, loss : 0.0637, accuracy : 98.08\n",
            "iteration : 200, loss : 0.0622, accuracy : 98.12\n",
            "iteration : 250, loss : 0.0614, accuracy : 98.14\n",
            "iteration : 300, loss : 0.0631, accuracy : 98.11\n",
            "iteration : 350, loss : 0.0641, accuracy : 98.08\n",
            "Epoch : 129, training loss : 0.0631, training accuracy : 98.09, test loss : 1.5513, test accuracy : 72.89\n",
            "\n",
            "Epoch: 130\n",
            "iteration :  50, loss : 0.0691, accuracy : 97.92\n",
            "iteration : 100, loss : 0.0678, accuracy : 97.93\n",
            "iteration : 150, loss : 0.0674, accuracy : 97.94\n",
            "iteration : 200, loss : 0.0651, accuracy : 98.02\n",
            "iteration : 250, loss : 0.0657, accuracy : 98.03\n",
            "iteration : 300, loss : 0.0655, accuracy : 98.03\n",
            "iteration : 350, loss : 0.0651, accuracy : 98.04\n",
            "Epoch : 130, training loss : 0.0646, training accuracy : 98.07, test loss : 1.5543, test accuracy : 72.88\n",
            "\n",
            "Epoch: 131\n",
            "iteration :  50, loss : 0.0665, accuracy : 98.14\n",
            "iteration : 100, loss : 0.0655, accuracy : 98.09\n",
            "iteration : 150, loss : 0.0659, accuracy : 98.02\n",
            "iteration : 200, loss : 0.0655, accuracy : 98.09\n",
            "iteration : 250, loss : 0.0648, accuracy : 98.09\n",
            "iteration : 300, loss : 0.0653, accuracy : 98.07\n",
            "iteration : 350, loss : 0.0640, accuracy : 98.08\n",
            "Epoch : 131, training loss : 0.0634, training accuracy : 98.08, test loss : 1.5649, test accuracy : 73.04\n",
            "\n",
            "Epoch: 132\n",
            "iteration :  50, loss : 0.0542, accuracy : 98.36\n",
            "iteration : 100, loss : 0.0564, accuracy : 98.34\n",
            "iteration : 150, loss : 0.0562, accuracy : 98.32\n",
            "iteration : 200, loss : 0.0567, accuracy : 98.28\n",
            "iteration : 250, loss : 0.0587, accuracy : 98.22\n",
            "iteration : 300, loss : 0.0603, accuracy : 98.19\n",
            "iteration : 350, loss : 0.0597, accuracy : 98.19\n",
            "Epoch : 132, training loss : 0.0602, training accuracy : 98.16, test loss : 1.5761, test accuracy : 72.82\n",
            "\n",
            "Epoch: 133\n",
            "iteration :  50, loss : 0.0608, accuracy : 98.20\n",
            "iteration : 100, loss : 0.0649, accuracy : 98.05\n",
            "iteration : 150, loss : 0.0633, accuracy : 98.18\n",
            "iteration : 200, loss : 0.0647, accuracy : 98.12\n",
            "iteration : 250, loss : 0.0632, accuracy : 98.14\n",
            "iteration : 300, loss : 0.0628, accuracy : 98.13\n",
            "iteration : 350, loss : 0.0626, accuracy : 98.11\n",
            "Epoch : 133, training loss : 0.0631, training accuracy : 98.09, test loss : 1.5567, test accuracy : 73.45\n",
            "\n",
            "Epoch: 134\n",
            "iteration :  50, loss : 0.0546, accuracy : 98.41\n",
            "iteration : 100, loss : 0.0536, accuracy : 98.35\n",
            "iteration : 150, loss : 0.0539, accuracy : 98.31\n",
            "iteration : 200, loss : 0.0539, accuracy : 98.32\n",
            "iteration : 250, loss : 0.0541, accuracy : 98.36\n",
            "iteration : 300, loss : 0.0558, accuracy : 98.30\n",
            "iteration : 350, loss : 0.0569, accuracy : 98.27\n",
            "Epoch : 134, training loss : 0.0567, training accuracy : 98.30, test loss : 1.5241, test accuracy : 73.78\n",
            "\n",
            "Epoch: 135\n",
            "iteration :  50, loss : 0.0512, accuracy : 98.62\n",
            "iteration : 100, loss : 0.0548, accuracy : 98.53\n",
            "iteration : 150, loss : 0.0558, accuracy : 98.38\n",
            "iteration : 200, loss : 0.0562, accuracy : 98.32\n",
            "iteration : 250, loss : 0.0552, accuracy : 98.33\n",
            "iteration : 300, loss : 0.0559, accuracy : 98.30\n",
            "iteration : 350, loss : 0.0567, accuracy : 98.25\n",
            "Epoch : 135, training loss : 0.0570, training accuracy : 98.28, test loss : 1.5403, test accuracy : 73.50\n",
            "\n",
            "Epoch: 136\n",
            "iteration :  50, loss : 0.0579, accuracy : 98.23\n",
            "iteration : 100, loss : 0.0578, accuracy : 98.16\n",
            "iteration : 150, loss : 0.0555, accuracy : 98.28\n",
            "iteration : 200, loss : 0.0569, accuracy : 98.24\n",
            "iteration : 250, loss : 0.0556, accuracy : 98.33\n",
            "iteration : 300, loss : 0.0559, accuracy : 98.31\n",
            "iteration : 350, loss : 0.0548, accuracy : 98.35\n",
            "Epoch : 136, training loss : 0.0545, training accuracy : 98.35, test loss : 1.5329, test accuracy : 73.35\n",
            "\n",
            "Epoch: 137\n",
            "iteration :  50, loss : 0.0571, accuracy : 98.30\n",
            "iteration : 100, loss : 0.0585, accuracy : 98.24\n",
            "iteration : 150, loss : 0.0575, accuracy : 98.29\n",
            "iteration : 200, loss : 0.0572, accuracy : 98.25\n",
            "iteration : 250, loss : 0.0581, accuracy : 98.25\n",
            "iteration : 300, loss : 0.0582, accuracy : 98.22\n",
            "iteration : 350, loss : 0.0583, accuracy : 98.22\n",
            "Epoch : 137, training loss : 0.0578, training accuracy : 98.23, test loss : 1.5318, test accuracy : 73.41\n",
            "\n",
            "Epoch: 138\n",
            "iteration :  50, loss : 0.0492, accuracy : 98.64\n",
            "iteration : 100, loss : 0.0562, accuracy : 98.28\n",
            "iteration : 150, loss : 0.0567, accuracy : 98.30\n",
            "iteration : 200, loss : 0.0580, accuracy : 98.26\n",
            "iteration : 250, loss : 0.0587, accuracy : 98.25\n",
            "iteration : 300, loss : 0.0581, accuracy : 98.25\n",
            "iteration : 350, loss : 0.0578, accuracy : 98.27\n",
            "Epoch : 138, training loss : 0.0580, training accuracy : 98.25, test loss : 1.5334, test accuracy : 73.63\n",
            "\n",
            "Epoch: 139\n",
            "iteration :  50, loss : 0.0607, accuracy : 98.12\n",
            "iteration : 100, loss : 0.0573, accuracy : 98.23\n",
            "iteration : 150, loss : 0.0567, accuracy : 98.30\n",
            "iteration : 200, loss : 0.0542, accuracy : 98.37\n",
            "iteration : 250, loss : 0.0540, accuracy : 98.39\n",
            "iteration : 300, loss : 0.0535, accuracy : 98.38\n",
            "iteration : 350, loss : 0.0541, accuracy : 98.36\n",
            "Epoch : 139, training loss : 0.0544, training accuracy : 98.35, test loss : 1.5565, test accuracy : 73.36\n",
            "\n",
            "Epoch: 140\n",
            "iteration :  50, loss : 0.0541, accuracy : 98.28\n",
            "iteration : 100, loss : 0.0529, accuracy : 98.38\n",
            "iteration : 150, loss : 0.0559, accuracy : 98.25\n",
            "iteration : 200, loss : 0.0598, accuracy : 98.18\n",
            "iteration : 250, loss : 0.0599, accuracy : 98.15\n",
            "iteration : 300, loss : 0.0585, accuracy : 98.19\n",
            "iteration : 350, loss : 0.0585, accuracy : 98.20\n",
            "Epoch : 140, training loss : 0.0582, training accuracy : 98.19, test loss : 1.5629, test accuracy : 73.33\n",
            "\n",
            "Epoch: 141\n",
            "iteration :  50, loss : 0.0531, accuracy : 98.52\n",
            "iteration : 100, loss : 0.0532, accuracy : 98.45\n",
            "iteration : 150, loss : 0.0560, accuracy : 98.38\n",
            "iteration : 200, loss : 0.0570, accuracy : 98.32\n",
            "iteration : 250, loss : 0.0552, accuracy : 98.39\n",
            "iteration : 300, loss : 0.0559, accuracy : 98.39\n",
            "iteration : 350, loss : 0.0555, accuracy : 98.39\n",
            "Epoch : 141, training loss : 0.0550, training accuracy : 98.39, test loss : 1.5331, test accuracy : 73.17\n",
            "\n",
            "Epoch: 142\n",
            "iteration :  50, loss : 0.0502, accuracy : 98.44\n",
            "iteration : 100, loss : 0.0534, accuracy : 98.35\n",
            "iteration : 150, loss : 0.0543, accuracy : 98.34\n",
            "iteration : 200, loss : 0.0539, accuracy : 98.36\n",
            "iteration : 250, loss : 0.0548, accuracy : 98.35\n",
            "iteration : 300, loss : 0.0548, accuracy : 98.32\n",
            "iteration : 350, loss : 0.0549, accuracy : 98.30\n",
            "Epoch : 142, training loss : 0.0547, training accuracy : 98.30, test loss : 1.5528, test accuracy : 73.23\n",
            "\n",
            "Epoch: 143\n",
            "iteration :  50, loss : 0.0540, accuracy : 98.33\n",
            "iteration : 100, loss : 0.0535, accuracy : 98.37\n",
            "iteration : 150, loss : 0.0530, accuracy : 98.42\n",
            "iteration : 200, loss : 0.0525, accuracy : 98.44\n",
            "iteration : 250, loss : 0.0515, accuracy : 98.45\n",
            "iteration : 300, loss : 0.0531, accuracy : 98.38\n",
            "iteration : 350, loss : 0.0533, accuracy : 98.39\n",
            "Epoch : 143, training loss : 0.0522, training accuracy : 98.40, test loss : 1.5525, test accuracy : 73.20\n",
            "\n",
            "Epoch: 144\n",
            "iteration :  50, loss : 0.0525, accuracy : 98.28\n",
            "iteration : 100, loss : 0.0519, accuracy : 98.38\n",
            "iteration : 150, loss : 0.0517, accuracy : 98.35\n",
            "iteration : 200, loss : 0.0531, accuracy : 98.36\n",
            "iteration : 250, loss : 0.0530, accuracy : 98.38\n",
            "iteration : 300, loss : 0.0533, accuracy : 98.38\n",
            "iteration : 350, loss : 0.0535, accuracy : 98.38\n",
            "Epoch : 144, training loss : 0.0539, training accuracy : 98.34, test loss : 1.5209, test accuracy : 73.52\n",
            "\n",
            "Epoch: 145\n",
            "iteration :  50, loss : 0.0496, accuracy : 98.66\n",
            "iteration : 100, loss : 0.0498, accuracy : 98.59\n",
            "iteration : 150, loss : 0.0518, accuracy : 98.51\n",
            "iteration : 200, loss : 0.0506, accuracy : 98.50\n",
            "iteration : 250, loss : 0.0504, accuracy : 98.51\n",
            "iteration : 300, loss : 0.0506, accuracy : 98.51\n",
            "iteration : 350, loss : 0.0512, accuracy : 98.44\n",
            "Epoch : 145, training loss : 0.0515, training accuracy : 98.42, test loss : 1.5635, test accuracy : 73.34\n",
            "\n",
            "Epoch: 146\n",
            "iteration :  50, loss : 0.0472, accuracy : 98.55\n",
            "iteration : 100, loss : 0.0493, accuracy : 98.51\n",
            "iteration : 150, loss : 0.0481, accuracy : 98.57\n",
            "iteration : 200, loss : 0.0467, accuracy : 98.64\n",
            "iteration : 250, loss : 0.0465, accuracy : 98.65\n",
            "iteration : 300, loss : 0.0474, accuracy : 98.62\n",
            "iteration : 350, loss : 0.0492, accuracy : 98.57\n",
            "Epoch : 146, training loss : 0.0499, training accuracy : 98.54, test loss : 1.5286, test accuracy : 73.73\n",
            "\n",
            "Epoch: 147\n",
            "iteration :  50, loss : 0.0538, accuracy : 98.45\n",
            "iteration : 100, loss : 0.0505, accuracy : 98.52\n",
            "iteration : 150, loss : 0.0498, accuracy : 98.55\n",
            "iteration : 200, loss : 0.0496, accuracy : 98.56\n",
            "iteration : 250, loss : 0.0507, accuracy : 98.51\n",
            "iteration : 300, loss : 0.0498, accuracy : 98.53\n",
            "iteration : 350, loss : 0.0496, accuracy : 98.52\n",
            "Epoch : 147, training loss : 0.0504, training accuracy : 98.52, test loss : 1.5570, test accuracy : 73.53\n",
            "\n",
            "Epoch: 148\n",
            "iteration :  50, loss : 0.0535, accuracy : 98.19\n",
            "iteration : 100, loss : 0.0520, accuracy : 98.33\n",
            "iteration : 150, loss : 0.0504, accuracy : 98.42\n",
            "iteration : 200, loss : 0.0506, accuracy : 98.43\n",
            "iteration : 250, loss : 0.0499, accuracy : 98.43\n",
            "iteration : 300, loss : 0.0498, accuracy : 98.43\n",
            "iteration : 350, loss : 0.0505, accuracy : 98.39\n",
            "Epoch : 148, training loss : 0.0515, training accuracy : 98.38, test loss : 1.5634, test accuracy : 73.23\n",
            "\n",
            "Epoch: 149\n",
            "iteration :  50, loss : 0.0429, accuracy : 98.77\n",
            "iteration : 100, loss : 0.0483, accuracy : 98.48\n",
            "iteration : 150, loss : 0.0504, accuracy : 98.46\n",
            "iteration : 200, loss : 0.0499, accuracy : 98.46\n",
            "iteration : 250, loss : 0.0493, accuracy : 98.45\n",
            "iteration : 300, loss : 0.0501, accuracy : 98.42\n",
            "iteration : 350, loss : 0.0499, accuracy : 98.42\n",
            "Epoch : 149, training loss : 0.0505, training accuracy : 98.41, test loss : 1.5523, test accuracy : 73.48\n",
            "\n",
            "Epoch: 150\n",
            "iteration :  50, loss : 0.0427, accuracy : 98.80\n",
            "iteration : 100, loss : 0.0471, accuracy : 98.51\n",
            "iteration : 150, loss : 0.0473, accuracy : 98.48\n",
            "iteration : 200, loss : 0.0467, accuracy : 98.53\n",
            "iteration : 250, loss : 0.0462, accuracy : 98.55\n",
            "iteration : 300, loss : 0.0462, accuracy : 98.58\n",
            "iteration : 350, loss : 0.0464, accuracy : 98.57\n",
            "Epoch : 150, training loss : 0.0469, training accuracy : 98.55, test loss : 1.5190, test accuracy : 73.57\n",
            "\n",
            "Epoch: 151\n",
            "iteration :  50, loss : 0.0430, accuracy : 98.77\n",
            "iteration : 100, loss : 0.0436, accuracy : 98.61\n",
            "iteration : 150, loss : 0.0453, accuracy : 98.56\n",
            "iteration : 200, loss : 0.0476, accuracy : 98.50\n",
            "iteration : 250, loss : 0.0464, accuracy : 98.53\n",
            "iteration : 300, loss : 0.0465, accuracy : 98.54\n",
            "iteration : 350, loss : 0.0463, accuracy : 98.54\n",
            "Epoch : 151, training loss : 0.0469, training accuracy : 98.54, test loss : 1.5371, test accuracy : 73.61\n",
            "\n",
            "Epoch: 152\n",
            "iteration :  50, loss : 0.0422, accuracy : 98.73\n",
            "iteration : 100, loss : 0.0426, accuracy : 98.79\n",
            "iteration : 150, loss : 0.0410, accuracy : 98.76\n",
            "iteration : 200, loss : 0.0419, accuracy : 98.69\n",
            "iteration : 250, loss : 0.0422, accuracy : 98.68\n",
            "iteration : 300, loss : 0.0423, accuracy : 98.67\n",
            "iteration : 350, loss : 0.0422, accuracy : 98.67\n",
            "Epoch : 152, training loss : 0.0421, training accuracy : 98.67, test loss : 1.5415, test accuracy : 73.71\n",
            "\n",
            "Epoch: 153\n",
            "iteration :  50, loss : 0.0452, accuracy : 98.58\n",
            "iteration : 100, loss : 0.0424, accuracy : 98.70\n",
            "iteration : 150, loss : 0.0434, accuracy : 98.69\n",
            "iteration : 200, loss : 0.0441, accuracy : 98.66\n",
            "iteration : 250, loss : 0.0429, accuracy : 98.72\n",
            "iteration : 300, loss : 0.0426, accuracy : 98.75\n",
            "iteration : 350, loss : 0.0422, accuracy : 98.75\n",
            "Epoch : 153, training loss : 0.0422, training accuracy : 98.73, test loss : 1.5197, test accuracy : 73.34\n",
            "\n",
            "Epoch: 154\n",
            "iteration :  50, loss : 0.0458, accuracy : 98.55\n",
            "iteration : 100, loss : 0.0436, accuracy : 98.60\n",
            "iteration : 150, loss : 0.0444, accuracy : 98.66\n",
            "iteration : 200, loss : 0.0435, accuracy : 98.66\n",
            "iteration : 250, loss : 0.0434, accuracy : 98.68\n",
            "iteration : 300, loss : 0.0437, accuracy : 98.67\n",
            "iteration : 350, loss : 0.0435, accuracy : 98.65\n",
            "Epoch : 154, training loss : 0.0443, training accuracy : 98.65, test loss : 1.5138, test accuracy : 73.89\n",
            "\n",
            "Epoch: 155\n",
            "iteration :  50, loss : 0.0448, accuracy : 98.58\n",
            "iteration : 100, loss : 0.0464, accuracy : 98.54\n",
            "iteration : 150, loss : 0.0465, accuracy : 98.52\n",
            "iteration : 200, loss : 0.0459, accuracy : 98.51\n",
            "iteration : 250, loss : 0.0446, accuracy : 98.58\n",
            "iteration : 300, loss : 0.0451, accuracy : 98.60\n",
            "iteration : 350, loss : 0.0455, accuracy : 98.59\n",
            "Epoch : 155, training loss : 0.0455, training accuracy : 98.58, test loss : 1.5394, test accuracy : 73.60\n",
            "\n",
            "Epoch: 156\n",
            "iteration :  50, loss : 0.0387, accuracy : 98.89\n",
            "iteration : 100, loss : 0.0400, accuracy : 98.84\n",
            "iteration : 150, loss : 0.0416, accuracy : 98.77\n",
            "iteration : 200, loss : 0.0415, accuracy : 98.77\n",
            "iteration : 250, loss : 0.0428, accuracy : 98.72\n",
            "iteration : 300, loss : 0.0437, accuracy : 98.67\n",
            "iteration : 350, loss : 0.0438, accuracy : 98.65\n",
            "Epoch : 156, training loss : 0.0445, training accuracy : 98.63, test loss : 1.5511, test accuracy : 73.67\n",
            "\n",
            "Epoch: 157\n",
            "iteration :  50, loss : 0.0483, accuracy : 98.58\n",
            "iteration : 100, loss : 0.0447, accuracy : 98.70\n",
            "iteration : 150, loss : 0.0436, accuracy : 98.67\n",
            "iteration : 200, loss : 0.0444, accuracy : 98.62\n",
            "iteration : 250, loss : 0.0435, accuracy : 98.66\n",
            "iteration : 300, loss : 0.0439, accuracy : 98.64\n",
            "iteration : 350, loss : 0.0449, accuracy : 98.63\n",
            "Epoch : 157, training loss : 0.0444, training accuracy : 98.62, test loss : 1.5327, test accuracy : 73.78\n",
            "\n",
            "Epoch: 158\n",
            "iteration :  50, loss : 0.0404, accuracy : 98.77\n",
            "iteration : 100, loss : 0.0412, accuracy : 98.77\n",
            "iteration : 150, loss : 0.0393, accuracy : 98.84\n",
            "iteration : 200, loss : 0.0396, accuracy : 98.80\n",
            "iteration : 250, loss : 0.0407, accuracy : 98.77\n",
            "iteration : 300, loss : 0.0411, accuracy : 98.76\n",
            "iteration : 350, loss : 0.0408, accuracy : 98.78\n",
            "Epoch : 158, training loss : 0.0410, training accuracy : 98.76, test loss : 1.5386, test accuracy : 73.66\n",
            "\n",
            "Epoch: 159\n",
            "iteration :  50, loss : 0.0463, accuracy : 98.55\n",
            "iteration : 100, loss : 0.0429, accuracy : 98.72\n",
            "iteration : 150, loss : 0.0447, accuracy : 98.74\n",
            "iteration : 200, loss : 0.0462, accuracy : 98.68\n",
            "iteration : 250, loss : 0.0452, accuracy : 98.68\n",
            "iteration : 300, loss : 0.0448, accuracy : 98.69\n",
            "iteration : 350, loss : 0.0455, accuracy : 98.65\n",
            "Epoch : 159, training loss : 0.0450, training accuracy : 98.65, test loss : 1.5298, test accuracy : 74.10\n",
            "\n",
            "Epoch: 160\n",
            "iteration :  50, loss : 0.0411, accuracy : 98.81\n",
            "iteration : 100, loss : 0.0452, accuracy : 98.70\n",
            "iteration : 150, loss : 0.0425, accuracy : 98.78\n",
            "iteration : 200, loss : 0.0432, accuracy : 98.75\n",
            "iteration : 250, loss : 0.0431, accuracy : 98.73\n",
            "iteration : 300, loss : 0.0430, accuracy : 98.72\n",
            "iteration : 350, loss : 0.0433, accuracy : 98.70\n",
            "Epoch : 160, training loss : 0.0441, training accuracy : 98.68, test loss : 1.5376, test accuracy : 73.80\n",
            "\n",
            "Epoch: 161\n",
            "iteration :  50, loss : 0.0442, accuracy : 98.70\n",
            "iteration : 100, loss : 0.0449, accuracy : 98.73\n",
            "iteration : 150, loss : 0.0455, accuracy : 98.69\n",
            "iteration : 200, loss : 0.0465, accuracy : 98.62\n",
            "iteration : 250, loss : 0.0450, accuracy : 98.66\n",
            "iteration : 300, loss : 0.0441, accuracy : 98.68\n",
            "iteration : 350, loss : 0.0436, accuracy : 98.68\n",
            "Epoch : 161, training loss : 0.0431, training accuracy : 98.71, test loss : 1.5428, test accuracy : 73.70\n",
            "\n",
            "Epoch: 162\n",
            "iteration :  50, loss : 0.0404, accuracy : 98.75\n",
            "iteration : 100, loss : 0.0400, accuracy : 98.73\n",
            "iteration : 150, loss : 0.0410, accuracy : 98.73\n",
            "iteration : 200, loss : 0.0392, accuracy : 98.79\n",
            "iteration : 250, loss : 0.0399, accuracy : 98.78\n",
            "iteration : 300, loss : 0.0397, accuracy : 98.80\n",
            "iteration : 350, loss : 0.0396, accuracy : 98.81\n",
            "Epoch : 162, training loss : 0.0400, training accuracy : 98.78, test loss : 1.5586, test accuracy : 73.59\n",
            "\n",
            "Epoch: 163\n",
            "iteration :  50, loss : 0.0442, accuracy : 98.64\n",
            "iteration : 100, loss : 0.0422, accuracy : 98.73\n",
            "iteration : 150, loss : 0.0402, accuracy : 98.81\n",
            "iteration : 200, loss : 0.0419, accuracy : 98.73\n",
            "iteration : 250, loss : 0.0414, accuracy : 98.72\n",
            "iteration : 300, loss : 0.0408, accuracy : 98.74\n",
            "iteration : 350, loss : 0.0408, accuracy : 98.73\n",
            "Epoch : 163, training loss : 0.0411, training accuracy : 98.71, test loss : 1.5406, test accuracy : 73.64\n",
            "\n",
            "Epoch: 164\n",
            "iteration :  50, loss : 0.0384, accuracy : 98.81\n",
            "iteration : 100, loss : 0.0392, accuracy : 98.84\n",
            "iteration : 150, loss : 0.0396, accuracy : 98.83\n",
            "iteration : 200, loss : 0.0388, accuracy : 98.84\n",
            "iteration : 250, loss : 0.0397, accuracy : 98.86\n",
            "iteration : 300, loss : 0.0395, accuracy : 98.84\n",
            "iteration : 350, loss : 0.0400, accuracy : 98.83\n",
            "Epoch : 164, training loss : 0.0403, training accuracy : 98.81, test loss : 1.5718, test accuracy : 73.55\n",
            "\n",
            "Epoch: 165\n",
            "iteration :  50, loss : 0.0357, accuracy : 98.83\n",
            "iteration : 100, loss : 0.0384, accuracy : 98.84\n",
            "iteration : 150, loss : 0.0376, accuracy : 98.85\n",
            "iteration : 200, loss : 0.0373, accuracy : 98.83\n",
            "iteration : 250, loss : 0.0381, accuracy : 98.78\n",
            "iteration : 300, loss : 0.0386, accuracy : 98.77\n",
            "iteration : 350, loss : 0.0375, accuracy : 98.80\n",
            "Epoch : 165, training loss : 0.0380, training accuracy : 98.78, test loss : 1.5314, test accuracy : 73.85\n",
            "\n",
            "Epoch: 166\n",
            "iteration :  50, loss : 0.0361, accuracy : 98.91\n",
            "iteration : 100, loss : 0.0348, accuracy : 98.94\n",
            "iteration : 150, loss : 0.0362, accuracy : 98.92\n",
            "iteration : 200, loss : 0.0367, accuracy : 98.89\n",
            "iteration : 250, loss : 0.0370, accuracy : 98.89\n",
            "iteration : 300, loss : 0.0372, accuracy : 98.90\n",
            "iteration : 350, loss : 0.0384, accuracy : 98.87\n",
            "Epoch : 166, training loss : 0.0378, training accuracy : 98.88, test loss : 1.5403, test accuracy : 73.32\n",
            "\n",
            "Epoch: 167\n",
            "iteration :  50, loss : 0.0416, accuracy : 98.69\n",
            "iteration : 100, loss : 0.0378, accuracy : 98.82\n",
            "iteration : 150, loss : 0.0372, accuracy : 98.84\n",
            "iteration : 200, loss : 0.0357, accuracy : 98.91\n",
            "iteration : 250, loss : 0.0347, accuracy : 98.96\n",
            "iteration : 300, loss : 0.0360, accuracy : 98.91\n",
            "iteration : 350, loss : 0.0367, accuracy : 98.89\n",
            "Epoch : 167, training loss : 0.0364, training accuracy : 98.88, test loss : 1.5191, test accuracy : 73.29\n",
            "\n",
            "Epoch: 168\n",
            "iteration :  50, loss : 0.0344, accuracy : 98.94\n",
            "iteration : 100, loss : 0.0328, accuracy : 99.02\n",
            "iteration : 150, loss : 0.0349, accuracy : 98.94\n",
            "iteration : 200, loss : 0.0363, accuracy : 98.88\n",
            "iteration : 250, loss : 0.0362, accuracy : 98.91\n",
            "iteration : 300, loss : 0.0365, accuracy : 98.90\n",
            "iteration : 350, loss : 0.0371, accuracy : 98.88\n",
            "Epoch : 168, training loss : 0.0367, training accuracy : 98.91, test loss : 1.5410, test accuracy : 73.58\n",
            "\n",
            "Epoch: 169\n",
            "iteration :  50, loss : 0.0373, accuracy : 98.69\n",
            "iteration : 100, loss : 0.0379, accuracy : 98.82\n",
            "iteration : 150, loss : 0.0386, accuracy : 98.81\n",
            "iteration : 200, loss : 0.0382, accuracy : 98.79\n",
            "iteration : 250, loss : 0.0380, accuracy : 98.83\n",
            "iteration : 300, loss : 0.0380, accuracy : 98.83\n",
            "iteration : 350, loss : 0.0371, accuracy : 98.85\n",
            "Epoch : 169, training loss : 0.0379, training accuracy : 98.84, test loss : 1.5435, test accuracy : 73.17\n",
            "\n",
            "Epoch: 170\n",
            "iteration :  50, loss : 0.0353, accuracy : 98.97\n",
            "iteration : 100, loss : 0.0336, accuracy : 99.02\n",
            "iteration : 150, loss : 0.0348, accuracy : 98.95\n",
            "iteration : 200, loss : 0.0349, accuracy : 98.91\n",
            "iteration : 250, loss : 0.0345, accuracy : 98.94\n",
            "iteration : 300, loss : 0.0345, accuracy : 98.93\n",
            "iteration : 350, loss : 0.0352, accuracy : 98.91\n",
            "Epoch : 170, training loss : 0.0350, training accuracy : 98.92, test loss : 1.5361, test accuracy : 73.39\n",
            "\n",
            "Epoch: 171\n",
            "iteration :  50, loss : 0.0399, accuracy : 98.73\n",
            "iteration : 100, loss : 0.0396, accuracy : 98.76\n",
            "iteration : 150, loss : 0.0377, accuracy : 98.81\n",
            "iteration : 200, loss : 0.0375, accuracy : 98.82\n",
            "iteration : 250, loss : 0.0363, accuracy : 98.86\n",
            "iteration : 300, loss : 0.0374, accuracy : 98.83\n",
            "iteration : 350, loss : 0.0372, accuracy : 98.83\n",
            "Epoch : 171, training loss : 0.0366, training accuracy : 98.86, test loss : 1.5298, test accuracy : 73.74\n",
            "\n",
            "Epoch: 172\n",
            "iteration :  50, loss : 0.0326, accuracy : 98.94\n",
            "iteration : 100, loss : 0.0369, accuracy : 98.88\n",
            "iteration : 150, loss : 0.0371, accuracy : 98.85\n",
            "iteration : 200, loss : 0.0376, accuracy : 98.80\n",
            "iteration : 250, loss : 0.0379, accuracy : 98.79\n",
            "iteration : 300, loss : 0.0388, accuracy : 98.79\n",
            "iteration : 350, loss : 0.0377, accuracy : 98.84\n",
            "Epoch : 172, training loss : 0.0376, training accuracy : 98.85, test loss : 1.5295, test accuracy : 73.79\n",
            "\n",
            "Epoch: 173\n",
            "iteration :  50, loss : 0.0379, accuracy : 98.91\n",
            "iteration : 100, loss : 0.0377, accuracy : 98.88\n",
            "iteration : 150, loss : 0.0339, accuracy : 99.01\n",
            "iteration : 200, loss : 0.0335, accuracy : 99.00\n",
            "iteration : 250, loss : 0.0342, accuracy : 98.96\n",
            "iteration : 300, loss : 0.0344, accuracy : 98.94\n",
            "iteration : 350, loss : 0.0343, accuracy : 98.94\n",
            "Epoch : 173, training loss : 0.0346, training accuracy : 98.94, test loss : 1.5305, test accuracy : 73.70\n",
            "\n",
            "Epoch: 174\n",
            "iteration :  50, loss : 0.0381, accuracy : 98.88\n",
            "iteration : 100, loss : 0.0394, accuracy : 98.78\n",
            "iteration : 150, loss : 0.0371, accuracy : 98.86\n",
            "iteration : 200, loss : 0.0369, accuracy : 98.86\n",
            "iteration : 250, loss : 0.0365, accuracy : 98.87\n",
            "iteration : 300, loss : 0.0367, accuracy : 98.85\n",
            "iteration : 350, loss : 0.0360, accuracy : 98.90\n",
            "Epoch : 174, training loss : 0.0367, training accuracy : 98.89, test loss : 1.5358, test accuracy : 73.79\n",
            "\n",
            "Epoch: 175\n",
            "iteration :  50, loss : 0.0384, accuracy : 98.91\n",
            "iteration : 100, loss : 0.0346, accuracy : 98.99\n",
            "iteration : 150, loss : 0.0353, accuracy : 98.96\n",
            "iteration : 200, loss : 0.0346, accuracy : 98.97\n",
            "iteration : 250, loss : 0.0339, accuracy : 98.98\n",
            "iteration : 300, loss : 0.0349, accuracy : 98.96\n",
            "iteration : 350, loss : 0.0356, accuracy : 98.93\n",
            "Epoch : 175, training loss : 0.0357, training accuracy : 98.93, test loss : 1.5193, test accuracy : 74.01\n",
            "\n",
            "Epoch: 176\n",
            "iteration :  50, loss : 0.0290, accuracy : 98.98\n",
            "iteration : 100, loss : 0.0287, accuracy : 99.13\n",
            "iteration : 150, loss : 0.0301, accuracy : 99.10\n",
            "iteration : 200, loss : 0.0303, accuracy : 99.09\n",
            "iteration : 250, loss : 0.0306, accuracy : 99.08\n",
            "iteration : 300, loss : 0.0310, accuracy : 99.08\n",
            "iteration : 350, loss : 0.0316, accuracy : 99.06\n",
            "Epoch : 176, training loss : 0.0324, training accuracy : 99.02, test loss : 1.5270, test accuracy : 73.60\n",
            "\n",
            "Epoch: 177\n",
            "iteration :  50, loss : 0.0332, accuracy : 99.02\n",
            "iteration : 100, loss : 0.0311, accuracy : 99.06\n",
            "iteration : 150, loss : 0.0315, accuracy : 99.03\n",
            "iteration : 200, loss : 0.0319, accuracy : 99.04\n",
            "iteration : 250, loss : 0.0335, accuracy : 99.00\n",
            "iteration : 300, loss : 0.0342, accuracy : 98.99\n",
            "iteration : 350, loss : 0.0346, accuracy : 98.96\n",
            "Epoch : 177, training loss : 0.0343, training accuracy : 98.96, test loss : 1.5177, test accuracy : 73.84\n",
            "\n",
            "Epoch: 178\n",
            "iteration :  50, loss : 0.0326, accuracy : 99.03\n",
            "iteration : 100, loss : 0.0335, accuracy : 98.98\n",
            "iteration : 150, loss : 0.0344, accuracy : 98.96\n",
            "iteration : 200, loss : 0.0352, accuracy : 98.97\n",
            "iteration : 250, loss : 0.0349, accuracy : 98.95\n",
            "iteration : 300, loss : 0.0347, accuracy : 98.91\n",
            "iteration : 350, loss : 0.0352, accuracy : 98.91\n",
            "Epoch : 178, training loss : 0.0347, training accuracy : 98.92, test loss : 1.5257, test accuracy : 73.75\n",
            "\n",
            "Epoch: 179\n",
            "iteration :  50, loss : 0.0325, accuracy : 99.02\n",
            "iteration : 100, loss : 0.0339, accuracy : 98.99\n",
            "iteration : 150, loss : 0.0324, accuracy : 99.06\n",
            "iteration : 200, loss : 0.0331, accuracy : 99.01\n",
            "iteration : 250, loss : 0.0335, accuracy : 99.01\n",
            "iteration : 300, loss : 0.0348, accuracy : 98.98\n",
            "iteration : 350, loss : 0.0355, accuracy : 98.95\n",
            "Epoch : 179, training loss : 0.0351, training accuracy : 98.96, test loss : 1.5135, test accuracy : 73.98\n",
            "\n",
            "Epoch: 180\n",
            "iteration :  50, loss : 0.0314, accuracy : 99.00\n",
            "iteration : 100, loss : 0.0307, accuracy : 99.02\n",
            "iteration : 150, loss : 0.0323, accuracy : 99.00\n",
            "iteration : 200, loss : 0.0319, accuracy : 99.00\n",
            "iteration : 250, loss : 0.0315, accuracy : 99.05\n",
            "iteration : 300, loss : 0.0320, accuracy : 99.03\n",
            "iteration : 350, loss : 0.0324, accuracy : 99.01\n",
            "Epoch : 180, training loss : 0.0323, training accuracy : 99.02, test loss : 1.5481, test accuracy : 73.70\n",
            "\n",
            "Epoch: 181\n",
            "iteration :  50, loss : 0.0317, accuracy : 99.03\n",
            "iteration : 100, loss : 0.0332, accuracy : 98.98\n",
            "iteration : 150, loss : 0.0330, accuracy : 98.98\n",
            "iteration : 200, loss : 0.0324, accuracy : 98.99\n",
            "iteration : 250, loss : 0.0336, accuracy : 98.95\n",
            "iteration : 300, loss : 0.0339, accuracy : 98.94\n",
            "iteration : 350, loss : 0.0337, accuracy : 98.94\n",
            "Epoch : 181, training loss : 0.0341, training accuracy : 98.93, test loss : 1.5312, test accuracy : 73.95\n",
            "\n",
            "Epoch: 182\n",
            "iteration :  50, loss : 0.0293, accuracy : 99.12\n",
            "iteration : 100, loss : 0.0318, accuracy : 99.08\n",
            "iteration : 150, loss : 0.0324, accuracy : 99.07\n",
            "iteration : 200, loss : 0.0333, accuracy : 99.02\n",
            "iteration : 250, loss : 0.0331, accuracy : 99.03\n",
            "iteration : 300, loss : 0.0326, accuracy : 99.05\n",
            "iteration : 350, loss : 0.0322, accuracy : 99.05\n",
            "Epoch : 182, training loss : 0.0320, training accuracy : 99.05, test loss : 1.5191, test accuracy : 73.71\n",
            "\n",
            "Epoch: 183\n",
            "iteration :  50, loss : 0.0291, accuracy : 99.16\n",
            "iteration : 100, loss : 0.0340, accuracy : 99.02\n",
            "iteration : 150, loss : 0.0341, accuracy : 99.00\n",
            "iteration : 200, loss : 0.0330, accuracy : 99.02\n",
            "iteration : 250, loss : 0.0320, accuracy : 99.04\n",
            "iteration : 300, loss : 0.0326, accuracy : 99.02\n",
            "iteration : 350, loss : 0.0317, accuracy : 99.06\n",
            "Epoch : 183, training loss : 0.0325, training accuracy : 99.01, test loss : 1.5385, test accuracy : 73.72\n",
            "\n",
            "Epoch: 184\n",
            "iteration :  50, loss : 0.0301, accuracy : 99.11\n",
            "iteration : 100, loss : 0.0355, accuracy : 98.95\n",
            "iteration : 150, loss : 0.0337, accuracy : 99.00\n",
            "iteration : 200, loss : 0.0343, accuracy : 98.95\n",
            "iteration : 250, loss : 0.0346, accuracy : 98.96\n",
            "iteration : 300, loss : 0.0332, accuracy : 99.00\n",
            "iteration : 350, loss : 0.0333, accuracy : 99.00\n",
            "Epoch : 184, training loss : 0.0336, training accuracy : 98.99, test loss : 1.5350, test accuracy : 73.50\n",
            "\n",
            "Epoch: 185\n",
            "iteration :  50, loss : 0.0329, accuracy : 99.00\n",
            "iteration : 100, loss : 0.0292, accuracy : 99.12\n",
            "iteration : 150, loss : 0.0307, accuracy : 99.06\n",
            "iteration : 200, loss : 0.0301, accuracy : 99.09\n",
            "iteration : 250, loss : 0.0304, accuracy : 99.09\n",
            "iteration : 300, loss : 0.0308, accuracy : 99.07\n",
            "iteration : 350, loss : 0.0306, accuracy : 99.05\n",
            "Epoch : 185, training loss : 0.0303, training accuracy : 99.05, test loss : 1.5352, test accuracy : 73.67\n",
            "\n",
            "Epoch: 186\n",
            "iteration :  50, loss : 0.0299, accuracy : 99.08\n",
            "iteration : 100, loss : 0.0268, accuracy : 99.20\n",
            "iteration : 150, loss : 0.0288, accuracy : 99.19\n",
            "iteration : 200, loss : 0.0292, accuracy : 99.18\n",
            "iteration : 250, loss : 0.0304, accuracy : 99.14\n",
            "iteration : 300, loss : 0.0306, accuracy : 99.11\n",
            "iteration : 350, loss : 0.0307, accuracy : 99.11\n",
            "Epoch : 186, training loss : 0.0304, training accuracy : 99.11, test loss : 1.5314, test accuracy : 73.75\n",
            "\n",
            "Epoch: 187\n",
            "iteration :  50, loss : 0.0333, accuracy : 99.00\n",
            "iteration : 100, loss : 0.0306, accuracy : 99.12\n",
            "iteration : 150, loss : 0.0326, accuracy : 99.08\n",
            "iteration : 200, loss : 0.0307, accuracy : 99.12\n",
            "iteration : 250, loss : 0.0305, accuracy : 99.12\n",
            "iteration : 300, loss : 0.0315, accuracy : 99.09\n",
            "iteration : 350, loss : 0.0310, accuracy : 99.10\n",
            "Epoch : 187, training loss : 0.0315, training accuracy : 99.08, test loss : 1.5346, test accuracy : 74.02\n",
            "\n",
            "Epoch: 188\n",
            "iteration :  50, loss : 0.0344, accuracy : 98.86\n",
            "iteration : 100, loss : 0.0373, accuracy : 98.88\n",
            "iteration : 150, loss : 0.0341, accuracy : 98.98\n",
            "iteration : 200, loss : 0.0324, accuracy : 99.06\n",
            "iteration : 250, loss : 0.0320, accuracy : 99.06\n",
            "iteration : 300, loss : 0.0324, accuracy : 99.05\n",
            "iteration : 350, loss : 0.0319, accuracy : 99.06\n",
            "Epoch : 188, training loss : 0.0319, training accuracy : 99.06, test loss : 1.5409, test accuracy : 73.89\n",
            "\n",
            "Epoch: 189\n",
            "iteration :  50, loss : 0.0259, accuracy : 99.23\n",
            "iteration : 100, loss : 0.0290, accuracy : 99.09\n",
            "iteration : 150, loss : 0.0291, accuracy : 99.14\n",
            "iteration : 200, loss : 0.0289, accuracy : 99.14\n",
            "iteration : 250, loss : 0.0293, accuracy : 99.11\n",
            "iteration : 300, loss : 0.0297, accuracy : 99.11\n",
            "iteration : 350, loss : 0.0295, accuracy : 99.12\n",
            "Epoch : 189, training loss : 0.0289, training accuracy : 99.13, test loss : 1.5367, test accuracy : 73.64\n",
            "\n",
            "Epoch: 190\n",
            "iteration :  50, loss : 0.0265, accuracy : 99.25\n",
            "iteration : 100, loss : 0.0272, accuracy : 99.20\n",
            "iteration : 150, loss : 0.0286, accuracy : 99.18\n",
            "iteration : 200, loss : 0.0288, accuracy : 99.17\n",
            "iteration : 250, loss : 0.0292, accuracy : 99.14\n",
            "iteration : 300, loss : 0.0294, accuracy : 99.12\n",
            "iteration : 350, loss : 0.0295, accuracy : 99.10\n",
            "Epoch : 190, training loss : 0.0295, training accuracy : 99.11, test loss : 1.5305, test accuracy : 73.94\n",
            "\n",
            "Epoch: 191\n",
            "iteration :  50, loss : 0.0284, accuracy : 99.05\n",
            "iteration : 100, loss : 0.0296, accuracy : 99.09\n",
            "iteration : 150, loss : 0.0292, accuracy : 99.12\n",
            "iteration : 200, loss : 0.0291, accuracy : 99.12\n",
            "iteration : 250, loss : 0.0295, accuracy : 99.12\n",
            "iteration : 300, loss : 0.0298, accuracy : 99.10\n",
            "iteration : 350, loss : 0.0298, accuracy : 99.11\n",
            "Epoch : 191, training loss : 0.0298, training accuracy : 99.10, test loss : 1.5382, test accuracy : 73.80\n",
            "\n",
            "Epoch: 192\n",
            "iteration :  50, loss : 0.0293, accuracy : 99.12\n",
            "iteration : 100, loss : 0.0270, accuracy : 99.20\n",
            "iteration : 150, loss : 0.0272, accuracy : 99.19\n",
            "iteration : 200, loss : 0.0276, accuracy : 99.17\n",
            "iteration : 250, loss : 0.0280, accuracy : 99.18\n",
            "iteration : 300, loss : 0.0281, accuracy : 99.17\n",
            "iteration : 350, loss : 0.0282, accuracy : 99.14\n",
            "Epoch : 192, training loss : 0.0286, training accuracy : 99.12, test loss : 1.5548, test accuracy : 73.98\n",
            "\n",
            "Epoch: 193\n",
            "iteration :  50, loss : 0.0296, accuracy : 99.11\n",
            "iteration : 100, loss : 0.0311, accuracy : 99.07\n",
            "iteration : 150, loss : 0.0302, accuracy : 99.11\n",
            "iteration : 200, loss : 0.0305, accuracy : 99.11\n",
            "iteration : 250, loss : 0.0304, accuracy : 99.12\n",
            "iteration : 300, loss : 0.0301, accuracy : 99.12\n",
            "iteration : 350, loss : 0.0301, accuracy : 99.11\n",
            "Epoch : 193, training loss : 0.0308, training accuracy : 99.09, test loss : 1.5455, test accuracy : 73.70\n",
            "\n",
            "Epoch: 194\n",
            "iteration :  50, loss : 0.0302, accuracy : 99.02\n",
            "iteration : 100, loss : 0.0296, accuracy : 99.04\n",
            "iteration : 150, loss : 0.0296, accuracy : 99.09\n",
            "iteration : 200, loss : 0.0293, accuracy : 99.11\n",
            "iteration : 250, loss : 0.0294, accuracy : 99.12\n",
            "iteration : 300, loss : 0.0279, accuracy : 99.18\n",
            "iteration : 350, loss : 0.0273, accuracy : 99.17\n",
            "Epoch : 194, training loss : 0.0270, training accuracy : 99.19, test loss : 1.5401, test accuracy : 73.61\n",
            "\n",
            "Epoch: 195\n",
            "iteration :  50, loss : 0.0275, accuracy : 99.14\n",
            "iteration : 100, loss : 0.0293, accuracy : 99.10\n",
            "iteration : 150, loss : 0.0286, accuracy : 99.12\n",
            "iteration : 200, loss : 0.0282, accuracy : 99.13\n",
            "iteration : 250, loss : 0.0275, accuracy : 99.17\n",
            "iteration : 300, loss : 0.0265, accuracy : 99.18\n",
            "iteration : 350, loss : 0.0267, accuracy : 99.17\n",
            "Epoch : 195, training loss : 0.0267, training accuracy : 99.18, test loss : 1.5351, test accuracy : 73.84\n",
            "\n",
            "Epoch: 196\n",
            "iteration :  50, loss : 0.0289, accuracy : 99.11\n",
            "iteration : 100, loss : 0.0283, accuracy : 99.10\n",
            "iteration : 150, loss : 0.0273, accuracy : 99.11\n",
            "iteration : 200, loss : 0.0259, accuracy : 99.16\n",
            "iteration : 250, loss : 0.0261, accuracy : 99.16\n",
            "iteration : 300, loss : 0.0264, accuracy : 99.16\n",
            "iteration : 350, loss : 0.0264, accuracy : 99.16\n",
            "Epoch : 196, training loss : 0.0261, training accuracy : 99.17, test loss : 1.5368, test accuracy : 73.76\n",
            "\n",
            "Epoch: 197\n",
            "iteration :  50, loss : 0.0259, accuracy : 99.16\n",
            "iteration : 100, loss : 0.0266, accuracy : 99.12\n",
            "iteration : 150, loss : 0.0267, accuracy : 99.11\n",
            "iteration : 200, loss : 0.0270, accuracy : 99.15\n",
            "iteration : 250, loss : 0.0277, accuracy : 99.14\n",
            "iteration : 300, loss : 0.0271, accuracy : 99.16\n",
            "iteration : 350, loss : 0.0279, accuracy : 99.14\n",
            "Epoch : 197, training loss : 0.0276, training accuracy : 99.14, test loss : 1.5387, test accuracy : 73.65\n",
            "\n",
            "Epoch: 198\n",
            "iteration :  50, loss : 0.0281, accuracy : 99.16\n",
            "iteration : 100, loss : 0.0285, accuracy : 99.11\n",
            "iteration : 150, loss : 0.0278, accuracy : 99.12\n",
            "iteration : 200, loss : 0.0267, accuracy : 99.18\n",
            "iteration : 250, loss : 0.0253, accuracy : 99.22\n",
            "iteration : 300, loss : 0.0248, accuracy : 99.23\n",
            "iteration : 350, loss : 0.0247, accuracy : 99.24\n",
            "Epoch : 198, training loss : 0.0247, training accuracy : 99.24, test loss : 1.5373, test accuracy : 73.76\n",
            "\n",
            "Epoch: 199\n",
            "iteration :  50, loss : 0.0263, accuracy : 99.14\n",
            "iteration : 100, loss : 0.0274, accuracy : 99.18\n",
            "iteration : 150, loss : 0.0287, accuracy : 99.10\n",
            "iteration : 200, loss : 0.0277, accuracy : 99.14\n",
            "iteration : 250, loss : 0.0279, accuracy : 99.14\n",
            "iteration : 300, loss : 0.0276, accuracy : 99.17\n",
            "iteration : 350, loss : 0.0274, accuracy : 99.16\n",
            "Epoch : 199, training loss : 0.0278, training accuracy : 99.15, test loss : 1.5480, test accuracy : 73.70\n",
            "\n",
            "Epoch: 200\n",
            "iteration :  50, loss : 0.0265, accuracy : 99.23\n",
            "iteration : 100, loss : 0.0278, accuracy : 99.20\n",
            "iteration : 150, loss : 0.0288, accuracy : 99.16\n",
            "iteration : 200, loss : 0.0285, accuracy : 99.14\n",
            "iteration : 250, loss : 0.0282, accuracy : 99.13\n",
            "iteration : 300, loss : 0.0274, accuracy : 99.15\n",
            "iteration : 350, loss : 0.0267, accuracy : 99.16\n",
            "Epoch : 200, training loss : 0.0260, training accuracy : 99.19, test loss : 1.5434, test accuracy : 73.69\n",
            "\n",
            "Epoch: 201\n",
            "iteration :  50, loss : 0.0290, accuracy : 99.11\n",
            "iteration : 100, loss : 0.0276, accuracy : 99.19\n",
            "iteration : 150, loss : 0.0271, accuracy : 99.19\n",
            "iteration : 200, loss : 0.0286, accuracy : 99.13\n",
            "iteration : 250, loss : 0.0284, accuracy : 99.14\n",
            "iteration : 300, loss : 0.0284, accuracy : 99.16\n",
            "iteration : 350, loss : 0.0284, accuracy : 99.16\n",
            "Epoch : 201, training loss : 0.0280, training accuracy : 99.17, test loss : 1.5686, test accuracy : 73.89\n",
            "\n",
            "Epoch: 202\n",
            "iteration :  50, loss : 0.0308, accuracy : 99.05\n",
            "iteration : 100, loss : 0.0250, accuracy : 99.21\n",
            "iteration : 150, loss : 0.0269, accuracy : 99.16\n",
            "iteration : 200, loss : 0.0279, accuracy : 99.13\n",
            "iteration : 250, loss : 0.0276, accuracy : 99.12\n",
            "iteration : 300, loss : 0.0267, accuracy : 99.16\n",
            "iteration : 350, loss : 0.0264, accuracy : 99.17\n",
            "Epoch : 202, training loss : 0.0261, training accuracy : 99.19, test loss : 1.5418, test accuracy : 73.89\n",
            "\n",
            "Epoch: 203\n",
            "iteration :  50, loss : 0.0281, accuracy : 99.11\n",
            "iteration : 100, loss : 0.0250, accuracy : 99.20\n",
            "iteration : 150, loss : 0.0264, accuracy : 99.21\n",
            "iteration : 200, loss : 0.0252, accuracy : 99.25\n",
            "iteration : 250, loss : 0.0249, accuracy : 99.26\n",
            "iteration : 300, loss : 0.0245, accuracy : 99.27\n",
            "iteration : 350, loss : 0.0251, accuracy : 99.25\n",
            "Epoch : 203, training loss : 0.0252, training accuracy : 99.25, test loss : 1.5483, test accuracy : 73.48\n",
            "\n",
            "Epoch: 204\n",
            "iteration :  50, loss : 0.0217, accuracy : 99.38\n",
            "iteration : 100, loss : 0.0244, accuracy : 99.28\n",
            "iteration : 150, loss : 0.0240, accuracy : 99.30\n",
            "iteration : 200, loss : 0.0245, accuracy : 99.27\n",
            "iteration : 250, loss : 0.0247, accuracy : 99.29\n",
            "iteration : 300, loss : 0.0239, accuracy : 99.32\n",
            "iteration : 350, loss : 0.0243, accuracy : 99.29\n",
            "Epoch : 204, training loss : 0.0241, training accuracy : 99.29, test loss : 1.5400, test accuracy : 73.77\n",
            "\n",
            "Epoch: 205\n",
            "iteration :  50, loss : 0.0227, accuracy : 99.30\n",
            "iteration : 100, loss : 0.0221, accuracy : 99.35\n",
            "iteration : 150, loss : 0.0212, accuracy : 99.36\n",
            "iteration : 200, loss : 0.0226, accuracy : 99.31\n",
            "iteration : 250, loss : 0.0234, accuracy : 99.28\n",
            "iteration : 300, loss : 0.0238, accuracy : 99.26\n",
            "iteration : 350, loss : 0.0249, accuracy : 99.23\n",
            "Epoch : 205, training loss : 0.0252, training accuracy : 99.22, test loss : 1.5482, test accuracy : 73.62\n",
            "\n",
            "Epoch: 206\n",
            "iteration :  50, loss : 0.0279, accuracy : 99.19\n",
            "iteration : 100, loss : 0.0257, accuracy : 99.20\n",
            "iteration : 150, loss : 0.0248, accuracy : 99.23\n",
            "iteration : 200, loss : 0.0240, accuracy : 99.26\n",
            "iteration : 250, loss : 0.0230, accuracy : 99.29\n",
            "iteration : 300, loss : 0.0226, accuracy : 99.31\n",
            "iteration : 350, loss : 0.0232, accuracy : 99.29\n",
            "Epoch : 206, training loss : 0.0235, training accuracy : 99.28, test loss : 1.5283, test accuracy : 74.28\n",
            "\n",
            "Epoch: 207\n",
            "iteration :  50, loss : 0.0237, accuracy : 99.28\n",
            "iteration : 100, loss : 0.0272, accuracy : 99.12\n",
            "iteration : 150, loss : 0.0259, accuracy : 99.23\n",
            "iteration : 200, loss : 0.0259, accuracy : 99.22\n",
            "iteration : 250, loss : 0.0260, accuracy : 99.21\n",
            "iteration : 300, loss : 0.0253, accuracy : 99.23\n",
            "iteration : 350, loss : 0.0249, accuracy : 99.24\n",
            "Epoch : 207, training loss : 0.0250, training accuracy : 99.24, test loss : 1.5436, test accuracy : 73.99\n",
            "\n",
            "Epoch: 208\n",
            "iteration :  50, loss : 0.0237, accuracy : 99.23\n",
            "iteration : 100, loss : 0.0212, accuracy : 99.30\n",
            "iteration : 150, loss : 0.0216, accuracy : 99.31\n",
            "iteration : 200, loss : 0.0215, accuracy : 99.32\n",
            "iteration : 250, loss : 0.0224, accuracy : 99.30\n",
            "iteration : 300, loss : 0.0220, accuracy : 99.31\n",
            "iteration : 350, loss : 0.0223, accuracy : 99.30\n",
            "Epoch : 208, training loss : 0.0227, training accuracy : 99.29, test loss : 1.5283, test accuracy : 73.88\n",
            "\n",
            "Epoch: 209\n",
            "iteration :  50, loss : 0.0254, accuracy : 99.19\n",
            "iteration : 100, loss : 0.0250, accuracy : 99.20\n",
            "iteration : 150, loss : 0.0251, accuracy : 99.27\n",
            "iteration : 200, loss : 0.0239, accuracy : 99.30\n",
            "iteration : 250, loss : 0.0235, accuracy : 99.33\n",
            "iteration : 300, loss : 0.0225, accuracy : 99.36\n",
            "iteration : 350, loss : 0.0229, accuracy : 99.34\n",
            "Epoch : 209, training loss : 0.0228, training accuracy : 99.33, test loss : 1.5229, test accuracy : 73.90\n",
            "\n",
            "Epoch: 210\n",
            "iteration :  50, loss : 0.0244, accuracy : 99.30\n",
            "iteration : 100, loss : 0.0240, accuracy : 99.30\n",
            "iteration : 150, loss : 0.0221, accuracy : 99.37\n",
            "iteration : 200, loss : 0.0225, accuracy : 99.35\n",
            "iteration : 250, loss : 0.0227, accuracy : 99.36\n",
            "iteration : 300, loss : 0.0231, accuracy : 99.34\n",
            "iteration : 350, loss : 0.0239, accuracy : 99.33\n",
            "Epoch : 210, training loss : 0.0242, training accuracy : 99.33, test loss : 1.5354, test accuracy : 73.97\n",
            "\n",
            "Epoch: 211\n",
            "iteration :  50, loss : 0.0183, accuracy : 99.59\n",
            "iteration : 100, loss : 0.0217, accuracy : 99.44\n",
            "iteration : 150, loss : 0.0232, accuracy : 99.35\n",
            "iteration : 200, loss : 0.0231, accuracy : 99.36\n",
            "iteration : 250, loss : 0.0230, accuracy : 99.38\n",
            "iteration : 300, loss : 0.0225, accuracy : 99.39\n",
            "iteration : 350, loss : 0.0231, accuracy : 99.36\n",
            "Epoch : 211, training loss : 0.0235, training accuracy : 99.34, test loss : 1.5331, test accuracy : 74.01\n",
            "\n",
            "Epoch: 212\n",
            "iteration :  50, loss : 0.0219, accuracy : 99.39\n",
            "iteration : 100, loss : 0.0229, accuracy : 99.34\n",
            "iteration : 150, loss : 0.0227, accuracy : 99.33\n",
            "iteration : 200, loss : 0.0236, accuracy : 99.32\n",
            "iteration : 250, loss : 0.0233, accuracy : 99.31\n",
            "iteration : 300, loss : 0.0239, accuracy : 99.30\n",
            "iteration : 350, loss : 0.0235, accuracy : 99.31\n",
            "Epoch : 212, training loss : 0.0242, training accuracy : 99.31, test loss : 1.5311, test accuracy : 73.81\n",
            "\n",
            "Epoch: 213\n",
            "iteration :  50, loss : 0.0267, accuracy : 99.05\n",
            "iteration : 100, loss : 0.0268, accuracy : 99.12\n",
            "iteration : 150, loss : 0.0256, accuracy : 99.19\n",
            "iteration : 200, loss : 0.0248, accuracy : 99.24\n",
            "iteration : 250, loss : 0.0243, accuracy : 99.26\n",
            "iteration : 300, loss : 0.0236, accuracy : 99.26\n",
            "iteration : 350, loss : 0.0234, accuracy : 99.27\n",
            "Epoch : 213, training loss : 0.0233, training accuracy : 99.27, test loss : 1.5213, test accuracy : 73.98\n",
            "\n",
            "Epoch: 214\n",
            "iteration :  50, loss : 0.0175, accuracy : 99.53\n",
            "iteration : 100, loss : 0.0203, accuracy : 99.44\n",
            "iteration : 150, loss : 0.0214, accuracy : 99.41\n",
            "iteration : 200, loss : 0.0211, accuracy : 99.42\n",
            "iteration : 250, loss : 0.0218, accuracy : 99.38\n",
            "iteration : 300, loss : 0.0215, accuracy : 99.39\n",
            "iteration : 350, loss : 0.0218, accuracy : 99.37\n",
            "Epoch : 214, training loss : 0.0216, training accuracy : 99.37, test loss : 1.5239, test accuracy : 73.87\n",
            "\n",
            "Epoch: 215\n",
            "iteration :  50, loss : 0.0283, accuracy : 99.11\n",
            "iteration : 100, loss : 0.0257, accuracy : 99.24\n",
            "iteration : 150, loss : 0.0242, accuracy : 99.34\n",
            "iteration : 200, loss : 0.0231, accuracy : 99.37\n",
            "iteration : 250, loss : 0.0234, accuracy : 99.35\n",
            "iteration : 300, loss : 0.0231, accuracy : 99.35\n",
            "iteration : 350, loss : 0.0235, accuracy : 99.33\n",
            "Epoch : 215, training loss : 0.0237, training accuracy : 99.31, test loss : 1.5341, test accuracy : 74.31\n",
            "\n",
            "Epoch: 216\n",
            "iteration :  50, loss : 0.0225, accuracy : 99.30\n",
            "iteration : 100, loss : 0.0230, accuracy : 99.30\n",
            "iteration : 150, loss : 0.0221, accuracy : 99.33\n",
            "iteration : 200, loss : 0.0222, accuracy : 99.32\n",
            "iteration : 250, loss : 0.0221, accuracy : 99.32\n",
            "iteration : 300, loss : 0.0223, accuracy : 99.32\n",
            "iteration : 350, loss : 0.0222, accuracy : 99.33\n",
            "Epoch : 216, training loss : 0.0227, training accuracy : 99.33, test loss : 1.5413, test accuracy : 73.94\n",
            "\n",
            "Epoch: 217\n",
            "iteration :  50, loss : 0.0218, accuracy : 99.23\n",
            "iteration : 100, loss : 0.0208, accuracy : 99.35\n",
            "iteration : 150, loss : 0.0208, accuracy : 99.34\n",
            "iteration : 200, loss : 0.0212, accuracy : 99.38\n",
            "iteration : 250, loss : 0.0211, accuracy : 99.38\n",
            "iteration : 300, loss : 0.0208, accuracy : 99.38\n",
            "iteration : 350, loss : 0.0210, accuracy : 99.38\n",
            "Epoch : 217, training loss : 0.0212, training accuracy : 99.37, test loss : 1.5432, test accuracy : 74.01\n",
            "\n",
            "Epoch: 218\n",
            "iteration :  50, loss : 0.0229, accuracy : 99.30\n",
            "iteration : 100, loss : 0.0207, accuracy : 99.34\n",
            "iteration : 150, loss : 0.0199, accuracy : 99.36\n",
            "iteration : 200, loss : 0.0205, accuracy : 99.39\n",
            "iteration : 250, loss : 0.0198, accuracy : 99.40\n",
            "iteration : 300, loss : 0.0197, accuracy : 99.40\n",
            "iteration : 350, loss : 0.0201, accuracy : 99.38\n",
            "Epoch : 218, training loss : 0.0206, training accuracy : 99.36, test loss : 1.5373, test accuracy : 74.04\n",
            "\n",
            "Epoch: 219\n",
            "iteration :  50, loss : 0.0190, accuracy : 99.39\n",
            "iteration : 100, loss : 0.0211, accuracy : 99.29\n",
            "iteration : 150, loss : 0.0228, accuracy : 99.29\n",
            "iteration : 200, loss : 0.0233, accuracy : 99.27\n",
            "iteration : 250, loss : 0.0233, accuracy : 99.28\n",
            "iteration : 300, loss : 0.0229, accuracy : 99.29\n",
            "iteration : 350, loss : 0.0227, accuracy : 99.31\n",
            "Epoch : 219, training loss : 0.0229, training accuracy : 99.29, test loss : 1.5384, test accuracy : 73.92\n",
            "\n",
            "Epoch: 220\n",
            "iteration :  50, loss : 0.0252, accuracy : 99.25\n",
            "iteration : 100, loss : 0.0214, accuracy : 99.35\n",
            "iteration : 150, loss : 0.0218, accuracy : 99.33\n",
            "iteration : 200, loss : 0.0225, accuracy : 99.30\n",
            "iteration : 250, loss : 0.0219, accuracy : 99.34\n",
            "iteration : 300, loss : 0.0225, accuracy : 99.32\n",
            "iteration : 350, loss : 0.0231, accuracy : 99.30\n",
            "Epoch : 220, training loss : 0.0224, training accuracy : 99.31, test loss : 1.5259, test accuracy : 74.12\n",
            "\n",
            "Epoch: 221\n",
            "iteration :  50, loss : 0.0183, accuracy : 99.41\n",
            "iteration : 100, loss : 0.0187, accuracy : 99.44\n",
            "iteration : 150, loss : 0.0188, accuracy : 99.45\n",
            "iteration : 200, loss : 0.0193, accuracy : 99.44\n",
            "iteration : 250, loss : 0.0202, accuracy : 99.41\n",
            "iteration : 300, loss : 0.0211, accuracy : 99.38\n",
            "iteration : 350, loss : 0.0208, accuracy : 99.40\n",
            "Epoch : 221, training loss : 0.0206, training accuracy : 99.39, test loss : 1.5294, test accuracy : 73.82\n",
            "\n",
            "Epoch: 222\n",
            "iteration :  50, loss : 0.0188, accuracy : 99.41\n",
            "iteration : 100, loss : 0.0206, accuracy : 99.28\n",
            "iteration : 150, loss : 0.0231, accuracy : 99.24\n",
            "iteration : 200, loss : 0.0221, accuracy : 99.28\n",
            "iteration : 250, loss : 0.0218, accuracy : 99.31\n",
            "iteration : 300, loss : 0.0214, accuracy : 99.32\n",
            "iteration : 350, loss : 0.0215, accuracy : 99.30\n",
            "Epoch : 222, training loss : 0.0215, training accuracy : 99.30, test loss : 1.5325, test accuracy : 73.99\n",
            "\n",
            "Epoch: 223\n",
            "iteration :  50, loss : 0.0209, accuracy : 99.36\n",
            "iteration : 100, loss : 0.0210, accuracy : 99.35\n",
            "iteration : 150, loss : 0.0193, accuracy : 99.41\n",
            "iteration : 200, loss : 0.0203, accuracy : 99.41\n",
            "iteration : 250, loss : 0.0201, accuracy : 99.40\n",
            "iteration : 300, loss : 0.0211, accuracy : 99.38\n",
            "iteration : 350, loss : 0.0215, accuracy : 99.37\n",
            "Epoch : 223, training loss : 0.0215, training accuracy : 99.36, test loss : 1.5325, test accuracy : 73.88\n",
            "\n",
            "Epoch: 224\n",
            "iteration :  50, loss : 0.0207, accuracy : 99.33\n",
            "iteration : 100, loss : 0.0189, accuracy : 99.45\n",
            "iteration : 150, loss : 0.0189, accuracy : 99.45\n",
            "iteration : 200, loss : 0.0196, accuracy : 99.42\n",
            "iteration : 250, loss : 0.0198, accuracy : 99.42\n",
            "iteration : 300, loss : 0.0196, accuracy : 99.42\n",
            "iteration : 350, loss : 0.0200, accuracy : 99.41\n",
            "Epoch : 224, training loss : 0.0205, training accuracy : 99.38, test loss : 1.5227, test accuracy : 74.21\n",
            "\n",
            "Epoch: 225\n",
            "iteration :  50, loss : 0.0196, accuracy : 99.47\n",
            "iteration : 100, loss : 0.0202, accuracy : 99.38\n",
            "iteration : 150, loss : 0.0212, accuracy : 99.39\n",
            "iteration : 200, loss : 0.0201, accuracy : 99.39\n",
            "iteration : 250, loss : 0.0208, accuracy : 99.37\n",
            "iteration : 300, loss : 0.0212, accuracy : 99.34\n",
            "iteration : 350, loss : 0.0209, accuracy : 99.34\n",
            "Epoch : 225, training loss : 0.0215, training accuracy : 99.32, test loss : 1.5237, test accuracy : 74.01\n",
            "\n",
            "Epoch: 226\n",
            "iteration :  50, loss : 0.0174, accuracy : 99.50\n",
            "iteration : 100, loss : 0.0205, accuracy : 99.46\n",
            "iteration : 150, loss : 0.0217, accuracy : 99.38\n",
            "iteration : 200, loss : 0.0221, accuracy : 99.39\n",
            "iteration : 250, loss : 0.0210, accuracy : 99.42\n",
            "iteration : 300, loss : 0.0208, accuracy : 99.42\n",
            "iteration : 350, loss : 0.0203, accuracy : 99.43\n",
            "Epoch : 226, training loss : 0.0204, training accuracy : 99.43, test loss : 1.5203, test accuracy : 73.82\n",
            "\n",
            "Epoch: 227\n",
            "iteration :  50, loss : 0.0185, accuracy : 99.50\n",
            "iteration : 100, loss : 0.0181, accuracy : 99.48\n",
            "iteration : 150, loss : 0.0191, accuracy : 99.46\n",
            "iteration : 200, loss : 0.0200, accuracy : 99.43\n",
            "iteration : 250, loss : 0.0203, accuracy : 99.40\n",
            "iteration : 300, loss : 0.0212, accuracy : 99.38\n",
            "iteration : 350, loss : 0.0212, accuracy : 99.39\n",
            "Epoch : 227, training loss : 0.0210, training accuracy : 99.38, test loss : 1.5207, test accuracy : 73.90\n",
            "\n",
            "Epoch: 228\n",
            "iteration :  50, loss : 0.0236, accuracy : 99.34\n",
            "iteration : 100, loss : 0.0200, accuracy : 99.45\n",
            "iteration : 150, loss : 0.0197, accuracy : 99.46\n",
            "iteration : 200, loss : 0.0202, accuracy : 99.44\n",
            "iteration : 250, loss : 0.0197, accuracy : 99.44\n",
            "iteration : 300, loss : 0.0196, accuracy : 99.45\n",
            "iteration : 350, loss : 0.0203, accuracy : 99.44\n",
            "Epoch : 228, training loss : 0.0198, training accuracy : 99.45, test loss : 1.5322, test accuracy : 74.00\n",
            "\n",
            "Epoch: 229\n",
            "iteration :  50, loss : 0.0214, accuracy : 99.41\n",
            "iteration : 100, loss : 0.0222, accuracy : 99.38\n",
            "iteration : 150, loss : 0.0214, accuracy : 99.37\n",
            "iteration : 200, loss : 0.0205, accuracy : 99.41\n",
            "iteration : 250, loss : 0.0209, accuracy : 99.41\n",
            "iteration : 300, loss : 0.0214, accuracy : 99.41\n",
            "iteration : 350, loss : 0.0221, accuracy : 99.39\n",
            "Epoch : 229, training loss : 0.0218, training accuracy : 99.39, test loss : 1.5267, test accuracy : 73.98\n",
            "\n",
            "Epoch: 230\n",
            "iteration :  50, loss : 0.0183, accuracy : 99.45\n",
            "iteration : 100, loss : 0.0197, accuracy : 99.42\n",
            "iteration : 150, loss : 0.0209, accuracy : 99.38\n",
            "iteration : 200, loss : 0.0199, accuracy : 99.40\n",
            "iteration : 250, loss : 0.0205, accuracy : 99.38\n",
            "iteration : 300, loss : 0.0210, accuracy : 99.36\n",
            "iteration : 350, loss : 0.0207, accuracy : 99.39\n",
            "Epoch : 230, training loss : 0.0204, training accuracy : 99.39, test loss : 1.5229, test accuracy : 74.27\n",
            "\n",
            "Epoch: 231\n",
            "iteration :  50, loss : 0.0206, accuracy : 99.30\n",
            "iteration : 100, loss : 0.0204, accuracy : 99.30\n",
            "iteration : 150, loss : 0.0210, accuracy : 99.29\n",
            "iteration : 200, loss : 0.0200, accuracy : 99.34\n",
            "iteration : 250, loss : 0.0207, accuracy : 99.33\n",
            "iteration : 300, loss : 0.0206, accuracy : 99.33\n",
            "iteration : 350, loss : 0.0200, accuracy : 99.34\n",
            "Epoch : 231, training loss : 0.0199, training accuracy : 99.34, test loss : 1.5255, test accuracy : 74.20\n",
            "\n",
            "Epoch: 232\n",
            "iteration :  50, loss : 0.0173, accuracy : 99.44\n",
            "iteration : 100, loss : 0.0187, accuracy : 99.34\n",
            "iteration : 150, loss : 0.0194, accuracy : 99.35\n",
            "iteration : 200, loss : 0.0197, accuracy : 99.34\n",
            "iteration : 250, loss : 0.0197, accuracy : 99.37\n",
            "iteration : 300, loss : 0.0194, accuracy : 99.36\n",
            "iteration : 350, loss : 0.0195, accuracy : 99.37\n",
            "Epoch : 232, training loss : 0.0194, training accuracy : 99.38, test loss : 1.5320, test accuracy : 74.06\n",
            "\n",
            "Epoch: 233\n",
            "iteration :  50, loss : 0.0212, accuracy : 99.30\n",
            "iteration : 100, loss : 0.0216, accuracy : 99.34\n",
            "iteration : 150, loss : 0.0205, accuracy : 99.39\n",
            "iteration : 200, loss : 0.0208, accuracy : 99.39\n",
            "iteration : 250, loss : 0.0202, accuracy : 99.41\n",
            "iteration : 300, loss : 0.0201, accuracy : 99.41\n",
            "iteration : 350, loss : 0.0203, accuracy : 99.41\n",
            "Epoch : 233, training loss : 0.0198, training accuracy : 99.43, test loss : 1.5180, test accuracy : 74.34\n",
            "\n",
            "Epoch: 234\n",
            "iteration :  50, loss : 0.0239, accuracy : 99.34\n",
            "iteration : 100, loss : 0.0201, accuracy : 99.44\n",
            "iteration : 150, loss : 0.0209, accuracy : 99.39\n",
            "iteration : 200, loss : 0.0209, accuracy : 99.37\n",
            "iteration : 250, loss : 0.0195, accuracy : 99.43\n",
            "iteration : 300, loss : 0.0191, accuracy : 99.45\n",
            "iteration : 350, loss : 0.0185, accuracy : 99.47\n",
            "Epoch : 234, training loss : 0.0186, training accuracy : 99.47, test loss : 1.5120, test accuracy : 74.16\n",
            "\n",
            "Epoch: 235\n",
            "iteration :  50, loss : 0.0204, accuracy : 99.36\n",
            "iteration : 100, loss : 0.0176, accuracy : 99.47\n",
            "iteration : 150, loss : 0.0177, accuracy : 99.45\n",
            "iteration : 200, loss : 0.0183, accuracy : 99.44\n",
            "iteration : 250, loss : 0.0181, accuracy : 99.43\n",
            "iteration : 300, loss : 0.0179, accuracy : 99.44\n",
            "iteration : 350, loss : 0.0178, accuracy : 99.45\n",
            "Epoch : 235, training loss : 0.0179, training accuracy : 99.46, test loss : 1.5147, test accuracy : 74.20\n",
            "\n",
            "Epoch: 236\n",
            "iteration :  50, loss : 0.0170, accuracy : 99.52\n",
            "iteration : 100, loss : 0.0196, accuracy : 99.38\n",
            "iteration : 150, loss : 0.0195, accuracy : 99.38\n",
            "iteration : 200, loss : 0.0189, accuracy : 99.42\n",
            "iteration : 250, loss : 0.0180, accuracy : 99.46\n",
            "iteration : 300, loss : 0.0180, accuracy : 99.46\n",
            "iteration : 350, loss : 0.0182, accuracy : 99.44\n",
            "Epoch : 236, training loss : 0.0179, training accuracy : 99.46, test loss : 1.5045, test accuracy : 73.93\n",
            "\n",
            "Epoch: 237\n",
            "iteration :  50, loss : 0.0155, accuracy : 99.44\n",
            "iteration : 100, loss : 0.0164, accuracy : 99.45\n",
            "iteration : 150, loss : 0.0176, accuracy : 99.43\n",
            "iteration : 200, loss : 0.0186, accuracy : 99.41\n",
            "iteration : 250, loss : 0.0186, accuracy : 99.42\n",
            "iteration : 300, loss : 0.0186, accuracy : 99.42\n",
            "iteration : 350, loss : 0.0187, accuracy : 99.41\n",
            "Epoch : 237, training loss : 0.0189, training accuracy : 99.42, test loss : 1.5081, test accuracy : 73.93\n",
            "\n",
            "Epoch: 238\n",
            "iteration :  50, loss : 0.0199, accuracy : 99.41\n",
            "iteration : 100, loss : 0.0195, accuracy : 99.45\n",
            "iteration : 150, loss : 0.0191, accuracy : 99.43\n",
            "iteration : 200, loss : 0.0194, accuracy : 99.43\n",
            "iteration : 250, loss : 0.0187, accuracy : 99.45\n",
            "iteration : 300, loss : 0.0183, accuracy : 99.47\n",
            "iteration : 350, loss : 0.0184, accuracy : 99.45\n",
            "Epoch : 238, training loss : 0.0190, training accuracy : 99.44, test loss : 1.5137, test accuracy : 74.00\n",
            "\n",
            "Epoch: 239\n",
            "iteration :  50, loss : 0.0206, accuracy : 99.30\n",
            "iteration : 100, loss : 0.0201, accuracy : 99.33\n",
            "iteration : 150, loss : 0.0209, accuracy : 99.35\n",
            "iteration : 200, loss : 0.0207, accuracy : 99.36\n",
            "iteration : 250, loss : 0.0207, accuracy : 99.35\n",
            "iteration : 300, loss : 0.0205, accuracy : 99.38\n",
            "iteration : 350, loss : 0.0206, accuracy : 99.38\n",
            "Epoch : 239, training loss : 0.0203, training accuracy : 99.40, test loss : 1.5061, test accuracy : 74.16\n",
            "\n",
            "Epoch: 240\n",
            "iteration :  50, loss : 0.0173, accuracy : 99.47\n",
            "iteration : 100, loss : 0.0189, accuracy : 99.42\n",
            "iteration : 150, loss : 0.0176, accuracy : 99.46\n",
            "iteration : 200, loss : 0.0176, accuracy : 99.45\n",
            "iteration : 250, loss : 0.0177, accuracy : 99.44\n",
            "iteration : 300, loss : 0.0182, accuracy : 99.44\n",
            "iteration : 350, loss : 0.0182, accuracy : 99.46\n",
            "Epoch : 240, training loss : 0.0181, training accuracy : 99.48, test loss : 1.5017, test accuracy : 74.06\n",
            "\n",
            "Epoch: 241\n",
            "iteration :  50, loss : 0.0235, accuracy : 99.30\n",
            "iteration : 100, loss : 0.0221, accuracy : 99.34\n",
            "iteration : 150, loss : 0.0209, accuracy : 99.39\n",
            "iteration : 200, loss : 0.0196, accuracy : 99.43\n",
            "iteration : 250, loss : 0.0197, accuracy : 99.42\n",
            "iteration : 300, loss : 0.0198, accuracy : 99.43\n",
            "iteration : 350, loss : 0.0194, accuracy : 99.44\n",
            "Epoch : 241, training loss : 0.0192, training accuracy : 99.44, test loss : 1.5087, test accuracy : 73.99\n",
            "\n",
            "Epoch: 242\n",
            "iteration :  50, loss : 0.0199, accuracy : 99.36\n",
            "iteration : 100, loss : 0.0202, accuracy : 99.43\n",
            "iteration : 150, loss : 0.0203, accuracy : 99.40\n",
            "iteration : 200, loss : 0.0198, accuracy : 99.40\n",
            "iteration : 250, loss : 0.0191, accuracy : 99.43\n",
            "iteration : 300, loss : 0.0186, accuracy : 99.45\n",
            "iteration : 350, loss : 0.0180, accuracy : 99.47\n",
            "Epoch : 242, training loss : 0.0185, training accuracy : 99.46, test loss : 1.5020, test accuracy : 74.16\n",
            "\n",
            "Epoch: 243\n",
            "iteration :  50, loss : 0.0205, accuracy : 99.41\n",
            "iteration : 100, loss : 0.0182, accuracy : 99.43\n",
            "iteration : 150, loss : 0.0181, accuracy : 99.45\n",
            "iteration : 200, loss : 0.0184, accuracy : 99.44\n",
            "iteration : 250, loss : 0.0185, accuracy : 99.42\n",
            "iteration : 300, loss : 0.0185, accuracy : 99.43\n",
            "iteration : 350, loss : 0.0190, accuracy : 99.42\n",
            "Epoch : 243, training loss : 0.0194, training accuracy : 99.40, test loss : 1.5032, test accuracy : 74.19\n",
            "\n",
            "Epoch: 244\n",
            "iteration :  50, loss : 0.0224, accuracy : 99.34\n",
            "iteration : 100, loss : 0.0222, accuracy : 99.34\n",
            "iteration : 150, loss : 0.0199, accuracy : 99.40\n",
            "iteration : 200, loss : 0.0188, accuracy : 99.43\n",
            "iteration : 250, loss : 0.0184, accuracy : 99.43\n",
            "iteration : 300, loss : 0.0186, accuracy : 99.42\n",
            "iteration : 350, loss : 0.0182, accuracy : 99.43\n",
            "Epoch : 244, training loss : 0.0183, training accuracy : 99.42, test loss : 1.5019, test accuracy : 74.06\n",
            "\n",
            "Epoch: 245\n",
            "iteration :  50, loss : 0.0160, accuracy : 99.52\n",
            "iteration : 100, loss : 0.0161, accuracy : 99.58\n",
            "iteration : 150, loss : 0.0175, accuracy : 99.49\n",
            "iteration : 200, loss : 0.0168, accuracy : 99.51\n",
            "iteration : 250, loss : 0.0178, accuracy : 99.46\n",
            "iteration : 300, loss : 0.0183, accuracy : 99.43\n",
            "iteration : 350, loss : 0.0183, accuracy : 99.43\n",
            "Epoch : 245, training loss : 0.0185, training accuracy : 99.42, test loss : 1.4910, test accuracy : 74.31\n",
            "\n",
            "Epoch: 246\n",
            "iteration :  50, loss : 0.0158, accuracy : 99.55\n",
            "iteration : 100, loss : 0.0155, accuracy : 99.54\n",
            "iteration : 150, loss : 0.0171, accuracy : 99.48\n",
            "iteration : 200, loss : 0.0169, accuracy : 99.50\n",
            "iteration : 250, loss : 0.0172, accuracy : 99.51\n",
            "iteration : 300, loss : 0.0179, accuracy : 99.48\n",
            "iteration : 350, loss : 0.0178, accuracy : 99.48\n",
            "Epoch : 246, training loss : 0.0175, training accuracy : 99.48, test loss : 1.4921, test accuracy : 74.25\n",
            "\n",
            "Epoch: 247\n",
            "iteration :  50, loss : 0.0179, accuracy : 99.47\n",
            "iteration : 100, loss : 0.0179, accuracy : 99.48\n",
            "iteration : 150, loss : 0.0170, accuracy : 99.51\n",
            "iteration : 200, loss : 0.0181, accuracy : 99.45\n",
            "iteration : 250, loss : 0.0190, accuracy : 99.43\n",
            "iteration : 300, loss : 0.0194, accuracy : 99.41\n",
            "iteration : 350, loss : 0.0193, accuracy : 99.42\n",
            "Epoch : 247, training loss : 0.0190, training accuracy : 99.43, test loss : 1.4958, test accuracy : 74.08\n",
            "\n",
            "Epoch: 248\n",
            "iteration :  50, loss : 0.0210, accuracy : 99.38\n",
            "iteration : 100, loss : 0.0187, accuracy : 99.42\n",
            "iteration : 150, loss : 0.0182, accuracy : 99.44\n",
            "iteration : 200, loss : 0.0186, accuracy : 99.44\n",
            "iteration : 250, loss : 0.0178, accuracy : 99.47\n",
            "iteration : 300, loss : 0.0178, accuracy : 99.46\n",
            "iteration : 350, loss : 0.0181, accuracy : 99.47\n",
            "Epoch : 248, training loss : 0.0179, training accuracy : 99.47, test loss : 1.4908, test accuracy : 74.33\n",
            "\n",
            "Epoch: 249\n",
            "iteration :  50, loss : 0.0180, accuracy : 99.50\n",
            "iteration : 100, loss : 0.0174, accuracy : 99.48\n",
            "iteration : 150, loss : 0.0195, accuracy : 99.35\n",
            "iteration : 200, loss : 0.0188, accuracy : 99.39\n",
            "iteration : 250, loss : 0.0180, accuracy : 99.42\n",
            "iteration : 300, loss : 0.0185, accuracy : 99.42\n",
            "iteration : 350, loss : 0.0190, accuracy : 99.41\n",
            "Epoch : 249, training loss : 0.0187, training accuracy : 99.43, test loss : 1.4907, test accuracy : 74.35\n",
            "\n",
            "Epoch: 250\n",
            "iteration :  50, loss : 0.0189, accuracy : 99.38\n",
            "iteration : 100, loss : 0.0181, accuracy : 99.39\n",
            "iteration : 150, loss : 0.0180, accuracy : 99.42\n",
            "iteration : 200, loss : 0.0185, accuracy : 99.39\n",
            "iteration : 250, loss : 0.0190, accuracy : 99.39\n",
            "iteration : 300, loss : 0.0186, accuracy : 99.39\n",
            "iteration : 350, loss : 0.0180, accuracy : 99.41\n",
            "Epoch : 250, training loss : 0.0180, training accuracy : 99.41, test loss : 1.4995, test accuracy : 74.18\n",
            "\n",
            "Epoch: 251\n",
            "iteration :  50, loss : 0.0150, accuracy : 99.53\n",
            "iteration : 100, loss : 0.0173, accuracy : 99.48\n",
            "iteration : 150, loss : 0.0169, accuracy : 99.47\n",
            "iteration : 200, loss : 0.0173, accuracy : 99.46\n",
            "iteration : 250, loss : 0.0170, accuracy : 99.47\n",
            "iteration : 300, loss : 0.0168, accuracy : 99.47\n",
            "iteration : 350, loss : 0.0167, accuracy : 99.48\n",
            "Epoch : 251, training loss : 0.0170, training accuracy : 99.47, test loss : 1.4947, test accuracy : 74.16\n",
            "\n",
            "Epoch: 252\n",
            "iteration :  50, loss : 0.0151, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0166, accuracy : 99.55\n",
            "iteration : 150, loss : 0.0176, accuracy : 99.50\n",
            "iteration : 200, loss : 0.0171, accuracy : 99.51\n",
            "iteration : 250, loss : 0.0170, accuracy : 99.50\n",
            "iteration : 300, loss : 0.0162, accuracy : 99.52\n",
            "iteration : 350, loss : 0.0169, accuracy : 99.51\n",
            "Epoch : 252, training loss : 0.0167, training accuracy : 99.52, test loss : 1.4810, test accuracy : 74.38\n",
            "\n",
            "Epoch: 253\n",
            "iteration :  50, loss : 0.0164, accuracy : 99.45\n",
            "iteration : 100, loss : 0.0176, accuracy : 99.44\n",
            "iteration : 150, loss : 0.0163, accuracy : 99.49\n",
            "iteration : 200, loss : 0.0158, accuracy : 99.52\n",
            "iteration : 250, loss : 0.0167, accuracy : 99.50\n",
            "iteration : 300, loss : 0.0168, accuracy : 99.49\n",
            "iteration : 350, loss : 0.0172, accuracy : 99.47\n",
            "Epoch : 253, training loss : 0.0173, training accuracy : 99.46, test loss : 1.4963, test accuracy : 74.11\n",
            "\n",
            "Epoch: 254\n",
            "iteration :  50, loss : 0.0202, accuracy : 99.33\n",
            "iteration : 100, loss : 0.0233, accuracy : 99.32\n",
            "iteration : 150, loss : 0.0212, accuracy : 99.36\n",
            "iteration : 200, loss : 0.0197, accuracy : 99.43\n",
            "iteration : 250, loss : 0.0189, accuracy : 99.46\n",
            "iteration : 300, loss : 0.0188, accuracy : 99.46\n",
            "iteration : 350, loss : 0.0187, accuracy : 99.45\n",
            "Epoch : 254, training loss : 0.0192, training accuracy : 99.45, test loss : 1.4924, test accuracy : 74.16\n",
            "\n",
            "Epoch: 255\n",
            "iteration :  50, loss : 0.0179, accuracy : 99.45\n",
            "iteration : 100, loss : 0.0166, accuracy : 99.45\n",
            "iteration : 150, loss : 0.0177, accuracy : 99.43\n",
            "iteration : 200, loss : 0.0175, accuracy : 99.45\n",
            "iteration : 250, loss : 0.0175, accuracy : 99.47\n",
            "iteration : 300, loss : 0.0174, accuracy : 99.47\n",
            "iteration : 350, loss : 0.0175, accuracy : 99.46\n",
            "Epoch : 255, training loss : 0.0175, training accuracy : 99.46, test loss : 1.4982, test accuracy : 74.37\n",
            "\n",
            "Epoch: 256\n",
            "iteration :  50, loss : 0.0177, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0177, accuracy : 99.51\n",
            "iteration : 150, loss : 0.0162, accuracy : 99.53\n",
            "iteration : 200, loss : 0.0159, accuracy : 99.54\n",
            "iteration : 250, loss : 0.0159, accuracy : 99.55\n",
            "iteration : 300, loss : 0.0162, accuracy : 99.54\n",
            "iteration : 350, loss : 0.0165, accuracy : 99.53\n",
            "Epoch : 256, training loss : 0.0166, training accuracy : 99.52, test loss : 1.4935, test accuracy : 74.47\n",
            "\n",
            "Epoch: 257\n",
            "iteration :  50, loss : 0.0181, accuracy : 99.42\n",
            "iteration : 100, loss : 0.0182, accuracy : 99.42\n",
            "iteration : 150, loss : 0.0180, accuracy : 99.46\n",
            "iteration : 200, loss : 0.0170, accuracy : 99.49\n",
            "iteration : 250, loss : 0.0172, accuracy : 99.49\n",
            "iteration : 300, loss : 0.0178, accuracy : 99.47\n",
            "iteration : 350, loss : 0.0174, accuracy : 99.48\n",
            "Epoch : 257, training loss : 0.0177, training accuracy : 99.47, test loss : 1.4937, test accuracy : 74.26\n",
            "\n",
            "Epoch: 258\n",
            "iteration :  50, loss : 0.0187, accuracy : 99.42\n",
            "iteration : 100, loss : 0.0185, accuracy : 99.43\n",
            "iteration : 150, loss : 0.0183, accuracy : 99.42\n",
            "iteration : 200, loss : 0.0186, accuracy : 99.41\n",
            "iteration : 250, loss : 0.0182, accuracy : 99.42\n",
            "iteration : 300, loss : 0.0179, accuracy : 99.43\n",
            "iteration : 350, loss : 0.0178, accuracy : 99.44\n",
            "Epoch : 258, training loss : 0.0174, training accuracy : 99.44, test loss : 1.4931, test accuracy : 74.30\n",
            "\n",
            "Epoch: 259\n",
            "iteration :  50, loss : 0.0160, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0157, accuracy : 99.57\n",
            "iteration : 150, loss : 0.0145, accuracy : 99.61\n",
            "iteration : 200, loss : 0.0138, accuracy : 99.63\n",
            "iteration : 250, loss : 0.0143, accuracy : 99.61\n",
            "iteration : 300, loss : 0.0148, accuracy : 99.59\n",
            "iteration : 350, loss : 0.0147, accuracy : 99.59\n",
            "Epoch : 259, training loss : 0.0149, training accuracy : 99.57, test loss : 1.4906, test accuracy : 74.53\n",
            "\n",
            "Epoch: 260\n",
            "iteration :  50, loss : 0.0150, accuracy : 99.55\n",
            "iteration : 100, loss : 0.0154, accuracy : 99.54\n",
            "iteration : 150, loss : 0.0161, accuracy : 99.52\n",
            "iteration : 200, loss : 0.0163, accuracy : 99.50\n",
            "iteration : 250, loss : 0.0167, accuracy : 99.49\n",
            "iteration : 300, loss : 0.0165, accuracy : 99.49\n",
            "iteration : 350, loss : 0.0169, accuracy : 99.49\n",
            "Epoch : 260, training loss : 0.0169, training accuracy : 99.49, test loss : 1.4997, test accuracy : 74.33\n",
            "\n",
            "Epoch: 261\n",
            "iteration :  50, loss : 0.0193, accuracy : 99.41\n",
            "iteration : 100, loss : 0.0187, accuracy : 99.40\n",
            "iteration : 150, loss : 0.0182, accuracy : 99.43\n",
            "iteration : 200, loss : 0.0180, accuracy : 99.44\n",
            "iteration : 250, loss : 0.0179, accuracy : 99.43\n",
            "iteration : 300, loss : 0.0171, accuracy : 99.47\n",
            "iteration : 350, loss : 0.0177, accuracy : 99.46\n",
            "Epoch : 261, training loss : 0.0177, training accuracy : 99.46, test loss : 1.5023, test accuracy : 74.15\n",
            "\n",
            "Epoch: 262\n",
            "iteration :  50, loss : 0.0141, accuracy : 99.53\n",
            "iteration : 100, loss : 0.0143, accuracy : 99.55\n",
            "iteration : 150, loss : 0.0155, accuracy : 99.53\n",
            "iteration : 200, loss : 0.0159, accuracy : 99.52\n",
            "iteration : 250, loss : 0.0163, accuracy : 99.51\n",
            "iteration : 300, loss : 0.0164, accuracy : 99.49\n",
            "iteration : 350, loss : 0.0171, accuracy : 99.48\n",
            "Epoch : 262, training loss : 0.0167, training accuracy : 99.49, test loss : 1.4919, test accuracy : 74.33\n",
            "\n",
            "Epoch: 263\n",
            "iteration :  50, loss : 0.0141, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0153, accuracy : 99.52\n",
            "iteration : 150, loss : 0.0167, accuracy : 99.52\n",
            "iteration : 200, loss : 0.0168, accuracy : 99.51\n",
            "iteration : 250, loss : 0.0170, accuracy : 99.49\n",
            "iteration : 300, loss : 0.0167, accuracy : 99.51\n",
            "iteration : 350, loss : 0.0169, accuracy : 99.49\n",
            "Epoch : 263, training loss : 0.0167, training accuracy : 99.50, test loss : 1.4926, test accuracy : 74.65\n",
            "\n",
            "Epoch: 264\n",
            "iteration :  50, loss : 0.0132, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0146, accuracy : 99.50\n",
            "iteration : 150, loss : 0.0162, accuracy : 99.48\n",
            "iteration : 200, loss : 0.0161, accuracy : 99.52\n",
            "iteration : 250, loss : 0.0162, accuracy : 99.51\n",
            "iteration : 300, loss : 0.0163, accuracy : 99.49\n",
            "iteration : 350, loss : 0.0168, accuracy : 99.48\n",
            "Epoch : 264, training loss : 0.0174, training accuracy : 99.46, test loss : 1.4949, test accuracy : 74.40\n",
            "\n",
            "Epoch: 265\n",
            "iteration :  50, loss : 0.0139, accuracy : 99.59\n",
            "iteration : 100, loss : 0.0145, accuracy : 99.55\n",
            "iteration : 150, loss : 0.0153, accuracy : 99.52\n",
            "iteration : 200, loss : 0.0169, accuracy : 99.49\n",
            "iteration : 250, loss : 0.0171, accuracy : 99.49\n",
            "iteration : 300, loss : 0.0174, accuracy : 99.48\n",
            "iteration : 350, loss : 0.0168, accuracy : 99.51\n",
            "Epoch : 265, training loss : 0.0166, training accuracy : 99.52, test loss : 1.4878, test accuracy : 74.45\n",
            "\n",
            "Epoch: 266\n",
            "iteration :  50, loss : 0.0144, accuracy : 99.59\n",
            "iteration : 100, loss : 0.0172, accuracy : 99.50\n",
            "iteration : 150, loss : 0.0164, accuracy : 99.52\n",
            "iteration : 200, loss : 0.0167, accuracy : 99.50\n",
            "iteration : 250, loss : 0.0167, accuracy : 99.51\n",
            "iteration : 300, loss : 0.0160, accuracy : 99.54\n",
            "iteration : 350, loss : 0.0164, accuracy : 99.54\n",
            "Epoch : 266, training loss : 0.0161, training accuracy : 99.54, test loss : 1.4935, test accuracy : 74.47\n",
            "\n",
            "Epoch: 267\n",
            "iteration :  50, loss : 0.0158, accuracy : 99.50\n",
            "iteration : 100, loss : 0.0171, accuracy : 99.43\n",
            "iteration : 150, loss : 0.0179, accuracy : 99.41\n",
            "iteration : 200, loss : 0.0173, accuracy : 99.45\n",
            "iteration : 250, loss : 0.0178, accuracy : 99.44\n",
            "iteration : 300, loss : 0.0178, accuracy : 99.45\n",
            "iteration : 350, loss : 0.0174, accuracy : 99.46\n",
            "Epoch : 267, training loss : 0.0179, training accuracy : 99.44, test loss : 1.4934, test accuracy : 74.34\n",
            "\n",
            "Epoch: 268\n",
            "iteration :  50, loss : 0.0123, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0128, accuracy : 99.69\n",
            "iteration : 150, loss : 0.0149, accuracy : 99.57\n",
            "iteration : 200, loss : 0.0148, accuracy : 99.58\n",
            "iteration : 250, loss : 0.0151, accuracy : 99.56\n",
            "iteration : 300, loss : 0.0152, accuracy : 99.55\n",
            "iteration : 350, loss : 0.0157, accuracy : 99.54\n",
            "Epoch : 268, training loss : 0.0160, training accuracy : 99.53, test loss : 1.4899, test accuracy : 74.32\n",
            "\n",
            "Epoch: 269\n",
            "iteration :  50, loss : 0.0181, accuracy : 99.45\n",
            "iteration : 100, loss : 0.0196, accuracy : 99.45\n",
            "iteration : 150, loss : 0.0183, accuracy : 99.50\n",
            "iteration : 200, loss : 0.0170, accuracy : 99.53\n",
            "iteration : 250, loss : 0.0168, accuracy : 99.54\n",
            "iteration : 300, loss : 0.0165, accuracy : 99.54\n",
            "iteration : 350, loss : 0.0164, accuracy : 99.53\n",
            "Epoch : 269, training loss : 0.0166, training accuracy : 99.52, test loss : 1.4870, test accuracy : 74.42\n",
            "\n",
            "Epoch: 270\n",
            "iteration :  50, loss : 0.0176, accuracy : 99.39\n",
            "iteration : 100, loss : 0.0164, accuracy : 99.48\n",
            "iteration : 150, loss : 0.0163, accuracy : 99.51\n",
            "iteration : 200, loss : 0.0163, accuracy : 99.50\n",
            "iteration : 250, loss : 0.0161, accuracy : 99.50\n",
            "iteration : 300, loss : 0.0159, accuracy : 99.52\n",
            "iteration : 350, loss : 0.0161, accuracy : 99.51\n",
            "Epoch : 270, training loss : 0.0160, training accuracy : 99.52, test loss : 1.4925, test accuracy : 74.30\n",
            "\n",
            "Epoch: 271\n",
            "iteration :  50, loss : 0.0207, accuracy : 99.38\n",
            "iteration : 100, loss : 0.0187, accuracy : 99.40\n",
            "iteration : 150, loss : 0.0173, accuracy : 99.44\n",
            "iteration : 200, loss : 0.0170, accuracy : 99.46\n",
            "iteration : 250, loss : 0.0167, accuracy : 99.47\n",
            "iteration : 300, loss : 0.0158, accuracy : 99.50\n",
            "iteration : 350, loss : 0.0162, accuracy : 99.50\n",
            "Epoch : 271, training loss : 0.0169, training accuracy : 99.50, test loss : 1.4903, test accuracy : 74.27\n",
            "\n",
            "Epoch: 272\n",
            "iteration :  50, loss : 0.0188, accuracy : 99.48\n",
            "iteration : 100, loss : 0.0172, accuracy : 99.53\n",
            "iteration : 150, loss : 0.0173, accuracy : 99.53\n",
            "iteration : 200, loss : 0.0163, accuracy : 99.56\n",
            "iteration : 250, loss : 0.0163, accuracy : 99.54\n",
            "iteration : 300, loss : 0.0159, accuracy : 99.56\n",
            "iteration : 350, loss : 0.0157, accuracy : 99.56\n",
            "Epoch : 272, training loss : 0.0159, training accuracy : 99.56, test loss : 1.4869, test accuracy : 74.37\n",
            "\n",
            "Epoch: 273\n",
            "iteration :  50, loss : 0.0168, accuracy : 99.45\n",
            "iteration : 100, loss : 0.0154, accuracy : 99.50\n",
            "iteration : 150, loss : 0.0154, accuracy : 99.52\n",
            "iteration : 200, loss : 0.0161, accuracy : 99.49\n",
            "iteration : 250, loss : 0.0164, accuracy : 99.49\n",
            "iteration : 300, loss : 0.0160, accuracy : 99.52\n",
            "iteration : 350, loss : 0.0159, accuracy : 99.52\n",
            "Epoch : 273, training loss : 0.0160, training accuracy : 99.52, test loss : 1.4902, test accuracy : 74.27\n",
            "\n",
            "Epoch: 274\n",
            "iteration :  50, loss : 0.0176, accuracy : 99.52\n",
            "iteration : 100, loss : 0.0172, accuracy : 99.48\n",
            "iteration : 150, loss : 0.0167, accuracy : 99.50\n",
            "iteration : 200, loss : 0.0159, accuracy : 99.51\n",
            "iteration : 250, loss : 0.0161, accuracy : 99.49\n",
            "iteration : 300, loss : 0.0164, accuracy : 99.50\n",
            "iteration : 350, loss : 0.0166, accuracy : 99.50\n",
            "Epoch : 274, training loss : 0.0166, training accuracy : 99.50, test loss : 1.4951, test accuracy : 74.34\n",
            "\n",
            "Epoch: 275\n",
            "iteration :  50, loss : 0.0111, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0140, accuracy : 99.57\n",
            "iteration : 150, loss : 0.0140, accuracy : 99.59\n",
            "iteration : 200, loss : 0.0148, accuracy : 99.59\n",
            "iteration : 250, loss : 0.0147, accuracy : 99.57\n",
            "iteration : 300, loss : 0.0148, accuracy : 99.56\n",
            "iteration : 350, loss : 0.0149, accuracy : 99.56\n",
            "Epoch : 275, training loss : 0.0150, training accuracy : 99.56, test loss : 1.4888, test accuracy : 74.31\n",
            "\n",
            "Epoch: 276\n",
            "iteration :  50, loss : 0.0119, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0139, accuracy : 99.59\n",
            "iteration : 150, loss : 0.0134, accuracy : 99.62\n",
            "iteration : 200, loss : 0.0147, accuracy : 99.59\n",
            "iteration : 250, loss : 0.0143, accuracy : 99.60\n",
            "iteration : 300, loss : 0.0144, accuracy : 99.60\n",
            "iteration : 350, loss : 0.0149, accuracy : 99.56\n",
            "Epoch : 276, training loss : 0.0152, training accuracy : 99.55, test loss : 1.4949, test accuracy : 74.45\n",
            "\n",
            "Epoch: 277\n",
            "iteration :  50, loss : 0.0177, accuracy : 99.47\n",
            "iteration : 100, loss : 0.0160, accuracy : 99.52\n",
            "iteration : 150, loss : 0.0162, accuracy : 99.54\n",
            "iteration : 200, loss : 0.0159, accuracy : 99.56\n",
            "iteration : 250, loss : 0.0164, accuracy : 99.54\n",
            "iteration : 300, loss : 0.0163, accuracy : 99.54\n",
            "iteration : 350, loss : 0.0165, accuracy : 99.54\n",
            "Epoch : 277, training loss : 0.0160, training accuracy : 99.54, test loss : 1.4954, test accuracy : 74.30\n",
            "\n",
            "Epoch: 278\n",
            "iteration :  50, loss : 0.0210, accuracy : 99.34\n",
            "iteration : 100, loss : 0.0188, accuracy : 99.38\n",
            "iteration : 150, loss : 0.0191, accuracy : 99.41\n",
            "iteration : 200, loss : 0.0188, accuracy : 99.43\n",
            "iteration : 250, loss : 0.0181, accuracy : 99.45\n",
            "iteration : 300, loss : 0.0178, accuracy : 99.45\n",
            "iteration : 350, loss : 0.0185, accuracy : 99.41\n",
            "Epoch : 278, training loss : 0.0180, training accuracy : 99.43, test loss : 1.4967, test accuracy : 74.36\n",
            "\n",
            "Epoch: 279\n",
            "iteration :  50, loss : 0.0126, accuracy : 99.62\n",
            "iteration : 100, loss : 0.0141, accuracy : 99.55\n",
            "iteration : 150, loss : 0.0140, accuracy : 99.55\n",
            "iteration : 200, loss : 0.0152, accuracy : 99.50\n",
            "iteration : 250, loss : 0.0152, accuracy : 99.52\n",
            "iteration : 300, loss : 0.0154, accuracy : 99.51\n",
            "iteration : 350, loss : 0.0157, accuracy : 99.52\n",
            "Epoch : 279, training loss : 0.0159, training accuracy : 99.51, test loss : 1.5013, test accuracy : 74.30\n",
            "\n",
            "Epoch: 280\n",
            "iteration :  50, loss : 0.0149, accuracy : 99.59\n",
            "iteration : 100, loss : 0.0147, accuracy : 99.56\n",
            "iteration : 150, loss : 0.0134, accuracy : 99.63\n",
            "iteration : 200, loss : 0.0144, accuracy : 99.60\n",
            "iteration : 250, loss : 0.0145, accuracy : 99.59\n",
            "iteration : 300, loss : 0.0140, accuracy : 99.60\n",
            "iteration : 350, loss : 0.0144, accuracy : 99.59\n",
            "Epoch : 280, training loss : 0.0145, training accuracy : 99.59, test loss : 1.4984, test accuracy : 74.34\n",
            "\n",
            "Epoch: 281\n",
            "iteration :  50, loss : 0.0178, accuracy : 99.47\n",
            "iteration : 100, loss : 0.0157, accuracy : 99.55\n",
            "iteration : 150, loss : 0.0154, accuracy : 99.57\n",
            "iteration : 200, loss : 0.0162, accuracy : 99.52\n",
            "iteration : 250, loss : 0.0165, accuracy : 99.52\n",
            "iteration : 300, loss : 0.0159, accuracy : 99.54\n",
            "iteration : 350, loss : 0.0155, accuracy : 99.55\n",
            "Epoch : 281, training loss : 0.0156, training accuracy : 99.54, test loss : 1.4960, test accuracy : 74.25\n",
            "\n",
            "Epoch: 282\n",
            "iteration :  50, loss : 0.0174, accuracy : 99.42\n",
            "iteration : 100, loss : 0.0171, accuracy : 99.48\n",
            "iteration : 150, loss : 0.0157, accuracy : 99.53\n",
            "iteration : 200, loss : 0.0168, accuracy : 99.50\n",
            "iteration : 250, loss : 0.0165, accuracy : 99.51\n",
            "iteration : 300, loss : 0.0164, accuracy : 99.51\n",
            "iteration : 350, loss : 0.0162, accuracy : 99.52\n",
            "Epoch : 282, training loss : 0.0162, training accuracy : 99.52, test loss : 1.5009, test accuracy : 74.34\n",
            "\n",
            "Epoch: 283\n",
            "iteration :  50, loss : 0.0144, accuracy : 99.61\n",
            "iteration : 100, loss : 0.0156, accuracy : 99.55\n",
            "iteration : 150, loss : 0.0167, accuracy : 99.51\n",
            "iteration : 200, loss : 0.0167, accuracy : 99.51\n",
            "iteration : 250, loss : 0.0171, accuracy : 99.50\n",
            "iteration : 300, loss : 0.0175, accuracy : 99.50\n",
            "iteration : 350, loss : 0.0169, accuracy : 99.52\n",
            "Epoch : 283, training loss : 0.0169, training accuracy : 99.51, test loss : 1.4949, test accuracy : 74.18\n",
            "\n",
            "Epoch: 284\n",
            "iteration :  50, loss : 0.0237, accuracy : 99.25\n",
            "iteration : 100, loss : 0.0219, accuracy : 99.32\n",
            "iteration : 150, loss : 0.0192, accuracy : 99.41\n",
            "iteration : 200, loss : 0.0179, accuracy : 99.47\n",
            "iteration : 250, loss : 0.0178, accuracy : 99.47\n",
            "iteration : 300, loss : 0.0178, accuracy : 99.49\n",
            "iteration : 350, loss : 0.0175, accuracy : 99.49\n",
            "Epoch : 284, training loss : 0.0173, training accuracy : 99.50, test loss : 1.4930, test accuracy : 74.51\n",
            "\n",
            "Epoch: 285\n",
            "iteration :  50, loss : 0.0159, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0171, accuracy : 99.51\n",
            "iteration : 150, loss : 0.0168, accuracy : 99.51\n",
            "iteration : 200, loss : 0.0159, accuracy : 99.54\n",
            "iteration : 250, loss : 0.0156, accuracy : 99.55\n",
            "iteration : 300, loss : 0.0158, accuracy : 99.54\n",
            "iteration : 350, loss : 0.0160, accuracy : 99.54\n",
            "Epoch : 285, training loss : 0.0160, training accuracy : 99.54, test loss : 1.4988, test accuracy : 74.34\n",
            "\n",
            "Epoch: 286\n",
            "iteration :  50, loss : 0.0172, accuracy : 99.41\n",
            "iteration : 100, loss : 0.0174, accuracy : 99.45\n",
            "iteration : 150, loss : 0.0180, accuracy : 99.39\n",
            "iteration : 200, loss : 0.0174, accuracy : 99.41\n",
            "iteration : 250, loss : 0.0179, accuracy : 99.42\n",
            "iteration : 300, loss : 0.0173, accuracy : 99.45\n",
            "iteration : 350, loss : 0.0172, accuracy : 99.46\n",
            "Epoch : 286, training loss : 0.0174, training accuracy : 99.45, test loss : 1.4952, test accuracy : 74.30\n",
            "\n",
            "Epoch: 287\n",
            "iteration :  50, loss : 0.0172, accuracy : 99.53\n",
            "iteration : 100, loss : 0.0170, accuracy : 99.51\n",
            "iteration : 150, loss : 0.0171, accuracy : 99.49\n",
            "iteration : 200, loss : 0.0164, accuracy : 99.51\n",
            "iteration : 250, loss : 0.0164, accuracy : 99.51\n",
            "iteration : 300, loss : 0.0161, accuracy : 99.51\n",
            "iteration : 350, loss : 0.0161, accuracy : 99.52\n",
            "Epoch : 287, training loss : 0.0158, training accuracy : 99.52, test loss : 1.4977, test accuracy : 74.29\n",
            "\n",
            "Epoch: 288\n",
            "iteration :  50, loss : 0.0199, accuracy : 99.39\n",
            "iteration : 100, loss : 0.0188, accuracy : 99.43\n",
            "iteration : 150, loss : 0.0178, accuracy : 99.47\n",
            "iteration : 200, loss : 0.0174, accuracy : 99.48\n",
            "iteration : 250, loss : 0.0166, accuracy : 99.53\n",
            "iteration : 300, loss : 0.0163, accuracy : 99.55\n",
            "iteration : 350, loss : 0.0162, accuracy : 99.54\n",
            "Epoch : 288, training loss : 0.0163, training accuracy : 99.54, test loss : 1.4984, test accuracy : 74.43\n",
            "\n",
            "Epoch: 289\n",
            "iteration :  50, loss : 0.0173, accuracy : 99.52\n",
            "iteration : 100, loss : 0.0181, accuracy : 99.45\n",
            "iteration : 150, loss : 0.0163, accuracy : 99.51\n",
            "iteration : 200, loss : 0.0161, accuracy : 99.53\n",
            "iteration : 250, loss : 0.0164, accuracy : 99.52\n",
            "iteration : 300, loss : 0.0160, accuracy : 99.54\n",
            "iteration : 350, loss : 0.0160, accuracy : 99.53\n",
            "Epoch : 289, training loss : 0.0159, training accuracy : 99.53, test loss : 1.4931, test accuracy : 74.34\n",
            "\n",
            "Epoch: 290\n",
            "iteration :  50, loss : 0.0173, accuracy : 99.42\n",
            "iteration : 100, loss : 0.0168, accuracy : 99.41\n",
            "iteration : 150, loss : 0.0150, accuracy : 99.51\n",
            "iteration : 200, loss : 0.0146, accuracy : 99.52\n",
            "iteration : 250, loss : 0.0147, accuracy : 99.51\n",
            "iteration : 300, loss : 0.0152, accuracy : 99.51\n",
            "iteration : 350, loss : 0.0158, accuracy : 99.49\n",
            "Epoch : 290, training loss : 0.0162, training accuracy : 99.49, test loss : 1.4926, test accuracy : 74.24\n",
            "\n",
            "Epoch: 291\n",
            "iteration :  50, loss : 0.0136, accuracy : 99.59\n",
            "iteration : 100, loss : 0.0145, accuracy : 99.57\n",
            "iteration : 150, loss : 0.0153, accuracy : 99.52\n",
            "iteration : 200, loss : 0.0158, accuracy : 99.48\n",
            "iteration : 250, loss : 0.0158, accuracy : 99.49\n",
            "iteration : 300, loss : 0.0157, accuracy : 99.50\n",
            "iteration : 350, loss : 0.0154, accuracy : 99.51\n",
            "Epoch : 291, training loss : 0.0155, training accuracy : 99.52, test loss : 1.4941, test accuracy : 74.36\n",
            "\n",
            "Epoch: 292\n",
            "iteration :  50, loss : 0.0182, accuracy : 99.41\n",
            "iteration : 100, loss : 0.0174, accuracy : 99.45\n",
            "iteration : 150, loss : 0.0180, accuracy : 99.40\n",
            "iteration : 200, loss : 0.0179, accuracy : 99.42\n",
            "iteration : 250, loss : 0.0175, accuracy : 99.44\n",
            "iteration : 300, loss : 0.0169, accuracy : 99.46\n",
            "iteration : 350, loss : 0.0170, accuracy : 99.45\n",
            "Epoch : 292, training loss : 0.0169, training accuracy : 99.46, test loss : 1.4974, test accuracy : 74.18\n",
            "\n",
            "Epoch: 293\n",
            "iteration :  50, loss : 0.0172, accuracy : 99.52\n",
            "iteration : 100, loss : 0.0174, accuracy : 99.48\n",
            "iteration : 150, loss : 0.0174, accuracy : 99.49\n",
            "iteration : 200, loss : 0.0169, accuracy : 99.50\n",
            "iteration : 250, loss : 0.0165, accuracy : 99.51\n",
            "iteration : 300, loss : 0.0168, accuracy : 99.49\n",
            "iteration : 350, loss : 0.0169, accuracy : 99.48\n",
            "Epoch : 293, training loss : 0.0169, training accuracy : 99.49, test loss : 1.4922, test accuracy : 74.27\n",
            "\n",
            "Epoch: 294\n",
            "iteration :  50, loss : 0.0152, accuracy : 99.52\n",
            "iteration : 100, loss : 0.0150, accuracy : 99.52\n",
            "iteration : 150, loss : 0.0154, accuracy : 99.51\n",
            "iteration : 200, loss : 0.0151, accuracy : 99.51\n",
            "iteration : 250, loss : 0.0152, accuracy : 99.53\n",
            "iteration : 300, loss : 0.0155, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0149, accuracy : 99.54\n",
            "Epoch : 294, training loss : 0.0150, training accuracy : 99.54, test loss : 1.4892, test accuracy : 74.31\n",
            "\n",
            "Epoch: 295\n",
            "iteration :  50, loss : 0.0131, accuracy : 99.62\n",
            "iteration : 100, loss : 0.0147, accuracy : 99.56\n",
            "iteration : 150, loss : 0.0147, accuracy : 99.56\n",
            "iteration : 200, loss : 0.0145, accuracy : 99.53\n",
            "iteration : 250, loss : 0.0148, accuracy : 99.52\n",
            "iteration : 300, loss : 0.0157, accuracy : 99.51\n",
            "iteration : 350, loss : 0.0158, accuracy : 99.52\n",
            "Epoch : 295, training loss : 0.0157, training accuracy : 99.51, test loss : 1.4934, test accuracy : 74.24\n",
            "\n",
            "Epoch: 296\n",
            "iteration :  50, loss : 0.0158, accuracy : 99.55\n",
            "iteration : 100, loss : 0.0168, accuracy : 99.48\n",
            "iteration : 150, loss : 0.0156, accuracy : 99.52\n",
            "iteration : 200, loss : 0.0156, accuracy : 99.52\n",
            "iteration : 250, loss : 0.0159, accuracy : 99.53\n",
            "iteration : 300, loss : 0.0161, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0157, accuracy : 99.55\n",
            "Epoch : 296, training loss : 0.0156, training accuracy : 99.54, test loss : 1.4926, test accuracy : 74.38\n",
            "\n",
            "Epoch: 297\n",
            "iteration :  50, loss : 0.0123, accuracy : 99.62\n",
            "iteration : 100, loss : 0.0140, accuracy : 99.57\n",
            "iteration : 150, loss : 0.0141, accuracy : 99.56\n",
            "iteration : 200, loss : 0.0152, accuracy : 99.52\n",
            "iteration : 250, loss : 0.0157, accuracy : 99.51\n",
            "iteration : 300, loss : 0.0153, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0159, accuracy : 99.51\n",
            "Epoch : 297, training loss : 0.0160, training accuracy : 99.53, test loss : 1.4980, test accuracy : 74.31\n",
            "\n",
            "Epoch: 298\n",
            "iteration :  50, loss : 0.0173, accuracy : 99.41\n",
            "iteration : 100, loss : 0.0160, accuracy : 99.45\n",
            "iteration : 150, loss : 0.0174, accuracy : 99.42\n",
            "iteration : 200, loss : 0.0164, accuracy : 99.46\n",
            "iteration : 250, loss : 0.0165, accuracy : 99.49\n",
            "iteration : 300, loss : 0.0164, accuracy : 99.50\n",
            "iteration : 350, loss : 0.0163, accuracy : 99.51\n",
            "Epoch : 298, training loss : 0.0162, training accuracy : 99.51, test loss : 1.4944, test accuracy : 74.50\n",
            "\n",
            "Epoch: 299\n",
            "iteration :  50, loss : 0.0173, accuracy : 99.34\n",
            "iteration : 100, loss : 0.0165, accuracy : 99.44\n",
            "iteration : 150, loss : 0.0154, accuracy : 99.49\n",
            "iteration : 200, loss : 0.0154, accuracy : 99.54\n",
            "iteration : 250, loss : 0.0157, accuracy : 99.51\n",
            "iteration : 300, loss : 0.0153, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0149, accuracy : 99.54\n",
            "Epoch : 299, training loss : 0.0150, training accuracy : 99.54, test loss : 1.4872, test accuracy : 74.36\n",
            "\n",
            "Epoch: 300\n",
            "iteration :  50, loss : 0.0163, accuracy : 99.62\n",
            "iteration : 100, loss : 0.0164, accuracy : 99.59\n",
            "iteration : 150, loss : 0.0162, accuracy : 99.53\n",
            "iteration : 200, loss : 0.0162, accuracy : 99.54\n",
            "iteration : 250, loss : 0.0157, accuracy : 99.56\n",
            "iteration : 300, loss : 0.0171, accuracy : 99.51\n",
            "iteration : 350, loss : 0.0167, accuracy : 99.51\n",
            "Epoch : 300, training loss : 0.0166, training accuracy : 99.51, test loss : 1.4874, test accuracy : 74.32\n"
          ]
        }
      ],
      "source": [
        "#------------------------------Main---------------------------------------------\n",
        "config = {\n",
        "    'lr': 0.01,\n",
        "    'momentum': 0.9,\n",
        "}\n",
        "train_loss_ = []\n",
        "train_acc_ = []\n",
        "test_loss_ = []\n",
        "test_acc_ = []\n",
        "#ResNet 34\n",
        "net = ResNet34().to('cuda')\n",
        "criterion = nn.CrossEntropyLoss().to('cuda')\n",
        "optimizer = optim.SGD(net.parameters(), lr=config['lr'],\n",
        "                      momentum=config['momentum'])\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\n",
        "\n",
        "for epoch in range(1, 301):\n",
        "    train_loss, train_acc = train(epoch, net, criterion, trainloader, scheduler)\n",
        "    test_loss, test_acc = test(epoch, net, criterion, testloader)\n",
        "    \n",
        "    print((\"Epoch : %3d, training loss : %0.4f, training accuracy : %2.2f, test loss \" + \\\n",
        "      \": %0.4f, test accuracy : %2.2f\") % (epoch, train_loss, train_acc, test_loss, test_acc))\n",
        "    \n",
        "    train_loss_.append(train_loss)\n",
        "    test_loss_.append(test_loss)\n",
        "    train_acc_.append(train_acc)\n",
        "    test_acc_.append(test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "81l23SSrYG7r",
        "outputId": "7935d8a2-76b5-4a3b-cc6d-1860567f95e4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(range(len(train_loss_)), train_loss_, 'b')\n",
        "plt.plot(range(len(test_loss_)), test_loss_, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"ResNet34: Loss vs Number of epochs\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "PgRzBZnLb9oq",
        "outputId": "dffe5f04-1491-4bcf-92fa-29954a971d9f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(range(len(train_acc_)), train_acc_, 'b')\n",
        "plt.plot(range(len(test_acc_)), test_acc_, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"ResNet34: Accuracy vs Number of epochs\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "id": "ArgupDVRwB8i",
        "outputId": "7a6857e2-4b87-48e3-ec4c-82c2da11c1b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 1\n",
            "iteration :  50, loss : 1.9576, accuracy : 27.44\n",
            "iteration : 100, loss : 1.8206, accuracy : 32.25\n",
            "iteration : 150, loss : 1.7145, accuracy : 36.39\n",
            "iteration : 200, loss : 1.6430, accuracy : 39.11\n",
            "iteration : 250, loss : 1.5699, accuracy : 42.02\n",
            "iteration : 300, loss : 1.5136, accuracy : 44.29\n",
            "iteration : 350, loss : 1.4667, accuracy : 46.18\n",
            "epoch :   1, training loss : 1.4241, training accuracy : 47.81, test loss : 1.1884, test accuracy : 58.42\n",
            "\n",
            "Epoch: 2\n",
            "iteration :  50, loss : 1.0251, accuracy : 63.84\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-cc4225c6e06c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-0515b8342e9c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, net, criterion, trainloader, scheduler)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# main body\n",
        "config = {\n",
        "    'lr': 0.01,\n",
        "    'momentum': 0.9,\n",
        "    'weight_decay': 5e-4\n",
        "}\n",
        "\n",
        "\n",
        "net = ResNet18().to('cuda')\n",
        "criterion = nn.CrossEntropyLoss().to('cuda')\n",
        "optimizer = optim.SGD(net.parameters(), lr=config['lr'],\n",
        "                      momentum=config['momentum'], weight_decay=config['weight_decay'])\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "\n",
        "for epoch in range(1, 200):\n",
        "    train_loss, train_acc = train(epoch, net, criterion, trainloader, scheduler)\n",
        "    test_loss, test_acc = test(epoch, net, criterion, testloader)\n",
        "    \n",
        "    print((\"Epoch : %3d, training loss : %0.4f, training accuracy : %2.2f, test loss \" + \\\n",
        "      \": %0.4f, test accuracy : %2.2f\") % (epoch, train_loss, train_acc, test_loss, test_acc))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "5e-4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3dnchRvgsIh",
        "outputId": "192c5271-8d64-4f5c-f047-c09375cc578f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0005"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "1e-2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-7CTXbhgsn3",
        "outputId": "092221a8-4d80-4c48-939f-655dcf5a0aee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.01"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jpkJiS0QguTc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "DL_GroupProject_DataAug.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "21a8b3095f71462fa150dbda57845c26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cbdc166f546545c18b74e4be74807802",
              "IPY_MODEL_ca5d8d313be841b180832c8adebed4e4",
              "IPY_MODEL_0c94233cf11047299a7804e4e0d73fa7"
            ],
            "layout": "IPY_MODEL_c10bd99090f84f8988c7fee82f6c81ce"
          }
        },
        "cbdc166f546545c18b74e4be74807802": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33f6c9c5ea0a485c9c6a9e92e51ab00a",
            "placeholder": "",
            "style": "IPY_MODEL_1a9181f148c9430883b16efea94e8c47",
            "value": ""
          }
        },
        "ca5d8d313be841b180832c8adebed4e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78cde31aef4a4816bedf5c3fe0c5e5ae",
            "max": 169001437,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5bd017a1a8bf44fc997eae7dda56c33e",
            "value": 169001437
          }
        },
        "0c94233cf11047299a7804e4e0d73fa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26d2a7ff54a04f58bfc9c508a12496bd",
            "placeholder": "",
            "style": "IPY_MODEL_5afcaa535195476185b666706120cb78",
            "value": " 169001984/? [00:03&lt;00:00, 51607717.02it/s]"
          }
        },
        "c10bd99090f84f8988c7fee82f6c81ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33f6c9c5ea0a485c9c6a9e92e51ab00a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a9181f148c9430883b16efea94e8c47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78cde31aef4a4816bedf5c3fe0c5e5ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bd017a1a8bf44fc997eae7dda56c33e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "26d2a7ff54a04f58bfc9c508a12496bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5afcaa535195476185b666706120cb78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}