{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfGrai_Qt7Ny"
      },
      "outputs": [],
      "source": [
        "# import all libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "40fedcaabd624ab3b95cfebf95034b15",
            "e380bbe9b15f42fd8cfa239b3379b449",
            "4eff03d9edec444b8daa57e55e8688b8",
            "ee95313186f9486c99c9198a5854d59c",
            "65dbc664a18941669908f3b200809529",
            "3081d16b112b495d9e1c371ab1776bbc",
            "27370b2f8fca438ab3cd62ea6e121faa",
            "9462c055aebf44b2a96146983bd119a6",
            "a09f4dd072984101bd97fca674ed4bb0",
            "ce7d6307fb584989a2afd7a1b9e49bfa",
            "042e06c739764160aa47460391c05f77"
          ]
        },
        "id": "VgAiImV0uURP",
        "outputId": "5f57c9e3-1f2e-4889-deca-d36e79cfa191"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/169001437 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "40fedcaabd624ab3b95cfebf95034b15"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# base setting for cifar100\n",
        "transform_train = transforms.Compose([\n",
        "    #transforms.RandomCrop(32, padding=4),\n",
        "    #transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5074,0.4867,0.4411),(0.2011,0.1987,0.2025)),\n",
        "    #transforms.RandomErasing(value = (0.5074,0.4867,0.4411))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5074,0.4867,0.4411),(0.2011,0.1987,0.2025)),\n",
        "])\n",
        "\n",
        "#download CIFAR100 dataset\n",
        "trainset = torchvision.datasets.CIFAR100(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR100(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "# we can use a larger batch size during test, because we do not save \n",
        "# intermediate variables for gradient computation, which leaves more memory\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n",
        "\n",
        "#classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "#           'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for image,label in trainset:\n",
        "    print(\"Image shape: \",image.shape)\n",
        "    print(\"Image tensor: \", image)\n",
        "    print(\"Label: \", label)\n",
        "    break"
      ],
      "metadata": {
        "id": "r9w0GtgVNgIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(trainset.classes)"
      ],
      "metadata": {
        "id": "gvORam6KNxvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_classes_items = dict()\n",
        "\n",
        "for train_item in trainset:\n",
        "    label = trainset.classes[train_item[1]]\n",
        "    if label not in train_classes_items:\n",
        "        train_classes_items[label] = 1\n",
        "    else:\n",
        "        train_classes_items[label] += 1\n",
        "\n",
        "train_classes_items"
      ],
      "metadata": {
        "id": "B_YV8xb0N-NA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_classes_items = dict()\n",
        "for test_item in testset:\n",
        "    label = testset.classes[test_item[1]]\n",
        "    if label not in test_classes_items:\n",
        "        test_classes_items[label] = 1\n",
        "    else:\n",
        "        test_classes_items[label] += 1\n",
        "\n",
        "test_classes_items"
      ],
      "metadata": {
        "id": "yNET6eDxOKm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iP2YfD1gEUC-"
      },
      "source": [
        "See more examples at: https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9e4QhB5B89H"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hldipDVsv-Jt"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "def train(epoch, net, criterion, trainloader, scheduler):\n",
        "    device = 'cuda'\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        if (batch_idx+1) % 50 == 0:\n",
        "          print(\"iteration : %3d, loss : %0.4f, accuracy : %2.2f\" % (batch_idx+1, train_loss/(batch_idx+1), 100.*correct/total))\n",
        "\n",
        "    scheduler.step()\n",
        "    return train_loss/(batch_idx+1), 100.*correct/total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgyCI0U08i2h"
      },
      "source": [
        "Test performance on the test set. Note the use of `torch.inference_mode()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkooK-hQu4a6"
      },
      "outputs": [],
      "source": [
        "def test(epoch, net, criterion, testloader):\n",
        "    device = 'cuda'\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.inference_mode():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    return test_loss/(batch_idx+1), 100.*correct/total\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEj8J7xqwAxD"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(net, acc, epoch):\n",
        "    # Save checkpoint.\n",
        "    print('Saving..')\n",
        "    state = {\n",
        "        'net': net.state_dict(),\n",
        "        'acc': acc,\n",
        "        'epoch': epoch,\n",
        "    }\n",
        "    if not os.path.isdir('checkpoint'):\n",
        "        os.mkdir('checkpoint')\n",
        "    torch.save(state, './checkpoint/ckpt.pth')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlCAjBEWwXNo"
      },
      "outputs": [],
      "source": [
        "# defining resnet models\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=100):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        # This is the \"stem\"\n",
        "        # For CIFAR (32x32 images), it does not perform downsampling\n",
        "        # It should downsample for ImageNet\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        # four stages with three downsampling\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
        "\n",
        "\n",
        "def test_resnet18():\n",
        "    net = ResNet18()\n",
        "    y = net(torch.randn(1, 3, 32, 32))\n",
        "    print(y.size())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9J1Pp8RUVEF",
        "outputId": "fb78b899-7295-463b-b33a-331a6a3412ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "iteration :  50, loss : 4.4796, accuracy : 3.25\n",
            "iteration : 100, loss : 4.2714, accuracy : 5.41\n",
            "iteration : 150, loss : 4.1341, accuracy : 6.85\n",
            "iteration : 200, loss : 4.0197, accuracy : 8.11\n",
            "iteration : 250, loss : 3.9311, accuracy : 9.51\n",
            "iteration : 300, loss : 3.8491, accuracy : 10.90\n",
            "iteration : 350, loss : 3.7730, accuracy : 12.01\n",
            "Epoch :   1, training loss : 3.7157, training accuracy : 12.88, test loss : 3.2159, test accuracy : 21.20\n",
            "\n",
            "Epoch: 2\n",
            "iteration :  50, loss : 3.0675, accuracy : 23.89\n",
            "iteration : 100, loss : 3.0137, accuracy : 24.89\n",
            "iteration : 150, loss : 2.9825, accuracy : 25.36\n",
            "iteration : 200, loss : 2.9392, accuracy : 26.07\n",
            "iteration : 250, loss : 2.9034, accuracy : 26.83\n",
            "iteration : 300, loss : 2.8724, accuracy : 27.47\n",
            "iteration : 350, loss : 2.8307, accuracy : 28.18\n",
            "Epoch :   2, training loss : 2.8061, training accuracy : 28.74, test loss : 2.6538, test accuracy : 31.93\n",
            "\n",
            "Epoch: 3\n",
            "iteration :  50, loss : 2.3077, accuracy : 38.00\n",
            "iteration : 100, loss : 2.3154, accuracy : 38.32\n",
            "iteration : 150, loss : 2.2973, accuracy : 38.77\n",
            "iteration : 200, loss : 2.2762, accuracy : 39.21\n",
            "iteration : 250, loss : 2.2487, accuracy : 39.83\n",
            "iteration : 300, loss : 2.2306, accuracy : 40.25\n",
            "iteration : 350, loss : 2.2148, accuracy : 40.69\n",
            "Epoch :   3, training loss : 2.2016, training accuracy : 41.01, test loss : 2.3998, test accuracy : 37.71\n",
            "\n",
            "Epoch: 4\n",
            "iteration :  50, loss : 1.7309, accuracy : 52.23\n",
            "iteration : 100, loss : 1.7570, accuracy : 51.34\n",
            "iteration : 150, loss : 1.7601, accuracy : 51.23\n",
            "iteration : 200, loss : 1.7571, accuracy : 51.21\n",
            "iteration : 250, loss : 1.7584, accuracy : 51.06\n",
            "iteration : 300, loss : 1.7633, accuracy : 50.82\n",
            "iteration : 350, loss : 1.7699, accuracy : 50.67\n",
            "Epoch :   4, training loss : 1.7659, training accuracy : 50.72, test loss : 2.1117, test accuracy : 44.70\n",
            "\n",
            "Epoch: 5\n",
            "iteration :  50, loss : 1.3233, accuracy : 62.09\n",
            "iteration : 100, loss : 1.3250, accuracy : 61.77\n",
            "iteration : 150, loss : 1.3408, accuracy : 61.15\n",
            "iteration : 200, loss : 1.3491, accuracy : 60.98\n",
            "iteration : 250, loss : 1.3636, accuracy : 60.67\n",
            "iteration : 300, loss : 1.3785, accuracy : 60.20\n",
            "iteration : 350, loss : 1.3862, accuracy : 60.02\n",
            "Epoch :   5, training loss : 1.3999, training accuracy : 59.56, test loss : 2.1134, test accuracy : 46.50\n",
            "\n",
            "Epoch: 6\n",
            "iteration :  50, loss : 1.0041, accuracy : 70.34\n",
            "iteration : 100, loss : 0.9882, accuracy : 70.53\n",
            "iteration : 150, loss : 1.0041, accuracy : 70.05\n",
            "iteration : 200, loss : 1.0216, accuracy : 69.59\n",
            "iteration : 250, loss : 1.0331, accuracy : 69.30\n",
            "iteration : 300, loss : 1.0471, accuracy : 68.76\n",
            "iteration : 350, loss : 1.0618, accuracy : 68.28\n",
            "Epoch :   6, training loss : 1.0750, training accuracy : 67.89, test loss : 2.2247, test accuracy : 44.55\n",
            "\n",
            "Epoch: 7\n",
            "iteration :  50, loss : 0.7024, accuracy : 78.86\n",
            "iteration : 100, loss : 0.6783, accuracy : 79.51\n",
            "iteration : 150, loss : 0.6737, accuracy : 79.60\n",
            "iteration : 200, loss : 0.6908, accuracy : 79.06\n",
            "iteration : 250, loss : 0.7158, accuracy : 78.19\n",
            "iteration : 300, loss : 0.7300, accuracy : 77.80\n",
            "iteration : 350, loss : 0.7482, accuracy : 77.18\n",
            "Epoch :   7, training loss : 0.7618, training accuracy : 76.65, test loss : 2.4221, test accuracy : 45.32\n",
            "\n",
            "Epoch: 8\n",
            "iteration :  50, loss : 0.4922, accuracy : 84.55\n",
            "iteration : 100, loss : 0.4633, accuracy : 85.73\n",
            "iteration : 150, loss : 0.4529, accuracy : 86.01\n",
            "iteration : 200, loss : 0.4581, accuracy : 85.84\n",
            "iteration : 250, loss : 0.4708, accuracy : 85.38\n",
            "iteration : 300, loss : 0.4823, accuracy : 84.93\n",
            "iteration : 350, loss : 0.4988, accuracy : 84.38\n",
            "Epoch :   8, training loss : 0.5116, training accuracy : 83.96, test loss : 2.4087, test accuracy : 46.56\n",
            "\n",
            "Epoch: 9\n",
            "iteration :  50, loss : 0.3423, accuracy : 89.62\n",
            "iteration : 100, loss : 0.3044, accuracy : 90.98\n",
            "iteration : 150, loss : 0.2897, accuracy : 91.44\n",
            "iteration : 200, loss : 0.2854, accuracy : 91.46\n",
            "iteration : 250, loss : 0.2897, accuracy : 91.15\n",
            "iteration : 300, loss : 0.2980, accuracy : 90.82\n",
            "iteration : 350, loss : 0.3030, accuracy : 90.62\n",
            "Epoch :   9, training loss : 0.3110, training accuracy : 90.31, test loss : 2.5132, test accuracy : 49.24\n",
            "\n",
            "Epoch: 10\n",
            "iteration :  50, loss : 0.1865, accuracy : 94.50\n",
            "iteration : 100, loss : 0.1749, accuracy : 94.87\n",
            "iteration : 150, loss : 0.1652, accuracy : 95.16\n",
            "iteration : 200, loss : 0.1652, accuracy : 95.11\n",
            "iteration : 250, loss : 0.1678, accuracy : 95.03\n",
            "iteration : 300, loss : 0.1698, accuracy : 94.98\n",
            "iteration : 350, loss : 0.1707, accuracy : 94.96\n",
            "Epoch :  10, training loss : 0.1740, training accuracy : 94.85, test loss : 2.5050, test accuracy : 49.22\n",
            "\n",
            "Epoch: 11\n",
            "iteration :  50, loss : 0.1034, accuracy : 97.28\n",
            "iteration : 100, loss : 0.0898, accuracy : 97.69\n",
            "iteration : 150, loss : 0.0870, accuracy : 97.74\n",
            "iteration : 200, loss : 0.0845, accuracy : 97.82\n",
            "iteration : 250, loss : 0.0861, accuracy : 97.83\n",
            "iteration : 300, loss : 0.0860, accuracy : 97.82\n",
            "iteration : 350, loss : 0.0844, accuracy : 97.87\n",
            "Epoch :  11, training loss : 0.0843, training accuracy : 97.86, test loss : 2.4689, test accuracy : 52.94\n",
            "\n",
            "Epoch: 12\n",
            "iteration :  50, loss : 0.0506, accuracy : 98.78\n",
            "iteration : 100, loss : 0.0457, accuracy : 98.94\n",
            "iteration : 150, loss : 0.0424, accuracy : 99.02\n",
            "iteration : 200, loss : 0.0405, accuracy : 99.06\n",
            "iteration : 250, loss : 0.0391, accuracy : 99.11\n",
            "iteration : 300, loss : 0.0386, accuracy : 99.13\n",
            "iteration : 350, loss : 0.0381, accuracy : 99.14\n",
            "Epoch :  12, training loss : 0.0379, training accuracy : 99.15, test loss : 2.5073, test accuracy : 52.49\n",
            "\n",
            "Epoch: 13\n",
            "iteration :  50, loss : 0.0219, accuracy : 99.48\n",
            "iteration : 100, loss : 0.0195, accuracy : 99.66\n",
            "iteration : 150, loss : 0.0176, accuracy : 99.72\n",
            "iteration : 200, loss : 0.0185, accuracy : 99.72\n",
            "iteration : 250, loss : 0.0173, accuracy : 99.74\n",
            "iteration : 300, loss : 0.0171, accuracy : 99.75\n",
            "iteration : 350, loss : 0.0172, accuracy : 99.75\n",
            "Epoch :  13, training loss : 0.0168, training accuracy : 99.76, test loss : 2.2277, test accuracy : 56.51\n",
            "\n",
            "Epoch: 14\n",
            "iteration :  50, loss : 0.0093, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0076, accuracy : 99.92\n",
            "iteration : 150, loss : 0.0069, accuracy : 99.93\n",
            "iteration : 200, loss : 0.0074, accuracy : 99.90\n",
            "iteration : 250, loss : 0.0079, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0083, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0079, accuracy : 99.88\n",
            "Epoch :  14, training loss : 0.0081, training accuracy : 99.88, test loss : 2.2261, test accuracy : 56.50\n",
            "\n",
            "Epoch: 15\n",
            "iteration :  50, loss : 0.0031, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0051, accuracy : 99.95\n",
            "iteration : 150, loss : 0.0055, accuracy : 99.92\n",
            "iteration : 200, loss : 0.0053, accuracy : 99.92\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.93\n",
            "iteration : 300, loss : 0.0048, accuracy : 99.93\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.93\n",
            "Epoch :  15, training loss : 0.0047, training accuracy : 99.93, test loss : 2.1976, test accuracy : 57.14\n",
            "\n",
            "Epoch: 16\n",
            "iteration :  50, loss : 0.0032, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0026, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0027, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0029, accuracy : 99.96\n",
            "iteration : 250, loss : 0.0030, accuracy : 99.96\n",
            "iteration : 300, loss : 0.0033, accuracy : 99.96\n",
            "iteration : 350, loss : 0.0035, accuracy : 99.95\n",
            "Epoch :  16, training loss : 0.0035, training accuracy : 99.95, test loss : 2.2091, test accuracy : 57.09\n",
            "\n",
            "Epoch: 17\n",
            "iteration :  50, loss : 0.0021, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0018, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0019, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0020, accuracy : 99.96\n",
            "iteration : 250, loss : 0.0024, accuracy : 99.96\n",
            "iteration : 300, loss : 0.0025, accuracy : 99.95\n",
            "iteration : 350, loss : 0.0026, accuracy : 99.95\n",
            "Epoch :  17, training loss : 0.0025, training accuracy : 99.95, test loss : 2.2054, test accuracy : 57.42\n",
            "\n",
            "Epoch: 18\n",
            "iteration :  50, loss : 0.0010, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0011, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0014, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0019, accuracy : 99.96\n",
            "iteration : 250, loss : 0.0018, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0018, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0020, accuracy : 99.96\n",
            "Epoch :  18, training loss : 0.0019, training accuracy : 99.96, test loss : 2.2243, test accuracy : 57.01\n",
            "\n",
            "Epoch: 19\n",
            "iteration :  50, loss : 0.0009, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0011, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0010, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0010, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0011, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0012, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0016, accuracy : 99.97\n",
            "Epoch :  19, training loss : 0.0016, training accuracy : 99.97, test loss : 2.2352, test accuracy : 57.18\n",
            "\n",
            "Epoch: 20\n",
            "iteration :  50, loss : 0.0018, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0016, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0017, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0016, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0017, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0017, accuracy : 99.96\n",
            "iteration : 350, loss : 0.0017, accuracy : 99.96\n",
            "Epoch :  20, training loss : 0.0017, training accuracy : 99.96, test loss : 2.2337, test accuracy : 57.35\n",
            "\n",
            "Epoch: 21\n",
            "iteration :  50, loss : 0.0018, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0015, accuracy : 99.96\n",
            "iteration : 150, loss : 0.0013, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0013, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0014, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0013, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0013, accuracy : 99.97\n",
            "Epoch :  21, training loss : 0.0014, training accuracy : 99.97, test loss : 2.2437, test accuracy : 57.20\n",
            "\n",
            "Epoch: 22\n",
            "iteration :  50, loss : 0.0010, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0012, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0014, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0014, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0013, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0012, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0013, accuracy : 99.97\n",
            "Epoch :  22, training loss : 0.0014, training accuracy : 99.97, test loss : 2.2460, test accuracy : 57.01\n",
            "\n",
            "Epoch: 23\n",
            "iteration :  50, loss : 0.0010, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0011, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0010, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0010, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0010, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0011, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0011, accuracy : 99.97\n",
            "Epoch :  23, training loss : 0.0012, training accuracy : 99.97, test loss : 2.2578, test accuracy : 57.27\n",
            "\n",
            "Epoch: 24\n",
            "iteration :  50, loss : 0.0018, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0014, accuracy : 99.96\n",
            "iteration : 150, loss : 0.0012, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0010, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0009, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0010, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0012, accuracy : 99.97\n",
            "Epoch :  24, training loss : 0.0012, training accuracy : 99.97, test loss : 2.2561, test accuracy : 57.38\n",
            "\n",
            "Epoch: 25\n",
            "iteration :  50, loss : 0.0012, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0011, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0009, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0009, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0010, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0010, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0011, accuracy : 99.97\n",
            "Epoch :  25, training loss : 0.0011, training accuracy : 99.97, test loss : 2.2677, test accuracy : 57.41\n",
            "\n",
            "Epoch: 26\n",
            "iteration :  50, loss : 0.0008, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0012, accuracy : 99.96\n",
            "iteration : 150, loss : 0.0010, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0009, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0010, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0011, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0011, accuracy : 99.97\n",
            "Epoch :  26, training loss : 0.0011, training accuracy : 99.97, test loss : 2.2660, test accuracy : 57.34\n",
            "\n",
            "Epoch: 27\n",
            "iteration :  50, loss : 0.0005, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0006, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0006, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0006, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0007, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0008, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0009, accuracy : 99.98\n",
            "Epoch :  27, training loss : 0.0009, training accuracy : 99.97, test loss : 2.2764, test accuracy : 57.29\n",
            "\n",
            "Epoch: 28\n",
            "iteration :  50, loss : 0.0008, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0009, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0007, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0007, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0007, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0008, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0009, accuracy : 99.98\n",
            "Epoch :  28, training loss : 0.0010, training accuracy : 99.97, test loss : 2.2763, test accuracy : 57.50\n",
            "\n",
            "Epoch: 29\n",
            "iteration :  50, loss : 0.0008, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0006, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0007, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0008, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0008, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0008, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0007, accuracy : 99.98\n",
            "Epoch :  29, training loss : 0.0009, training accuracy : 99.97, test loss : 2.2881, test accuracy : 57.33\n",
            "\n",
            "Epoch: 30\n",
            "iteration :  50, loss : 0.0009, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0008, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0011, accuracy : 99.96\n",
            "iteration : 200, loss : 0.0011, accuracy : 99.96\n",
            "iteration : 250, loss : 0.0012, accuracy : 99.96\n",
            "iteration : 300, loss : 0.0011, accuracy : 99.96\n",
            "iteration : 350, loss : 0.0011, accuracy : 99.97\n",
            "Epoch :  30, training loss : 0.0010, training accuracy : 99.97, test loss : 2.2933, test accuracy : 57.04\n",
            "\n",
            "Epoch: 31\n",
            "iteration :  50, loss : 0.0007, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0008, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0007, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0007, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0008, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0008, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0008, accuracy : 99.97\n",
            "Epoch :  31, training loss : 0.0009, training accuracy : 99.97, test loss : 2.2922, test accuracy : 57.23\n",
            "\n",
            "Epoch: 32\n",
            "iteration :  50, loss : 0.0005, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0010, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0008, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0008, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0009, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0008, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0008, accuracy : 99.98\n",
            "Epoch :  32, training loss : 0.0009, training accuracy : 99.97, test loss : 2.2931, test accuracy : 57.28\n",
            "\n",
            "Epoch: 33\n",
            "iteration :  50, loss : 0.0007, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0007, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0007, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0007, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0007, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0008, accuracy : 99.98\n",
            "Epoch :  33, training loss : 0.0008, training accuracy : 99.97, test loss : 2.2936, test accuracy : 57.35\n",
            "\n",
            "Epoch: 34\n",
            "iteration :  50, loss : 0.0008, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0007, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0007, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0009, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0008, accuracy : 99.97\n",
            "Epoch :  34, training loss : 0.0009, training accuracy : 99.97, test loss : 2.2996, test accuracy : 57.25\n",
            "\n",
            "Epoch: 35\n",
            "iteration :  50, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0007, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0008, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0009, accuracy : 99.96\n",
            "iteration : 350, loss : 0.0009, accuracy : 99.96\n",
            "Epoch :  35, training loss : 0.0008, training accuracy : 99.97, test loss : 2.3009, test accuracy : 57.21\n",
            "\n",
            "Epoch: 36\n",
            "iteration :  50, loss : 0.0003, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0005, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0007, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0007, accuracy : 99.98\n",
            "Epoch :  36, training loss : 0.0007, training accuracy : 99.98, test loss : 2.3044, test accuracy : 57.25\n",
            "\n",
            "Epoch: 37\n",
            "iteration :  50, loss : 0.0008, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0009, accuracy : 99.96\n",
            "iteration : 200, loss : 0.0010, accuracy : 99.96\n",
            "iteration : 250, loss : 0.0009, accuracy : 99.96\n",
            "iteration : 300, loss : 0.0009, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0008, accuracy : 99.97\n",
            "Epoch :  37, training loss : 0.0008, training accuracy : 99.97, test loss : 2.3129, test accuracy : 57.30\n",
            "\n",
            "Epoch: 38\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0008, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0007, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0008, accuracy : 99.97\n",
            "Epoch :  38, training loss : 0.0007, training accuracy : 99.97, test loss : 2.3145, test accuracy : 57.33\n",
            "\n",
            "Epoch: 39\n",
            "iteration :  50, loss : 0.0009, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0007, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0007, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0007, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0008, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0008, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0007, accuracy : 99.98\n",
            "Epoch :  39, training loss : 0.0007, training accuracy : 99.97, test loss : 2.3144, test accuracy : 57.08\n",
            "\n",
            "Epoch: 40\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0007, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0008, accuracy : 99.97\n",
            "Epoch :  40, training loss : 0.0007, training accuracy : 99.97, test loss : 2.3112, test accuracy : 57.43\n",
            "\n",
            "Epoch: 41\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0005, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0006, accuracy : 99.98\n",
            "Epoch :  41, training loss : 0.0007, training accuracy : 99.97, test loss : 2.3195, test accuracy : 57.09\n",
            "\n",
            "Epoch: 42\n",
            "iteration :  50, loss : 0.0011, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0008, accuracy : 99.96\n",
            "iteration : 150, loss : 0.0009, accuracy : 99.96\n",
            "iteration : 200, loss : 0.0008, accuracy : 99.96\n",
            "iteration : 250, loss : 0.0008, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0007, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0008, accuracy : 99.97\n",
            "Epoch :  42, training loss : 0.0008, training accuracy : 99.97, test loss : 2.3250, test accuracy : 57.30\n",
            "\n",
            "Epoch: 43\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0005, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0005, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0005, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0006, accuracy : 99.98\n",
            "Epoch :  43, training loss : 0.0007, training accuracy : 99.98, test loss : 2.3237, test accuracy : 57.28\n",
            "\n",
            "Epoch: 44\n",
            "iteration :  50, loss : 0.0008, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0007, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0008, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0007, accuracy : 99.97\n",
            "Epoch :  44, training loss : 0.0007, training accuracy : 99.97, test loss : 2.3262, test accuracy : 57.33\n",
            "\n",
            "Epoch: 45\n",
            "iteration :  50, loss : 0.0007, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0008, accuracy : 99.96\n",
            "iteration : 250, loss : 0.0007, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0007, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0007, accuracy : 99.97\n",
            "Epoch :  45, training loss : 0.0007, training accuracy : 99.97, test loss : 2.3350, test accuracy : 57.17\n",
            "\n",
            "Epoch: 46\n",
            "iteration :  50, loss : 0.0007, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0006, accuracy : 99.97\n",
            "Epoch :  46, training loss : 0.0006, training accuracy : 99.97, test loss : 2.3365, test accuracy : 57.29\n",
            "\n",
            "Epoch: 47\n",
            "iteration :  50, loss : 0.0007, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0007, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0005, accuracy : 99.98\n",
            "Epoch :  47, training loss : 0.0007, training accuracy : 99.97, test loss : 2.3369, test accuracy : 57.43\n",
            "\n",
            "Epoch: 48\n",
            "iteration :  50, loss : 0.0007, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0007, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0007, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0006, accuracy : 99.97\n",
            "Epoch :  48, training loss : 0.0006, training accuracy : 99.97, test loss : 2.3352, test accuracy : 57.41\n",
            "\n",
            "Epoch: 49\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0006, accuracy : 99.97\n",
            "Epoch :  49, training loss : 0.0006, training accuracy : 99.97, test loss : 2.3341, test accuracy : 57.20\n",
            "\n",
            "Epoch: 50\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0006, accuracy : 99.98\n",
            "Epoch :  50, training loss : 0.0006, training accuracy : 99.97, test loss : 2.3384, test accuracy : 57.39\n",
            "\n",
            "Epoch: 51\n",
            "iteration :  50, loss : 0.0008, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0006, accuracy : 99.96\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0007, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0006, accuracy : 99.97\n",
            "Epoch :  51, training loss : 0.0006, training accuracy : 99.97, test loss : 2.3449, test accuracy : 57.20\n",
            "\n",
            "Epoch: 52\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0005, accuracy : 99.98\n",
            "Epoch :  52, training loss : 0.0006, training accuracy : 99.97, test loss : 2.3493, test accuracy : 57.37\n",
            "\n",
            "Epoch: 53\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0007, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0007, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0006, accuracy : 99.98\n",
            "Epoch :  53, training loss : 0.0006, training accuracy : 99.97, test loss : 2.3420, test accuracy : 57.44\n",
            "\n",
            "Epoch: 54\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0006, accuracy : 99.97\n",
            "Epoch :  54, training loss : 0.0006, training accuracy : 99.97, test loss : 2.3446, test accuracy : 57.54\n",
            "\n",
            "Epoch: 55\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0007, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0005, accuracy : 99.98\n",
            "Epoch :  55, training loss : 0.0006, training accuracy : 99.97, test loss : 2.3527, test accuracy : 57.28\n",
            "\n",
            "Epoch: 56\n",
            "iteration :  50, loss : 0.0007, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0006, accuracy : 99.96\n",
            "iteration : 250, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0006, accuracy : 99.97\n",
            "Epoch :  56, training loss : 0.0006, training accuracy : 99.97, test loss : 2.3476, test accuracy : 57.35\n",
            "\n",
            "Epoch: 57\n",
            "iteration :  50, loss : 0.0003, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0008, accuracy : 99.95\n",
            "iteration : 200, loss : 0.0008, accuracy : 99.96\n",
            "iteration : 250, loss : 0.0007, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0006, accuracy : 99.97\n",
            "Epoch :  57, training loss : 0.0006, training accuracy : 99.97, test loss : 2.3487, test accuracy : 57.24\n",
            "\n",
            "Epoch: 58\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0006, accuracy : 99.98\n",
            "Epoch :  58, training loss : 0.0006, training accuracy : 99.98, test loss : 2.3552, test accuracy : 57.31\n",
            "\n",
            "Epoch: 59\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0007, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0009, accuracy : 99.95\n",
            "iteration : 200, loss : 0.0007, accuracy : 99.96\n",
            "iteration : 250, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0005, accuracy : 99.98\n",
            "Epoch :  59, training loss : 0.0006, training accuracy : 99.97, test loss : 2.3597, test accuracy : 57.31\n",
            "\n",
            "Epoch: 60\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0006, accuracy : 99.97\n",
            "Epoch :  60, training loss : 0.0005, training accuracy : 99.97, test loss : 2.3590, test accuracy : 57.44\n",
            "\n",
            "Epoch: 61\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch :  61, training loss : 0.0005, training accuracy : 99.98, test loss : 2.3536, test accuracy : 57.43\n",
            "\n",
            "Epoch: 62\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0005, accuracy : 99.97\n",
            "Epoch :  62, training loss : 0.0006, training accuracy : 99.97, test loss : 2.3625, test accuracy : 57.29\n",
            "\n",
            "Epoch: 63\n",
            "iteration :  50, loss : 0.0008, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0007, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0005, accuracy : 99.98\n",
            "Epoch :  63, training loss : 0.0006, training accuracy : 99.97, test loss : 2.3700, test accuracy : 57.23\n",
            "\n",
            "Epoch: 64\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0005, accuracy : 99.98\n",
            "Epoch :  64, training loss : 0.0005, training accuracy : 99.97, test loss : 2.3615, test accuracy : 57.47\n",
            "\n",
            "Epoch: 65\n",
            "iteration :  50, loss : 0.0003, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0003, accuracy : 100.00\n",
            "iteration : 250, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0005, accuracy : 99.98\n",
            "Epoch :  65, training loss : 0.0005, training accuracy : 99.98, test loss : 2.3750, test accuracy : 57.39\n",
            "\n",
            "Epoch: 66\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0005, accuracy : 99.97\n",
            "Epoch :  66, training loss : 0.0005, training accuracy : 99.97, test loss : 2.3718, test accuracy : 57.36\n",
            "\n",
            "Epoch: 67\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0005, accuracy : 99.98\n",
            "Epoch :  67, training loss : 0.0005, training accuracy : 99.98, test loss : 2.3710, test accuracy : 57.46\n",
            "\n",
            "Epoch: 68\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch :  68, training loss : 0.0005, training accuracy : 99.97, test loss : 2.3671, test accuracy : 57.23\n",
            "\n",
            "Epoch: 69\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0006, accuracy : 99.97\n",
            "Epoch :  69, training loss : 0.0006, training accuracy : 99.97, test loss : 2.3681, test accuracy : 57.26\n",
            "\n",
            "Epoch: 70\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0006, accuracy : 99.96\n",
            "iteration : 150, loss : 0.0007, accuracy : 99.95\n",
            "iteration : 200, loss : 0.0005, accuracy : 99.96\n",
            "iteration : 250, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch :  70, training loss : 0.0005, training accuracy : 99.97, test loss : 2.3746, test accuracy : 57.29\n",
            "\n",
            "Epoch: 71\n",
            "iteration :  50, loss : 0.0007, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0006, accuracy : 99.96\n",
            "iteration : 200, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0005, accuracy : 99.97\n",
            "Epoch :  71, training loss : 0.0005, training accuracy : 99.97, test loss : 2.3691, test accuracy : 57.42\n",
            "\n",
            "Epoch: 72\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch :  72, training loss : 0.0005, training accuracy : 99.97, test loss : 2.3770, test accuracy : 57.41\n",
            "\n",
            "Epoch: 73\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0005, accuracy : 99.98\n",
            "Epoch :  73, training loss : 0.0005, training accuracy : 99.98, test loss : 2.3788, test accuracy : 57.41\n",
            "\n",
            "Epoch: 74\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0005, accuracy : 99.98\n",
            "Epoch :  74, training loss : 0.0005, training accuracy : 99.98, test loss : 2.3822, test accuracy : 57.31\n",
            "\n",
            "Epoch: 75\n",
            "iteration :  50, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0005, accuracy : 99.97\n",
            "Epoch :  75, training loss : 0.0005, training accuracy : 99.97, test loss : 2.3801, test accuracy : 57.35\n",
            "\n",
            "Epoch: 76\n",
            "iteration :  50, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch :  76, training loss : 0.0005, training accuracy : 99.97, test loss : 2.3769, test accuracy : 57.50\n",
            "\n",
            "Epoch: 77\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0006, accuracy : 99.96\n",
            "iteration : 250, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0006, accuracy : 99.96\n",
            "Epoch :  77, training loss : 0.0005, training accuracy : 99.97, test loss : 2.3824, test accuracy : 57.40\n",
            "\n",
            "Epoch: 78\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch :  78, training loss : 0.0005, training accuracy : 99.97, test loss : 2.3865, test accuracy : 57.31\n",
            "\n",
            "Epoch: 79\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0003, accuracy : 100.00\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch :  79, training loss : 0.0005, training accuracy : 99.98, test loss : 2.3831, test accuracy : 57.40\n",
            "\n",
            "Epoch: 80\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0006, accuracy : 99.95\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0005, accuracy : 99.96\n",
            "iteration : 250, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch :  80, training loss : 0.0005, training accuracy : 99.97, test loss : 2.3846, test accuracy : 57.28\n",
            "\n",
            "Epoch: 81\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0005, accuracy : 99.97\n",
            "Epoch :  81, training loss : 0.0005, training accuracy : 99.97, test loss : 2.3847, test accuracy : 57.29\n",
            "\n",
            "Epoch: 82\n",
            "iteration :  50, loss : 0.0009, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch :  82, training loss : 0.0005, training accuracy : 99.97, test loss : 2.3857, test accuracy : 57.40\n",
            "\n",
            "Epoch: 83\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0006, accuracy : 99.96\n",
            "iteration : 200, loss : 0.0005, accuracy : 99.96\n",
            "iteration : 250, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0005, accuracy : 99.97\n",
            "Epoch :  83, training loss : 0.0005, training accuracy : 99.97, test loss : 2.3840, test accuracy : 57.27\n",
            "\n",
            "Epoch: 84\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0005, accuracy : 99.97\n",
            "Epoch :  84, training loss : 0.0005, training accuracy : 99.97, test loss : 2.3960, test accuracy : 57.25\n",
            "\n",
            "Epoch: 85\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0006, accuracy : 99.96\n",
            "iteration : 200, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0005, accuracy : 99.97\n",
            "Epoch :  85, training loss : 0.0005, training accuracy : 99.97, test loss : 2.3927, test accuracy : 57.27\n",
            "\n",
            "Epoch: 86\n",
            "iteration :  50, loss : 0.0007, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0006, accuracy : 99.95\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.96\n",
            "iteration : 200, loss : 0.0006, accuracy : 99.96\n",
            "iteration : 250, loss : 0.0006, accuracy : 99.96\n",
            "iteration : 300, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0005, accuracy : 99.97\n",
            "Epoch :  86, training loss : 0.0005, training accuracy : 99.97, test loss : 2.3999, test accuracy : 57.22\n",
            "\n",
            "Epoch: 87\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0005, accuracy : 99.97\n",
            "Epoch :  87, training loss : 0.0005, training accuracy : 99.97, test loss : 2.3965, test accuracy : 57.37\n",
            "\n",
            "Epoch: 88\n",
            "iteration :  50, loss : 0.0007, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch :  88, training loss : 0.0004, training accuracy : 99.97, test loss : 2.3959, test accuracy : 57.34\n",
            "\n",
            "Epoch: 89\n",
            "iteration :  50, loss : 0.0007, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0005, accuracy : 99.98\n",
            "Epoch :  89, training loss : 0.0004, training accuracy : 99.98, test loss : 2.3925, test accuracy : 57.49\n",
            "\n",
            "Epoch: 90\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch :  90, training loss : 0.0005, training accuracy : 99.97, test loss : 2.3969, test accuracy : 57.44\n",
            "\n",
            "Epoch: 91\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0005, accuracy : 99.96\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0005, accuracy : 99.97\n",
            "Epoch :  91, training loss : 0.0005, training accuracy : 99.97, test loss : 2.3920, test accuracy : 57.60\n",
            "\n",
            "Epoch: 92\n",
            "iteration :  50, loss : 0.0003, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0005, accuracy : 99.97\n",
            "Epoch :  92, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4095, test accuracy : 57.14\n",
            "\n",
            "Epoch: 93\n",
            "iteration :  50, loss : 0.0003, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0005, accuracy : 99.98\n",
            "Epoch :  93, training loss : 0.0005, training accuracy : 99.98, test loss : 2.4033, test accuracy : 57.38\n",
            "\n",
            "Epoch: 94\n",
            "iteration :  50, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0005, accuracy : 99.97\n",
            "Epoch :  94, training loss : 0.0004, training accuracy : 99.97, test loss : 2.3952, test accuracy : 57.52\n",
            "\n",
            "Epoch: 95\n",
            "iteration :  50, loss : 0.0003, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch :  95, training loss : 0.0005, training accuracy : 99.97, test loss : 2.4047, test accuracy : 57.19\n",
            "\n",
            "Epoch: 96\n",
            "iteration :  50, loss : 0.0003, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch :  96, training loss : 0.0004, training accuracy : 99.98, test loss : 2.4026, test accuracy : 57.44\n",
            "\n",
            "Epoch: 97\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch :  97, training loss : 0.0004, training accuracy : 99.98, test loss : 2.4002, test accuracy : 57.21\n",
            "\n",
            "Epoch: 98\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch :  98, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4072, test accuracy : 57.32\n",
            "\n",
            "Epoch: 99\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch :  99, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4171, test accuracy : 57.43\n",
            "\n",
            "Epoch: 100\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch : 100, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4033, test accuracy : 57.52\n",
            "\n",
            "Epoch: 101\n",
            "iteration :  50, loss : 0.0003, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch : 101, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4176, test accuracy : 57.38\n",
            "\n",
            "Epoch: 102\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch : 102, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4072, test accuracy : 57.41\n",
            "\n",
            "Epoch: 103\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch : 103, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4072, test accuracy : 57.49\n",
            "\n",
            "Epoch: 104\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0006, accuracy : 99.96\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.96\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch : 104, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4116, test accuracy : 57.23\n",
            "\n",
            "Epoch: 105\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0005, accuracy : 99.98\n",
            "Epoch : 105, training loss : 0.0004, training accuracy : 99.98, test loss : 2.4071, test accuracy : 57.43\n",
            "\n",
            "Epoch: 106\n",
            "iteration :  50, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.96\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch : 106, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4080, test accuracy : 57.52\n",
            "\n",
            "Epoch: 107\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch : 107, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4044, test accuracy : 57.40\n",
            "\n",
            "Epoch: 108\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 108, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4150, test accuracy : 57.30\n",
            "\n",
            "Epoch: 109\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0005, accuracy : 99.96\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch : 109, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4208, test accuracy : 57.49\n",
            "\n",
            "Epoch: 110\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch : 110, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4154, test accuracy : 57.37\n",
            "\n",
            "Epoch: 111\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0005, accuracy : 99.96\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch : 111, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4234, test accuracy : 57.25\n",
            "\n",
            "Epoch: 112\n",
            "iteration :  50, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch : 112, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4169, test accuracy : 57.32\n",
            "\n",
            "Epoch: 113\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0005, accuracy : 99.96\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch : 113, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4271, test accuracy : 57.38\n",
            "\n",
            "Epoch: 114\n",
            "iteration :  50, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.96\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch : 114, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4209, test accuracy : 57.23\n",
            "\n",
            "Epoch: 115\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch : 115, training loss : 0.0004, training accuracy : 99.98, test loss : 2.4202, test accuracy : 57.26\n",
            "\n",
            "Epoch: 116\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch : 116, training loss : 0.0004, training accuracy : 99.98, test loss : 2.4182, test accuracy : 57.32\n",
            "\n",
            "Epoch: 117\n",
            "iteration :  50, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch : 117, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4249, test accuracy : 57.28\n",
            "\n",
            "Epoch: 118\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch : 118, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4231, test accuracy : 57.26\n",
            "\n",
            "Epoch: 119\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 119, training loss : 0.0004, training accuracy : 99.98, test loss : 2.4257, test accuracy : 57.36\n",
            "\n",
            "Epoch: 120\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch : 120, training loss : 0.0004, training accuracy : 99.98, test loss : 2.4230, test accuracy : 57.17\n",
            "\n",
            "Epoch: 121\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 121, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4246, test accuracy : 57.27\n",
            "\n",
            "Epoch: 122\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch : 122, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4264, test accuracy : 57.14\n",
            "\n",
            "Epoch: 123\n",
            "iteration :  50, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 123, training loss : 0.0004, training accuracy : 99.98, test loss : 2.4177, test accuracy : 57.44\n",
            "\n",
            "Epoch: 124\n",
            "iteration :  50, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch : 124, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4298, test accuracy : 57.26\n",
            "\n",
            "Epoch: 125\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch : 125, training loss : 0.0004, training accuracy : 99.98, test loss : 2.4301, test accuracy : 57.34\n",
            "\n",
            "Epoch: 126\n",
            "iteration :  50, loss : 0.0006, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch : 126, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4316, test accuracy : 57.17\n",
            "\n",
            "Epoch: 127\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch : 127, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4356, test accuracy : 57.40\n",
            "\n",
            "Epoch: 128\n",
            "iteration :  50, loss : 0.0003, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch : 128, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4333, test accuracy : 57.29\n",
            "\n",
            "Epoch: 129\n",
            "iteration :  50, loss : 0.0003, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 150, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch : 129, training loss : 0.0004, training accuracy : 99.98, test loss : 2.4358, test accuracy : 57.37\n",
            "\n",
            "Epoch: 130\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 130, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4311, test accuracy : 57.12\n",
            "\n",
            "Epoch: 131\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch : 131, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4358, test accuracy : 57.24\n",
            "\n",
            "Epoch: 132\n",
            "iteration :  50, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 132, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4346, test accuracy : 57.11\n",
            "\n",
            "Epoch: 133\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch : 133, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4193, test accuracy : 57.27\n",
            "\n",
            "Epoch: 134\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch : 134, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4335, test accuracy : 57.05\n",
            "\n",
            "Epoch: 135\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch : 135, training loss : 0.0004, training accuracy : 99.98, test loss : 2.4354, test accuracy : 57.39\n",
            "\n",
            "Epoch: 136\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0005, accuracy : 99.96\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.96\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch : 136, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4368, test accuracy : 57.42\n",
            "\n",
            "Epoch: 137\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 137, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4299, test accuracy : 57.29\n",
            "\n",
            "Epoch: 138\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0005, accuracy : 99.96\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.96\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch : 138, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4450, test accuracy : 57.21\n",
            "\n",
            "Epoch: 139\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch : 139, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4360, test accuracy : 57.42\n",
            "\n",
            "Epoch: 140\n",
            "iteration :  50, loss : 0.0007, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0006, accuracy : 99.96\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch : 140, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4402, test accuracy : 57.25\n",
            "\n",
            "Epoch: 141\n",
            "iteration :  50, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 250, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 141, training loss : 0.0004, training accuracy : 99.98, test loss : 2.4406, test accuracy : 57.15\n",
            "\n",
            "Epoch: 142\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 142, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4389, test accuracy : 57.28\n",
            "\n",
            "Epoch: 143\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch : 143, training loss : 0.0004, training accuracy : 99.98, test loss : 2.4422, test accuracy : 57.26\n",
            "\n",
            "Epoch: 144\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0006, accuracy : 99.95\n",
            "iteration : 200, loss : 0.0005, accuracy : 99.95\n",
            "iteration : 250, loss : 0.0005, accuracy : 99.96\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.96\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch : 144, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4328, test accuracy : 57.50\n",
            "\n",
            "Epoch: 145\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 145, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4417, test accuracy : 57.22\n",
            "\n",
            "Epoch: 146\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 146, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4404, test accuracy : 57.22\n",
            "\n",
            "Epoch: 147\n",
            "iteration :  50, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch : 147, training loss : 0.0004, training accuracy : 99.98, test loss : 2.4453, test accuracy : 57.09\n",
            "\n",
            "Epoch: 148\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0006, accuracy : 99.95\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.96\n",
            "iteration : 200, loss : 0.0005, accuracy : 99.96\n",
            "iteration : 250, loss : 0.0005, accuracy : 99.96\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch : 148, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4401, test accuracy : 57.18\n",
            "\n",
            "Epoch: 149\n",
            "iteration :  50, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch : 149, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4395, test accuracy : 57.46\n",
            "\n",
            "Epoch: 150\n",
            "iteration :  50, loss : 0.0003, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.99\n",
            "Epoch : 150, training loss : 0.0004, training accuracy : 99.98, test loss : 2.4451, test accuracy : 57.36\n",
            "\n",
            "Epoch: 151\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch : 151, training loss : 0.0004, training accuracy : 99.98, test loss : 2.4357, test accuracy : 57.21\n",
            "\n",
            "Epoch: 152\n",
            "iteration :  50, loss : 0.0007, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0006, accuracy : 99.95\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.96\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.96\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch : 152, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4499, test accuracy : 57.03\n",
            "\n",
            "Epoch: 153\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 153, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4400, test accuracy : 57.19\n",
            "\n",
            "Epoch: 154\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch : 154, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4382, test accuracy : 57.32\n",
            "\n",
            "Epoch: 155\n",
            "iteration :  50, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch : 155, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4489, test accuracy : 57.17\n",
            "\n",
            "Epoch: 156\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 156, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4385, test accuracy : 57.36\n",
            "\n",
            "Epoch: 157\n",
            "iteration :  50, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch : 157, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4392, test accuracy : 57.43\n",
            "\n",
            "Epoch: 158\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 158, training loss : 0.0004, training accuracy : 99.98, test loss : 2.4428, test accuracy : 57.35\n",
            "\n",
            "Epoch: 159\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch : 159, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4486, test accuracy : 57.31\n",
            "\n",
            "Epoch: 160\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch : 160, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4442, test accuracy : 57.20\n",
            "\n",
            "Epoch: 161\n",
            "iteration :  50, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch : 161, training loss : 0.0004, training accuracy : 99.98, test loss : 2.4410, test accuracy : 57.40\n",
            "\n",
            "Epoch: 162\n",
            "iteration :  50, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0005, accuracy : 99.96\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.96\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch : 162, training loss : 0.0004, training accuracy : 99.98, test loss : 2.4465, test accuracy : 57.23\n",
            "\n",
            "Epoch: 163\n",
            "iteration :  50, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch : 163, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4535, test accuracy : 57.43\n",
            "\n",
            "Epoch: 164\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch : 164, training loss : 0.0004, training accuracy : 99.98, test loss : 2.4395, test accuracy : 57.31\n",
            "\n",
            "Epoch: 165\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 165, training loss : 0.0004, training accuracy : 99.98, test loss : 2.4440, test accuracy : 57.36\n",
            "\n",
            "Epoch: 166\n",
            "iteration :  50, loss : 0.0006, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0005, accuracy : 99.96\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch : 166, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4472, test accuracy : 57.38\n",
            "\n",
            "Epoch: 167\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch : 167, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4435, test accuracy : 57.38\n",
            "\n",
            "Epoch: 168\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch : 168, training loss : 0.0004, training accuracy : 99.98, test loss : 2.4418, test accuracy : 57.37\n",
            "\n",
            "Epoch: 169\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 169, training loss : 0.0004, training accuracy : 99.98, test loss : 2.4472, test accuracy : 57.17\n",
            "\n",
            "Epoch: 170\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 170, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4482, test accuracy : 57.35\n",
            "\n",
            "Epoch: 171\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 171, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4521, test accuracy : 57.24\n",
            "\n",
            "Epoch: 172\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.97\n",
            "Epoch : 172, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4556, test accuracy : 57.12\n",
            "\n",
            "Epoch: 173\n",
            "iteration :  50, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch : 173, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4570, test accuracy : 57.19\n",
            "\n",
            "Epoch: 174\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.97\n",
            "Epoch : 174, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4500, test accuracy : 57.24\n",
            "\n",
            "Epoch: 175\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.97\n",
            "Epoch : 175, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4545, test accuracy : 57.06\n",
            "\n",
            "Epoch: 176\n",
            "iteration :  50, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch : 176, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4447, test accuracy : 57.13\n",
            "\n",
            "Epoch: 177\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 177, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4470, test accuracy : 57.35\n",
            "\n",
            "Epoch: 178\n",
            "iteration :  50, loss : 0.0003, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0005, accuracy : 99.96\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch : 178, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4472, test accuracy : 57.22\n",
            "\n",
            "Epoch: 179\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 179, training loss : 0.0004, training accuracy : 99.97, test loss : 2.4626, test accuracy : 57.04\n",
            "\n",
            "Epoch: 180\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch : 180, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4530, test accuracy : 57.10\n",
            "\n",
            "Epoch: 181\n",
            "iteration :  50, loss : 0.0009, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0007, accuracy : 99.94\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.96\n",
            "iteration : 200, loss : 0.0005, accuracy : 99.96\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch : 181, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4564, test accuracy : 57.00\n",
            "\n",
            "Epoch: 182\n",
            "iteration :  50, loss : 0.0004, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 182, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4566, test accuracy : 57.39\n",
            "\n",
            "Epoch: 183\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 183, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4491, test accuracy : 57.18\n",
            "\n",
            "Epoch: 184\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.96\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 184, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4539, test accuracy : 57.34\n",
            "\n",
            "Epoch: 185\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch : 185, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4589, test accuracy : 57.14\n",
            "\n",
            "Epoch: 186\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.97\n",
            "Epoch : 186, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4547, test accuracy : 57.24\n",
            "\n",
            "Epoch: 187\n",
            "iteration :  50, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 187, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4551, test accuracy : 57.31\n",
            "\n",
            "Epoch: 188\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 188, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4626, test accuracy : 57.12\n",
            "\n",
            "Epoch: 189\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 189, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4598, test accuracy : 57.29\n",
            "\n",
            "Epoch: 190\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 190, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4597, test accuracy : 57.23\n",
            "\n",
            "Epoch: 191\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.96\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.96\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.96\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.96\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.96\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.97\n",
            "Epoch : 191, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4525, test accuracy : 57.16\n",
            "\n",
            "Epoch: 192\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 192, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4523, test accuracy : 57.08\n",
            "\n",
            "Epoch: 193\n",
            "iteration :  50, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.96\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.97\n",
            "Epoch : 193, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4531, test accuracy : 57.18\n",
            "\n",
            "Epoch: 194\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch : 194, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4551, test accuracy : 57.28\n",
            "\n",
            "Epoch: 195\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 195, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4531, test accuracy : 57.26\n",
            "\n",
            "Epoch: 196\n",
            "iteration :  50, loss : 0.0006, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 196, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4607, test accuracy : 57.16\n",
            "\n",
            "Epoch: 197\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.97\n",
            "Epoch : 197, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4527, test accuracy : 57.35\n",
            "\n",
            "Epoch: 198\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.96\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.97\n",
            "Epoch : 198, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4607, test accuracy : 57.22\n",
            "\n",
            "Epoch: 199\n",
            "iteration :  50, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 199, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4508, test accuracy : 57.23\n",
            "\n",
            "Epoch: 200\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 200, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4496, test accuracy : 57.24\n",
            "\n",
            "Epoch: 201\n",
            "iteration :  50, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.97\n",
            "Epoch : 201, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4519, test accuracy : 57.25\n",
            "\n",
            "Epoch: 202\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.97\n",
            "Epoch : 202, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4565, test accuracy : 57.23\n",
            "\n",
            "Epoch: 203\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.97\n",
            "Epoch : 203, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4495, test accuracy : 57.23\n",
            "\n",
            "Epoch: 204\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.97\n",
            "Epoch : 204, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4587, test accuracy : 57.22\n",
            "\n",
            "Epoch: 205\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 205, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4588, test accuracy : 57.16\n",
            "\n",
            "Epoch: 206\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.97\n",
            "Epoch : 206, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4640, test accuracy : 57.12\n",
            "\n",
            "Epoch: 207\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.96\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 207, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4633, test accuracy : 57.21\n",
            "\n",
            "Epoch: 208\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.97\n",
            "Epoch : 208, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4619, test accuracy : 57.24\n",
            "\n",
            "Epoch: 209\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch : 209, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4599, test accuracy : 57.11\n",
            "\n",
            "Epoch: 210\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 210, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4572, test accuracy : 57.40\n",
            "\n",
            "Epoch: 211\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 211, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4694, test accuracy : 57.24\n",
            "\n",
            "Epoch: 212\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch : 212, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4554, test accuracy : 57.25\n",
            "\n",
            "Epoch: 213\n",
            "iteration :  50, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch : 213, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4605, test accuracy : 57.32\n",
            "\n",
            "Epoch: 214\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.97\n",
            "Epoch : 214, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4615, test accuracy : 57.39\n",
            "\n",
            "Epoch: 215\n",
            "iteration :  50, loss : 0.0007, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 215, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4603, test accuracy : 57.17\n",
            "\n",
            "Epoch: 216\n",
            "iteration :  50, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.97\n",
            "Epoch : 216, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4641, test accuracy : 57.26\n",
            "\n",
            "Epoch: 217\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.97\n",
            "Epoch : 217, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4595, test accuracy : 57.09\n",
            "\n",
            "Epoch: 218\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 218, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4550, test accuracy : 57.16\n",
            "\n",
            "Epoch: 219\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.97\n",
            "Epoch : 219, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4575, test accuracy : 57.31\n",
            "\n",
            "Epoch: 220\n",
            "iteration :  50, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.97\n",
            "Epoch : 220, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4579, test accuracy : 57.11\n",
            "\n",
            "Epoch: 221\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.97\n",
            "Epoch : 221, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4548, test accuracy : 57.25\n",
            "\n",
            "Epoch: 222\n",
            "iteration :  50, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.96\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.97\n",
            "Epoch : 222, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4650, test accuracy : 57.13\n",
            "\n",
            "Epoch: 223\n",
            "iteration :  50, loss : 0.0004, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 223, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4616, test accuracy : 57.20\n",
            "\n",
            "Epoch: 224\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.97\n",
            "Epoch : 224, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4539, test accuracy : 57.26\n",
            "\n",
            "Epoch: 225\n",
            "iteration :  50, loss : 0.0003, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 225, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4640, test accuracy : 57.33\n",
            "\n",
            "Epoch: 226\n",
            "iteration :  50, loss : 0.0004, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 226, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4629, test accuracy : 57.33\n",
            "\n",
            "Epoch: 227\n",
            "iteration :  50, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 227, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4625, test accuracy : 57.24\n",
            "\n",
            "Epoch: 228\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 228, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4663, test accuracy : 57.41\n",
            "\n",
            "Epoch: 229\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0005, accuracy : 99.96\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 229, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4591, test accuracy : 57.15\n",
            "\n",
            "Epoch: 230\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 230, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4629, test accuracy : 57.29\n",
            "\n",
            "Epoch: 231\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 231, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4555, test accuracy : 57.27\n",
            "\n",
            "Epoch: 232\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.97\n",
            "Epoch : 232, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4590, test accuracy : 57.18\n",
            "\n",
            "Epoch: 233\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.97\n",
            "Epoch : 233, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4518, test accuracy : 57.27\n",
            "\n",
            "Epoch: 234\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.97\n",
            "Epoch : 234, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4543, test accuracy : 57.26\n",
            "\n",
            "Epoch: 235\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0006, accuracy : 99.95\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 235, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4576, test accuracy : 57.26\n",
            "\n",
            "Epoch: 236\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.97\n",
            "Epoch : 236, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4574, test accuracy : 57.28\n",
            "\n",
            "Epoch: 237\n",
            "iteration :  50, loss : 0.0003, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.97\n",
            "Epoch : 237, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4610, test accuracy : 57.39\n",
            "\n",
            "Epoch: 238\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 238, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4586, test accuracy : 57.23\n",
            "\n",
            "Epoch: 239\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 239, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4676, test accuracy : 57.18\n",
            "\n",
            "Epoch: 240\n",
            "iteration :  50, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 240, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4614, test accuracy : 57.53\n",
            "\n",
            "Epoch: 241\n",
            "iteration :  50, loss : 0.0006, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 241, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4595, test accuracy : 57.15\n",
            "\n",
            "Epoch: 242\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 242, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4636, test accuracy : 57.27\n",
            "\n",
            "Epoch: 243\n",
            "iteration :  50, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 243, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4726, test accuracy : 57.04\n",
            "\n",
            "Epoch: 244\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 244, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4653, test accuracy : 57.10\n",
            "\n",
            "Epoch: 245\n",
            "iteration :  50, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 245, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4625, test accuracy : 57.15\n",
            "\n",
            "Epoch: 246\n",
            "iteration :  50, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.97\n",
            "Epoch : 246, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4626, test accuracy : 57.14\n",
            "\n",
            "Epoch: 247\n",
            "iteration :  50, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0003, accuracy : 100.00\n",
            "iteration : 150, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 200, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 247, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4605, test accuracy : 57.24\n",
            "\n",
            "Epoch: 248\n",
            "iteration :  50, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 248, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4557, test accuracy : 57.45\n",
            "\n",
            "Epoch: 249\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.99\n",
            "Epoch : 249, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4631, test accuracy : 57.25\n",
            "\n",
            "Epoch: 250\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.99\n",
            "Epoch : 250, training loss : 0.0003, training accuracy : 99.99, test loss : 2.4674, test accuracy : 57.32\n",
            "\n",
            "Epoch: 251\n",
            "iteration :  50, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 251, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4607, test accuracy : 57.38\n",
            "\n",
            "Epoch: 252\n",
            "iteration :  50, loss : 0.0004, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 252, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4615, test accuracy : 57.14\n",
            "\n",
            "Epoch: 253\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 253, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4647, test accuracy : 57.31\n",
            "\n",
            "Epoch: 254\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 254, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4636, test accuracy : 57.26\n",
            "\n",
            "Epoch: 255\n",
            "iteration :  50, loss : 0.0003, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 255, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4715, test accuracy : 57.19\n",
            "\n",
            "Epoch: 256\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.97\n",
            "Epoch : 256, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4726, test accuracy : 57.11\n",
            "\n",
            "Epoch: 257\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 257, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4660, test accuracy : 57.20\n",
            "\n",
            "Epoch: 258\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.96\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 258, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4661, test accuracy : 57.28\n",
            "\n",
            "Epoch: 259\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 259, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4670, test accuracy : 57.23\n",
            "\n",
            "Epoch: 260\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 260, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4612, test accuracy : 57.37\n",
            "\n",
            "Epoch: 261\n",
            "iteration :  50, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.97\n",
            "Epoch : 261, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4701, test accuracy : 57.13\n",
            "\n",
            "Epoch: 262\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 262, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4709, test accuracy : 57.49\n",
            "\n",
            "Epoch: 263\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 263, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4705, test accuracy : 57.07\n",
            "\n",
            "Epoch: 264\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.96\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 264, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4693, test accuracy : 57.15\n",
            "\n",
            "Epoch: 265\n",
            "iteration :  50, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.99\n",
            "Epoch : 265, training loss : 0.0003, training accuracy : 99.99, test loss : 2.4658, test accuracy : 57.03\n",
            "\n",
            "Epoch: 266\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.96\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 266, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4571, test accuracy : 57.24\n",
            "\n",
            "Epoch: 267\n",
            "iteration :  50, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 267, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4499, test accuracy : 57.36\n",
            "\n",
            "Epoch: 268\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 268, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4592, test accuracy : 57.35\n",
            "\n",
            "Epoch: 269\n",
            "iteration :  50, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 269, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4579, test accuracy : 57.13\n",
            "\n",
            "Epoch: 270\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 270, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4634, test accuracy : 57.20\n",
            "\n",
            "Epoch: 271\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.96\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.95\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.96\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.97\n",
            "Epoch : 271, training loss : 0.0003, training accuracy : 99.97, test loss : 2.4624, test accuracy : 57.10\n",
            "\n",
            "Epoch: 272\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 272, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4660, test accuracy : 57.25\n",
            "\n",
            "Epoch: 273\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 273, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4610, test accuracy : 57.27\n",
            "\n",
            "Epoch: 274\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 274, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4691, test accuracy : 57.34\n",
            "\n",
            "Epoch: 275\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 275, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4768, test accuracy : 57.05\n",
            "\n",
            "Epoch: 276\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.99\n",
            "Epoch : 276, training loss : 0.0003, training accuracy : 99.99, test loss : 2.4618, test accuracy : 57.24\n",
            "\n",
            "Epoch: 277\n",
            "iteration :  50, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 277, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4651, test accuracy : 57.26\n",
            "\n",
            "Epoch: 278\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 278, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4644, test accuracy : 57.26\n",
            "\n",
            "Epoch: 279\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 279, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4702, test accuracy : 57.23\n",
            "\n",
            "Epoch: 280\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0003, accuracy : 100.00\n",
            "iteration : 250, loss : 0.0003, accuracy : 100.00\n",
            "iteration : 300, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.99\n",
            "Epoch : 280, training loss : 0.0003, training accuracy : 99.99, test loss : 2.4535, test accuracy : 57.43\n",
            "\n",
            "Epoch: 281\n",
            "iteration :  50, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.99\n",
            "Epoch : 281, training loss : 0.0003, training accuracy : 99.99, test loss : 2.4681, test accuracy : 57.33\n",
            "\n",
            "Epoch: 282\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 282, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4662, test accuracy : 57.28\n",
            "\n",
            "Epoch: 283\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 283, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4671, test accuracy : 57.19\n",
            "\n",
            "Epoch: 284\n",
            "iteration :  50, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 284, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4625, test accuracy : 57.05\n",
            "\n",
            "Epoch: 285\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0002, accuracy : 99.99\n",
            "Epoch : 285, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4581, test accuracy : 57.33\n",
            "\n",
            "Epoch: 286\n",
            "iteration :  50, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.97\n",
            "Epoch : 286, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4595, test accuracy : 57.25\n",
            "\n",
            "Epoch: 287\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 287, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4603, test accuracy : 57.20\n",
            "\n",
            "Epoch: 288\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 288, training loss : 0.0003, training accuracy : 99.99, test loss : 2.4630, test accuracy : 57.06\n",
            "\n",
            "Epoch: 289\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.99\n",
            "Epoch : 289, training loss : 0.0003, training accuracy : 99.99, test loss : 2.4717, test accuracy : 57.17\n",
            "\n",
            "Epoch: 290\n",
            "iteration :  50, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.99\n",
            "Epoch : 290, training loss : 0.0003, training accuracy : 99.99, test loss : 2.4665, test accuracy : 57.22\n",
            "\n",
            "Epoch: 291\n",
            "iteration :  50, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 291, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4671, test accuracy : 57.24\n",
            "\n",
            "Epoch: 292\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 292, training loss : 0.0003, training accuracy : 99.99, test loss : 2.4635, test accuracy : 57.23\n",
            "\n",
            "Epoch: 293\n",
            "iteration :  50, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 293, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4631, test accuracy : 57.22\n",
            "\n",
            "Epoch: 294\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.97\n",
            "Epoch : 294, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4690, test accuracy : 57.12\n",
            "\n",
            "Epoch: 295\n",
            "iteration :  50, loss : 0.0007, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0006, accuracy : 99.95\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.96\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.96\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 295, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4621, test accuracy : 57.11\n",
            "\n",
            "Epoch: 296\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 296, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4694, test accuracy : 57.23\n",
            "\n",
            "Epoch: 297\n",
            "iteration :  50, loss : 0.0003, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 297, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4669, test accuracy : 57.40\n",
            "\n",
            "Epoch: 298\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0005, accuracy : 99.95\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.96\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 298, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4677, test accuracy : 57.25\n",
            "\n",
            "Epoch: 299\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 299, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4684, test accuracy : 57.24\n",
            "\n",
            "Epoch: 300\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 300, training loss : 0.0003, training accuracy : 99.98, test loss : 2.4624, test accuracy : 57.28\n"
          ]
        }
      ],
      "source": [
        "#------------------------------Main---------------------------------------------\n",
        "config = {\n",
        "    'lr': 0.01,\n",
        "    'momentum': 0.9,\n",
        "}\n",
        "train_loss_ = []\n",
        "train_acc_ = []\n",
        "test_loss_ = []\n",
        "test_acc_ = []\n",
        "#ResNet 34\n",
        "net = ResNet34().to('cuda')\n",
        "criterion = nn.CrossEntropyLoss().to('cuda')\n",
        "optimizer = optim.SGD(net.parameters(), lr=config['lr'],\n",
        "                      momentum=config['momentum'])\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\n",
        "\n",
        "for epoch in range(1, 301):\n",
        "    train_loss, train_acc = train(epoch, net, criterion, trainloader, scheduler)\n",
        "    test_loss, test_acc = test(epoch, net, criterion, testloader)\n",
        "    \n",
        "    print((\"Epoch : %3d, training loss : %0.4f, training accuracy : %2.2f, test loss \" + \\\n",
        "      \": %0.4f, test accuracy : %2.2f\") % (epoch, train_loss, train_acc, test_loss, test_acc))\n",
        "    \n",
        "    train_loss_.append(train_loss)\n",
        "    test_loss_.append(test_loss)\n",
        "    train_acc_.append(train_acc)\n",
        "    test_acc_.append(test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "81l23SSrYG7r",
        "outputId": "a99d4b6a-d346-494f-f460-dc31f74af4bc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(range(len(train_loss_)), train_loss_, 'b')\n",
        "plt.plot(range(len(test_loss_)), test_loss_, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"ResNet34: Loss vs Number of epochs\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "PgRzBZnLb9oq",
        "outputId": "cd4a36ef-c06b-41eb-f53c-adf498ff8374"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZhcdZX/8fdJp5POBlkJWUnYNyVACCBRWRQIyuaCbIoOj7j9HHWUAWZGBn2c3w9lBkFnhEFBUATZhKCCsgUQBEIIYQkJSYAsnZ2s3emk00mf3x/nW51Kp7pTnaSrunM/r+epp6rueu5S99zv9976XnN3REREALqUOwAREek4lBRERKSJkoKIiDRRUhARkSZKCiIi0kRJQUREmigpiAgAZjbKzNzMupZp/ieY2WwzqzWzc8oRQ7N4yro+ykVJoUTMbK6ZrU87/BIzu93Meu/kNL+Ydtp/bta92sxOLGL8bXZ6MzvJzN4ws9VmtsLMHjSzYQXG7W9my83suR2I+8Q03yvaOm6W7Oz27YR+CPy3u/d294fKHUxWKSmU1pnu3hsYAxwJXLULprkS+Gcz67MLpgXwFnCau/cFhgKzgZsKDPdjYMYOzuMSIu4v7OD4O8RCZ9vnd/X2LYkdPLveB5i+q2ORtulsP5DdgrsvAf5KJAcAzOw4M/t7OkN/Lf9MMJ0xvmtmNWb2npldlDe5GcALwD8VmpeZdTGzK83snXTmf6+Z9U+9n03vq1MJ5nh3X+rui/ImsRnYv9k0PwQcDvy6rctuZr2AzwDfAA4ws7HN+n/ZzGakZX3LzI5K3UeY2R9S6WSFmf136n6Nmd2ZN/5WpR8ze9rM/sPMngfqgH3N7Et583jXzL7SLIazzWyama1N6+10M/usmb3SbLh/MrOJBZbxc2Y2pVm375jZw+nzGWnZasxsoZl9r5VVtr3te7uZ/Sjv+4lmVp33fa6ZXW5mr5vZOjO71cwGm9mjaf5PmFm/ZpP9BzNbZGaL82NrbV/KW++Xmtl84KkW4v2ymc0xs5Vm9rCZDU3d3wH2Bf6Y9sXuBcYdamYPpH3gPTP7x7x+15jZ/WZ2T1quqWZ2RF7/Q9K+sNrMppvZWXn9epjZf5nZPDNbY2bPmVmPvFlfZGbzzex9M/vXvPHGmdmUtJ8sNbPrCy1zp+PuepXgBcwFPpY+DwfeAG5M34cBK4AziET98fR9ENALWAsclIYdAhyWPn8ReI5ILquA/ql7NXBi+vwt4MU0z+7A/wJ3p36jAAe6Not1JLAaaAQagC/m9asApgJH5+bfbNzVwPhW1sPngcVpOn8Efp7X77PAQuAYwIhktE8a9jXgp2l9VOXmAVwD3Jk3ja2WCXgamA8cBnQFKoFPAPuleXyUSBZHpeHHAWvSNuiSts3Bad2tBA7Jm9erwKcLLGNPoAY4IK/by8D56fNi4MPpc7/cvAtMp5jtezvwo7xxTgSqm+13LwKD07IsS9vvyLQenwL+vdm6uzut5w8Ay9my3xazL/0mjdujwPKcDLwPHJXG/znwbKHfSIFxuwCvAFcD3YgE8i5Rqs3tBw3ECUcl8D3gvfS5EpgD/Esa9+S0fXK/qf9J+8kwYl/7UIovt0y/BHoARwD1uX2ASNafT597A8eV+zizS45V5Q4gK6+0w9emndGBJ4G+qd8VwG+bDf9XopqlF3Gg/XTzHxp5B2XgXuDH6XP+QWMGcEreOEPSj6crLSSFvGH7p9iOy+v2HeCm5vNvw3p4Arghfb4gHXQq85b5WwXGOT4Nt02cFJcUfridmB7KzZc40P20heFuAv4jfT6MOFB3b2HYO4Gr0+cD0nbvmb7PB74C7LGduIrZvrez/aRwUd73B3LbL33/JvBQs3V3cF7/nwC3tmFf2reV5bkV+Ene995p/FF5sbaUFI4F5jfrdhXw67z94MW8fl1IyTe9lgBd8vrfncbpAqwHjigwz9wyDc/rNpktyf1Z4AfAwLb8Bjr6S9VHpXWOu/chfrgHAwNT932Az6ai7WozWw2MB4a4+zrgc8BXgcVm9mczO7jAtK8GvmZmg5t13wd4MG+6M4gqoebDbcPdVwJ3ABPNrGsq6v8j8K+tj1mYmY0ATgJ+lzpNJM5WP5G+jwDeKTDqCGCeu2/akfkCC5rFMcHMXkxVGKuJElpuW7QUA8S6uNDMjCjx3Ovu9S0MexeR9AAuJA68den7p9M855nZM2Z2fBHL0NL2LcbSvM/rC3xvfsND/vqaR1xbguL2pa3WdTND0/QAcPdaokS8zY0MBewDDG32G/mXlubt7o1E8hyaXgtSt/zlGkZs9ypa3uYQCSWnji3r61LgQGCmmb1sZp8sYjk6PCWFMnD3Z4gzvP9MnRYQJYW+ea9e7n5tGv6v7v5x4sxsJlGcbT7NmcAf2PaAvQCY0GzaVe6+kDgL2p6uwF7AHkTVyhDgLTNbAtwIjLO4m6qiiGl9ntjn/pjGf5f4QV6SF+t+BcZbAIy0whcv1xHVNTl7FximaTlTXfUDxLof7HFB/RGiKqm1GHD3F4GNxJnnhcBvCw2XPA4MMrMxRHK4K286L7v72cR6fYgoBbSqle1bzPK31Yi8zyOB3DWm1valplBbme4i4uAONF1fGkBUGW7PAuC9ZvPu4+5nFIrb4oaC4Wmei4ARtvVNBiPTfN8HNtDCNm+Nu8929wuI7fhj4P60TJ2akkL53AB8PF0MuxM408xOM7MKM6tKFwyHp4uCZ6edrZ6ogmpsYZo/AL4E9M3rdjPwH2a2D4CZDTKzs1O/5Wla++YGNrNPmdlB6aLiIOB64NVUaniUKFKPSa+riXr1Me6+uYhlviTFOCbv9WngDDMbAPwK+J6ZHW1h/xT3ZKIq4Foz65XWzwlpmtOAj5jZSDPbk+3f0dWNqC9eDmwyswnAqXn9bwW+ZGanpHUwrFnJ7DfAfwMN7t7i7bju3gDcB1xHVMM9DmBm3czsIjPbMw2zlpa3Z3OFtu80Yv31N7O9gW8XOa3WfN/MeprZYWl+96Ture1LxbibWLdjUnL+v8BL7j63iHEnAzVmdkW6MFxhZoeb2TF5wxyd9t+uxHqoJ66BvESc4f+zmVVa3MRxJvD7VHq4Dbg+XciuMLPjrcCF7ubM7GIzG5SmsTp1LnZbdlhKCmXi7suJA8zV7r4AOJsoDi8nzoouJ7ZPF+LOk0XEhc6PAl9rYZrvEWev+WcrNwIPA4+ZWQ3xIzk2DV8H/AfwfCqSH0cUqf9C1IG/Qezk56bh6919Se5FXJBtSJ8BsLhz5MPNY0vT3gf4n/xpuPvDxEXAC9z9vhTPXWn+DxEXVzcTP+L9ifr4aqJKDXd/nDhovU5ciPzTdtZ7DVEFdi9xTeDCtH5y/ScTB8KfpuV7hryz27R+DycS+fbcBXwMuK9Z1dfngblmtpaoFryo0MgFYi+0fX9LXISfCzzGlgP4zniG2CZPAv/p7o+l7i3uS8Vw9yeA7xMltcXE2fn5RY67GfgkcSLxHnGG/ytgz7zBJhL7xSpiHX/K3RvcfSOx/0xI4/0C+EIqfUFclH6DuBlgJXHWX8yx8XRgupnVEuvmfHdfX8zydGSWLpiISBHSrYrLiDuGZpc7Hglmdg2wv7tfXO5YOjuVFETa5mvAy0oIsrvKVJseIjvDzOYSF6TL3i6PSHtR9ZGIiDRR9ZGIiDTp1NVHAwcO9FGjRpU7DBGRTuWVV155390HFerXqZPCqFGjmDJlyvYHFBGRJmY2r6V+qj4SEZEmSgoiItJESUFERJooKYiISBMlBRERadJuScHMbjOzZWb2Zl63/mb2uJnNTu/9Unczs59ZPKbvdUuPYBQRkdJqz5LC7UQrgvmuBJ509wOIFhivTN0nEE+nOgC4jMIPihcRkXbWbv9TcPdnzWxUs85nE08dg3iK1dPE4x7PBn7j0ebGi2bW18yGuPvi9oqvrerqYOpUWLIEVq6ETZvAHRob4z3/1bxbY6dvYb19qIWVbWmdbEvrpLAJE+CodqhTKfWf1wbnHeiXsOVResPY+jF+1anbNknBzC4jShOMHDmy/SLN8+yzcMYZsG5dSWYnIrJdAwbsHkmhibu7mbX5HMDdbwFuARg7dmy7n0OsWgUXXghDh8JPfwojR0K/flBZCWbQpUu8516tfZdtab1sS+ukMK2XrXVpp8r/UieFpblqITMbQjysBOJZqfnPhR1Occ9tbXd33gkLF8ILL8Bxx5U7GhGR9lXqW1IfZstD2i8hHp+X6/6FdBfSccCajnI94e674YMfVEIQkWxoz1tS7wZeAA4ys2ozuxS4lnhY/Wzi2bXXpsEfAd4lngv7S+Dr7RVXW8ybFyWECy4odyQiIqXRnncftXQoPaXAsA58o71i2VHPPx/vn/hEeeMQESkV/aO5FbNmxcWtAw8sdyQiIqWhpNCKWbNg1Cjo3r3ckYiIlIaSQitmzVIpQUSyRUmhBe5KCiKSPUoKLVi6FGpqlBREJFuUFFowa1a8KymISJYoKbRg/vx4HzWqrGGIiJSUkkILFi2K9yFDyhuHiEgpKSm0YPFi6NUL+vQpdyQiIqWjpNCCxYujZVQRkSxRUmjBokWqOhKR7FFSaMHixUoKIpI9SgoFuKv6SESySUmhgJqaePSmSgoikjVKCgUsTo/3UVIQkaxRUihASUFEskpJoYClS+N9773LG4eISKkpKRSwcmW8DxhQ3jhEREpNSaGAXFLo16+8cYiIlJqSQgErV0YTF3rimohkjZJCAStXQv/+5Y5CRKT0lBQKUFIQkaxSUihASUFEskpJoQAlBRHJKiWFApQURCSrlBSacVdSEJHsUlJopq4ONm5UUhCRbFJSaCb3xzUlBRHJIiWFZpQURCTLlBSaUbtHIpJlSgrNqN0jEckyJYVm1qyJ9z33LG8cIiLloKTQzNq18b7HHuWNQ0SkHJQUmlFSEJEsU1JoZu3aaDa7oqLckYiIlJ6SQjNr16qUICLZVZakYGbfMbPpZvammd1tZlVmNtrMXjKzOWZ2j5l1K0dsSgoikmUlTwpmNgz4R2Csux8OVADnAz8Gfuru+wOrgEtLHRvE3UdKCiKSVeWqPuoK9DCzrkBPYDFwMnB/6n8HcE45AlNJQUSyrORJwd0XAv8JzCeSwRrgFWC1u29Kg1UDw0odGygpiEi2laP6qB9wNjAaGAr0Ak5vw/iXmdkUM5uyfPnyXR6fkoKIZFk5qo8+Brzn7svdvQH4A3AC0DdVJwEMBxYWGtndb3H3se4+dtCgQbs8uLVr9W9mEcmuciSF+cBxZtbTzAw4BXgLmAR8Jg1zCTCx1IG5q6QgItlWjmsKLxEXlKcCb6QYbgGuAP7JzOYAA4BbSx1bXR00NiopiEh2dd3+ILueu/878O/NOr8LjCtDOE3UxIWIZJ3+0Zwn10KqkoKIZJWSQh6VFEQk65QU8igpiEjWKSnkUVIQkaxTUsiTSwr6n4KIZJWSQh6VFEQk65QU8uSSQp8+5Y1DRKRclBTyrF0LPXpAZWW5IxERKQ8lhTxq4kJEsk5JIY8esCMiWaekkEclBRHJOiWFPGo2W0SyTkkhj0oKIpJ1Sgp5lBREJOuUFPIoKYhI1ikpJHrqmoiIkkKTDRtg0yYlBRHJNiWFRA/YERFRUmiixvBERJQUmqjZbBERJYUmKimIiCgpNFFSEBFRUmiipCAioqTQRElBRERJoYmeuiYioqTQZM0a6N49XiIiWaWkkKiJCxERJYUmepaCiEgRScHMzjSz3T55qKQgIlJcSeFzwGwz+4mZHdzeAZWLkoKISBFJwd0vBo4E3gFuN7MXzOwyM9ut7tNRUhARKfKagruvBe4Hfg8MAc4FpprZN9sxtpJSUhARKe6awllm9iDwNFAJjHP3CcARwHfbN7zSUVIQEYGuRQzzaeCn7v5sfkd3rzOzS9snrNJyj/8pKCmISNYVkxSuARbnvphZD2Cwu8919yfbK7BSqq+HhgYlBRGRYq4p3Ac05n3fnLrtNvQsBRGRUExS6OruG3Nf0udu7RdS6akxPBGRUExSWG5mZ+W+mNnZwPs7M1Mz62tm95vZTDObYWbHm1l/M3vczGan9347M4+2UFIQEQnFJIWvAv9iZvPNbAFwBfCVnZzvjcBf3P1g4i6mGcCVwJPufgDwZPpeEkoKIiJhuxea3f0d4Dgz652+1+7MDM1sT+AjwBfT9DYCG1MJ5MQ02B3ELbBX7My8iqWkICISirn7CDP7BHAYUGVmALj7D3dwnqOB5cCvzewI4BXgW8QdTbm7nJYAg1uI5TLgMoCRI0fuYAhbU1IQEQnF/HntZqL9o28CBnwW2Gcn5tkVOAq4yd2PBNbRrKrI3R3wQiO7+y3uPtbdxw4aNGgnwthCD9gREQnFXFP4kLt/AVjl7j8AjgcO3Il5VgPV7v5S+n4/kSSWmtkQgPS+bCfm0Sa1qUJMSUFEsq6YpLAhvdeZ2VCggWj/aIe4+xJggZkdlDqdArwFPAxckrpdAkzc0Xm0VW0tmEGPHqWao4hIx1TMNYU/mllf4DpgKlGt88udnO83gd+ZWTfgXeBLRIK6NzWdMQ84byfnUbSaGujdOxJDu2hogIoK6NLOj6VYswY2bIDBBS/HbM29bQu8bh307Nm2cRoa4v2112Do0HjV1cUzTysqtp1+Y2MU1zZvjm4VFbB+fbz694+YN22CysriY2hPbV2H0j5WrIj9Y1dvi8b0n932/t12MK0mhfRwnSfdfTXwgJn9Cahy9zU7M1N3nwaMLdDrlJ2Z7o6qrW1D1dEbb8Tr2mvhu9+NHeePf4Sjj4azzoLf/x6eegqWLIFBg2DvveGJJ+L9yivjQPfqq/DeezBwIBx0UBzoli6FZcvgsMNgypTYwcePh5kzox2Ovn1hr73g0Udj3AMOgP32i/FGjYLHH4/uFRVw+ukx/r77Rv8lS2KevXvDySfHwfqhh2DECKiqggUL4DOfib90V1fDCy/EMN26wcEHx0WXl16Cfv1gzBh4//2Y9h57xLrYZ5+IpWfPOGC/8EKMM21aHOwBhg2DY4+NdTV8eMy7sTGm9/e/R+Iwi2VZuDA2yAc/CJMmRZI45JDoP2sWHH98rKe5c2P648fHAXr9eti4MWJfuBBOOglWr451ctddsY0aGmKdP/54xFRTE9tt8+ZYjhkzYP/94eab4Z134vNRR8Ezz0TMZ5wB8+fD8uVw770R7wUXwIEHxjR+9CNYtQpOPRXOPTfW6YMPxjZevz72jYaGiP+ww2DRophenz7w9tsR2+rVEVeXLrEPjR0L06fH+hszJsZZuzaW79VX4U9/go9+FM48c8u+dNVV8KEPxbTHj4/1NGsWjBsX73vsEdt2v/3g3Xfh5Zdj3FGjYPLkiHuffWK95r8qKmI/GDEi4pw3L2J87rlY5+PGxbqZOTOW6aSTYpyjj47tvHw5HHkkLF4c6/a99yKe6urYF3v0iPVw7rkx7V69YOTI2NYNDbHP1NTEMOPHR8w//zlcdFEsy6mnRnwLFsS+MHky3Hln7D/9+sX8TjghflONjTHOQQfFNKdNi204e3b8Xp95Jn4z110X6+u99+C++2LaBx0U84KYztSpsd579oRPfCJ+w/X10X/uXHj++Vjeww+PddS7N9x/f/yOx46N9T59ekwjt8/26BHDV1TAI4/A00/DEUfEfrtyZex348fvqsNgE4truq0MYPZquiDc4YwdO9anTJmy09M5//z4bb399nYG/NWv4Mtfjs89e8ZZL8QOtGRJfO7SJXa6kSPjBzBvXuwEc+bEwRJiBxs9OsZZujS69e8PAwbEDllVFd02bIiz66qqGK6uLg4MuYPlokXxg1yyBCZMgGOOiR328cfj4DxvXuxAe+4ZMaxcGTtWTU0sdE1NHKh69oQ//CHGGTQoDrp9+sT8ZsyIH+b48bE806ZFMpszJ+I78MA4qCxbtmV97LdfHDQOPDDWTf/+8MMfxo/6S1+CiRPjx9y1a0zz2GPjANbQEDEPHQqvvBI/wgsuiIT43HNxIBw7Nj7Pnh2JySx+kJWV8SOqrIxuffrE+GZxMBs3Lg6Q3bvHPI48MuI1iwNYc4MHR3KdPj12jsMP35Jgu3aNH/Xxx0dSrK7eMt4++8SB+6mnYv1CJNfc2eaECbE9p0+PdVtVFfvKihWxbAsWxH7Qp08cbN58M5Zj331jPRey116x/vP17RsHl/79Y7tD7Adr1kQ8DQ2xXnK6d4/lWrcuxqmv35LQi9GtWyz7nDkx/sEHR1LN/S5yKiu3lCBzBgyIdVBbG/vD4MHxY+zbN+LI7aM9esR6zM3r5ZdjWiecEAfdlpx0UuwvXbvCkCHw4oux3isrt9xlkovNLOa/ZEnMv6Ym9vOcwYMjwUyevGV/z62/QYMi/lwy6N49tmGPHrEvzZ8f2zN3EbNfv9hG2zkGN8V27LGxv61dG+Nefz1ccsn2xy3AzF5x90In5kVVHz1pZp8G/uDbyyCd1HZLCosWwdlnxwY56SS4/PI4iN12WxxsPvSh+MH++c9xJn744dtOwz369+8fB5NcUXf9+jgT6JZaDpk2bcvOv2EDfOAD0b2uLna2fv22TG/z5hhu8+a2VakUqvbYFVUh7nF21q3bttO64ILo3rdvnHmZxY+0sXHbqqS2amzctojvHtttwIA4aO+335YEsWJFdM99f+21OLDmDr7PPAMnnhgJDbYkMPfYzoMGbbl/efPm2DbTpsX0xo6NA87GjXEGv3FjJILcsubvaLkz3+7dW162zZvjwDFgwJYz8BEjot/f/hbLdfzxcRCdNCn2nWXLIvl26xZxzpwZB7O+feH11+OMtaIiDpRz5sQB+YgjIsbq6li+rl0jdrOtX5s3x/pbsCBOFnJn+3vssSWunPr6LaWeWbPiZKZ79zhzHjEiprHvvrFszZd5wYI48Dc2xvLsvfe2+1R9fRwgBw2Kbd29e5xw5BJtZWWciffvv/V4uZJonz6RMGfMiCR/6KFbEsPChTG9FSsi9rVrY3nPPDPWXW4dNDbGdjzmmNgH58yJhH/aaRFH7pCZi72xMfaVd96JY8rGjfH93Xdj/QwfHtvJPeb5xhsxzLHHlqxxtmJKCjVAL2ATcdHZiLtGy35X/64qKZx4Yrw//XQLAzz4IHzqU1HFcuONcSYrItJJ7VRJwd13+xs1a2qiVNmiBQvi/Re/iLMSEZHd1HaTgpl9pFD35g/d6cy2W31UXR1FyYEDSxaTiEg5FHNN4fK8z1XAOKJpipPbJaIyyN2S2qIFC6KuT7cfishurpjqozPzv5vZCOCGdouoDGprt5MUqqu3vYgmIrIb2pF/ZVQDh+zqQMrFvYjqo1xJQURkN1fMNYWfs6Vxui7AGOKfzbuFurpIDC2WFBob4/Y0lRREJAOKuaaQf8/nJuBud2/lnyKdS6uN4S1ZEvfXb9qkkoKIZEIxSeF+YIO7bwYwswoz6+nuddsZr1PI/em0YEnhqae2/HlBSUFEMqCYawpPAvnth/YAnmifcEovV1IomBTmz4/3j30s/jUqIrKbK6akUJX/CE53rzWznu0YU0m1Wn00f340K/H44yWNSUSkXIopKawzs6NyX8zsaGB9+4VUWq1WH82bF+2viIhkRDElhW8D95nZIqLdo72Jx3PuFlqtPpo3Lxr8EhHJiGL+vPaymR0M5J6U9ra7N7Q2TmeSKylsU33kHknhlLI84kFEpCy2W31kZt8Aern7m+7+JtDbzL7e/qGVRoslhVWroqeqj0QkQ4q5pvDl9OQ1ANx9FfDl9guptApeaG5oiKc5gZKCiGRKMUmhwmxLS3BmVgF0a7+QSqumJp6r0S1/iSZOhGuuic+HHlqOsEREyqKYC81/Ae4xs/9N378CPNp+IZVWwcbwXnstnq701lvxSEkRkYwoJilcAVwGfDV9f524A2m3UFNT4CLz9Olx15ESgohkzHarj9y9EXgJmEs8S+FkYEb7hlU6BUsK06er2khEMqnFkoKZHQhckF7vA/cAuPtJpQmtNLZJCvX18fDt884rW0wiIuXSWvXRTOBvwCfdfQ6AmX2nJFGVUE0N9OntsK4OevWCt9+O5rIPO6zcoYmIlFxr1UefAhYDk8zsl2Z2CvGP5t1KbS18vOYBGDQIFi+Gl16KHh/4QHkDExEpgxaTgrs/5O7nAwcDk4jmLvYys5vM7NRSBdjeamrgQyv+COvXw7PPwu9+FxeYdU1BRDKomAvN69z9rvSs5uHAq8QdSbuF2hrn0GVPx5ff/x6eeQY+/3mw3a5QJCKyXW16RrO7r3L3W9x9t2kQaEDNXAbUpucmPPRQ/D/h4ovLG5SISJm0KSnsbjZtgqPr05NFP/KReL/0Uhg1qmwxiYiUU6aTwrp1MJRF8eW66+BTn4Jrry1vUCIiZVTMP5p3WzU10JfVNHbpSpdjjoEHHih3SCIiZZXpkkJtbSSFjb366sKyiAhKCvRjFZv69Ct3KCIiHUKmk0JT9dEefcsdiohIh5DppJArKfieSgoiIpDxpJArKVg/VR+JiEAZk4KZVZjZq2b2p/R9tJm9ZGZzzOweM2v3p7vlLjRXDFBJQUQEyltS+BZbP5fhx8BP3X1/YBVwaXsHsK7W6ccqKgaqpCAiAmVKCmY2HPgE8Kv03YiH99yfBrkDOKe942io2UB3NtJ1oEoKIiJQvpLCDcA/A43p+wBgtbtvSt+rgWGFRjSzy8xsiplNWb58+U4F4StXAaj6SEQkKXlSMLNPAsvc/ZUdGT81yDfW3ccOGjRo52JZszre+6v6SEQEytPMxQnAWWZ2BlAF7AHcCPQ1s66ptDAcWNjegXRZG0mBviopiIhAGUoK7n6Vuw9391HA+cBT7n4R8SCfz6TBLgEmtncsFWuj+gjdkioiAnSs/ylcAfyTmc0hrjHc2t4z7FqrkoKISL6ytpLq7k8DT6fP7wLjSjn/butSSUFJQUQE6FglhZKrWrciPvTvX95AREQ6iEwnhV7r32dt137QNdOPlRARaZLppNB7w/vUdB9Y7jBERDqMTCeFPhvfp7ZKSUFEJCfTSaFvw/vU9VBSEBHJyXZS2Pw+63spKYiI5GQ6KfRvfJ/63gPKHYaISIeR2aTg6+rowQY27qGSgohITvL5xGMAAA8tSURBVGaTQsPi9+N9TyUFEZGczCaF+oWRFDb3V1IQEcnJbFLYuCiSgispiIg0yWxS2LQkJYUBSgoiIjmZTQqbl0W7R10G6e4jEZGczCaFxjU1AFT271PmSEREOo7MJoXNNevYRAXd+3QrdygiIh1GZpOC16xjHb3o2cvKHYqISIeR3aRQV0cdPenRo9yRiIh0HJlNCqyLkoKSgojIFplNClaXqo96ljsSEZGOI7NJocv6OpUURESayWxSqNiwjjp6UlVV7khERDqO7CaF+qg+6t693JGIiHQcmU0KXevXUUcvunYtdyQiIh1HZpNCZUMd9RU9Mf1NQUSkSXaTwsZ1bKjoVe4wREQ6lMwmhe4N66jvqqQgIpIvm0mhoYEK30RDpf6kICKSL5tJYd06ADZ2U0lBRCRfNu+9SUmhQUlBJJMaGhqorq5mw4YN5Q6lXVVVVTF8+HAqKyuLHifTSWFzN1UfiWRRdXU1ffr0YdSoUdhueguiu7NixQqqq6sZPXp00eNls/qorg6ATd1VUhDJog0bNjBgwIDdNiEAmBkDBgxoc2kom0khV1KoUlIQyardOSHk7MgyZjopNFap+khEJF82k0KqPmrsoZKCiJTe6tWr+cUvftHm8c444wxWr17dDhFtkc2kkEoK3lNJQURKr6WksGnTplbHe+SRR+jbt297hQVk/O4jPWFHRL79bZg2bddOc8wYuOGGlvtfeeWVvPPOO4wZM4bKykqqqqro168fM2fOZNasWZxzzjksWLCADRs28K1vfYvLLrsMgFGjRjFlyhRqa2uZMGEC48eP5+9//zvDhg1j4sSJ9NgFD4jJdEmBXiopiEjpXXvttey3335MmzaN6667jqlTp3LjjTcya9YsAG677TZeeeUVpkyZws9+9jNWrFixzTRmz57NN77xDaZPn07fvn154IEHdklsJS8pmNkI4DfAYMCBW9z9RjPrD9wDjALmAue5+6p2CWLQIKZUjFNSEJFWz+hLZdy4cVv9l+BnP/sZDz74IAALFixg9uzZDBgwYKtxRo8ezZgxYwA4+uijmTt37i6JpRwlhU3Ad939UOA44BtmdihwJfCkux8APJm+t4+LL2Z815fo2rNbu81CRKRYvfJOUJ9++mmeeOIJXnjhBV577TWOPPLIgv816J73hLCKiortXo8oVsmTgrsvdvep6XMNMAMYBpwN3JEGuwM4p/1igPp69ChOESmLPn36UFNTU7DfmjVr6NevHz179mTmzJm8+OKLJY2trBeazWwUcCTwEjDY3RenXkuI6qVC41wGXAYwcuTIHZpvQ0O861GcIlIOAwYM4IQTTuDwww+nR48eDB685XB3+umnc/PNN3PIIYdw0EEHcdxxx5U0trIlBTPrDTwAfNvd1+b/887d3cy80HjufgtwC8DYsWMLDrM99fXxrqQgIuVy1113FezevXt3Hn300YL9ctcNBg4cyJtvvtnU/Xvf+94ui6ssdx+ZWSWREH7n7n9InZea2ZDUfwiwrL3mn6ueU1IQEdlayZOCRZHgVmCGu1+f1+th4JL0+RJgYnvFkCsp6JqCiMjWylF9dALweeANM8v9ZeRfgGuBe83sUmAecF57BaDqIxGRwkqeFNz9OaClpvtOKUUMSgoiIoVl8h/NSgoiIoVlMinkLjTrmoKIyNYymRRUUhCRctrRprMBbrjhBupS8//tQUlBRKTEOnJSyGTT2UoKItKkDG1n5zed/fGPf5y99tqLe++9l/r6es4991x+8IMfsG7dOs477zyqq6vZvHkz3//+91m6dCmLFi3ipJNOYuDAgUyaNGnXxk1Gk4KuKYhIOV177bW8+eabTJs2jccee4z777+fyZMn4+6cddZZPPvssyxfvpyhQ4fy5z//GYg2kfbcc0+uv/56Jk2axMCBA9sltkwmBZUURKRJmdvOfuyxx3jsscc48sgjAaitrWX27Nl8+MMf5rvf/S5XXHEFn/zkJ/nwhz9ckniUFEREysjdueqqq/jKV76yTb+pU6fyyCOP8G//9m+ccsopXH311e0ejy40i4iUWH7T2aeddhq33XYbtbW1ACxcuJBly5axaNEievbsycUXX8zll1/O1KlTtxm3PWSypKBrCiJSTvlNZ0+YMIELL7yQ448/HoDevXtz5513MmfOHC6//HK6dOlCZWUlN910EwCXXXYZp59+OkOHDm2XC83mvkOtT3cIY8eO9SlTprR5vIkT4be/hbvugm56+JpI5syYMYNDDjmk3GGURKFlNbNX3H1soeEzWVI4++x4iYjI1jJ5TUFERApTUhCRTOrMVefF2pFlVFIQkcypqqpixYoVu3VicHdWrFhBVRvvqMnkNQURybbhw4dTXV3N8uXLyx1Ku6qqqmL48OFtGkdJQUQyp7KyktGjR5c7jA5J1UciItJESUFERJooKYiISJNO/Y9mM1sOzNvB0QcC7+/CcMpJy9IxaVk6Ji0L7OPugwr16NRJYWeY2ZSW/ubd2WhZOiYtS8ekZWmdqo9ERKSJkoKIiDTJclK4pdwB7EJalo5Jy9IxaVlakdlrCiIisq0slxRERKQZJQUREWmSyaRgZqeb2dtmNsfMrix3PG1lZnPN7A0zm2ZmU1K3/mb2uJnNTu/9yh1nIWZ2m5ktM7M387oVjN3Cz9J2et3Mjipf5NtqYVmuMbOFadtMM7Mz8vpdlZblbTM7rTxRb8vMRpjZJDN7y8ymm9m3UvdOt11aWZbOuF2qzGyymb2WluUHqftoM3spxXyPmXVL3bun73NS/1E7NGN3z9QLqADeAfYFugGvAYeWO642LsNcYGCzbj8BrkyfrwR+XO44W4j9I8BRwJvbix04A3gUMOA44KVyx1/EslwDfK/AsIemfa07MDrtgxXlXoYU2xDgqPS5DzArxdvptksry9IZt4sBvdPnSuCltL7vBc5P3W8GvpY+fx24OX0+H7hnR+abxZLCOGCOu7/r7huB3wO7w8M5zwbuSJ/vAM4pYywtcvdngZXNOrcU+9nAbzy8CPQ1syGliXT7WliWlpwN/N7d6939PWAOsS+Wnbsvdvep6XMNMAMYRifcLq0sS0s68nZxd69NXyvTy4GTgftT9+bbJbe97gdOMTNr63yzmBSGAQvyvlfT+k7TETnwmJm9YmaXpW6D3X1x+rwEGFye0HZIS7F31m31f1K1ym151XidYllSlcORxFlpp94uzZYFOuF2MbMKM5sGLAMeJ0oyq919UxokP96mZUn91wAD2jrPLCaF3cF4dz8KmAB8w8w+kt/To/zYKe817syxJzcB+wFjgMXAf5U3nOKZWW/gAeDb7r42v19n2y4FlqVTbhd33+zuY4DhRAnm4PaeZxaTwkJgRN734albp+HuC9P7MuBBYmdZmivCp/dl5YuwzVqKvdNtK3dfmn7IjcAv2VIV0aGXxcwqiYPo79z9D6lzp9wuhZals26XHHdfDUwCjieq63IPSMuPt2lZUv89gRVtnVcWk8LLwAHpCn434oLMw2WOqWhm1svM+uQ+A6cCbxLLcEka7BJgYnki3CEtxf4w8IV0t8txwJq86owOqVnd+rnEtoFYlvPTHSKjgQOAyaWOr5BU73wrMMPdr8/r1em2S0vL0km3yyAz65s+9wA+TlwjmQR8Jg3WfLvkttdngKdSCa9tyn2FvRwv4u6JWUT93L+WO542xr4vcbfEa8D0XPxE3eGTwGzgCaB/uWNtIf67ieJ7A1EfemlLsRN3X/xP2k5vAGPLHX8Ry/LbFOvr6Uc6JG/4f03L8jYwodzx58U1nqgaeh2Yll5ndMbt0sqydMbt8kHg1RTzm8DVqfu+ROKaA9wHdE/dq9L3Oan/vjsyXzVzISIiTbJYfSQiIi1QUhARkSZKCiIi0kRJQUREmigpiIhIEyUF6RTMzM3sv/K+f8/MrtlF077dzD6z/SF3ej6fNbMZZjapvefVbL5fNLP/LuU8pfNSUpDOoh74lJkNLHcg+fL+WVqMS4Evu/tJ7RWPyM5SUpDOYhPxPNrvNO/R/EzfzGrT+4lm9oyZTTSzd83sWjO7KLVR/4aZ7Zc3mY+Z2RQzm2Vmn0zjV5jZdWb2cmpI7St50/2bmT0MvFUgngvS9N80sx+nblcTf6y61cyuKzDO5XnzybWbP8rMZprZ71IJ434z65n6nWJmr6b53GZm3VP3Y8zs7xZt8E/O/fsdGGpmf7F4NsJP8pbv9hTnG2a2zbqV7GnLWY5Iuf0P8HruoFakI4BDiCau3wV+5e7jLB6+8k3g22m4UUR7OPsBk8xsf+ALRBMOx6SD7vNm9lga/ijgcI/mlpuY2VDgx8DRwCqiNdtz3P2HZnYy0ab/lGbjnEo0rzCO+Lfww6mRw/nAQcCl7v68md0GfD1VBd0OnOLus8zsN8DXzOwXwD3A59z9ZTPbA1ifZjOGaDG0HnjbzH4O7AUMc/fDUxx927BeZTelkoJ0Gh6tXf4G+Mc2jPayRxv79URTBrmD+htEIsi5190b3X02kTwOJtqV+oJF08UvEc0+HJCGn9w8ISTHAE+7+3KP5ot/RzyMpzWnpterwNQ079x8Frj78+nznURp4yDgPXeflbrfkeZxELDY3V+GWF++pYnlJ919jbtvIEo3+6Tl3NfMfm5mpwNbtYwq2aSSgnQ2NxAHzl/nddtEOsExsy7EE/Vy6vM+N+Z9b2Tr/b95ey9OnLV/093/mt/DzE4E1u1Y+AUZ8P/c/X+bzWdUC3HtiPz1sBno6u6rzOwI4DTgq8B5wD/s4PRlN6GSgnQq7r6SeBzhpXmd5xLVNQBnEU+oaqvPmlmXdJ1hX6JxtL8S1TKVAGZ2YGqZtjWTgY+a2UAzqwAuAJ7Zzjh/Bf7B4hkAmNkwM9sr9RtpZsenzxcCz6XYRqUqLoDPp3m8DQwxs2PSdPq0diE8XbTv4u4PAP9GVIlJxqmkIJ3RfwH/J+/7L4GJZvYa8Bd27Cx+PnFA3wP4qrtvMLNfEVVMU1OTzMvZzmNO3X2xmV1JNG9swJ/dvdVmzN39MTM7BHghZkMtcDFxRv828SCl24hqn5tSbF8C7ksH/ZeJZ/NuNLPPAT9PTS2vBz7WyqyHAb9OpSuAq1qLU7JBraSKdFCp+uhPuQvBIqWg6iMREWmikoKIiDRRSUFERJooKYiISBMlBRERaaKkICIiTZQURESkyf8HY6x8VrCo56QAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(range(len(train_acc_)), train_acc_, 'b')\n",
        "plt.plot(range(len(test_acc_)), test_acc_, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"ResNet34: Accuracy vs Number of epochs\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "id": "ArgupDVRwB8i",
        "outputId": "7a6857e2-4b87-48e3-ec4c-82c2da11c1b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 1\n",
            "iteration :  50, loss : 1.9576, accuracy : 27.44\n",
            "iteration : 100, loss : 1.8206, accuracy : 32.25\n",
            "iteration : 150, loss : 1.7145, accuracy : 36.39\n",
            "iteration : 200, loss : 1.6430, accuracy : 39.11\n",
            "iteration : 250, loss : 1.5699, accuracy : 42.02\n",
            "iteration : 300, loss : 1.5136, accuracy : 44.29\n",
            "iteration : 350, loss : 1.4667, accuracy : 46.18\n",
            "epoch :   1, training loss : 1.4241, training accuracy : 47.81, test loss : 1.1884, test accuracy : 58.42\n",
            "\n",
            "Epoch: 2\n",
            "iteration :  50, loss : 1.0251, accuracy : 63.84\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-cc4225c6e06c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-0515b8342e9c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, net, criterion, trainloader, scheduler)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# main body\n",
        "config = {\n",
        "    'lr': 0.01,\n",
        "    'momentum': 0.9,\n",
        "    'weight_decay': 5e-4\n",
        "}\n",
        "\n",
        "\n",
        "net = ResNet18().to('cuda')\n",
        "criterion = nn.CrossEntropyLoss().to('cuda')\n",
        "optimizer = optim.SGD(net.parameters(), lr=config['lr'],\n",
        "                      momentum=config['momentum'], weight_decay=config['weight_decay'])\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "\n",
        "for epoch in range(1, 200):\n",
        "    train_loss, train_acc = train(epoch, net, criterion, trainloader, scheduler)\n",
        "    test_loss, test_acc = test(epoch, net, criterion, testloader)\n",
        "    \n",
        "    print((\"Epoch : %3d, training loss : %0.4f, training accuracy : %2.2f, test loss \" + \\\n",
        "      \": %0.4f, test accuracy : %2.2f\") % (epoch, train_loss, train_acc, test_loss, test_acc))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "5e-4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3dnchRvgsIh",
        "outputId": "192c5271-8d64-4f5c-f047-c09375cc578f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0005"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "1e-2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-7CTXbhgsn3",
        "outputId": "092221a8-4d80-4c48-939f-655dcf5a0aee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.01"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jpkJiS0QguTc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "DL_GroupProject_BaseModel.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "40fedcaabd624ab3b95cfebf95034b15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e380bbe9b15f42fd8cfa239b3379b449",
              "IPY_MODEL_4eff03d9edec444b8daa57e55e8688b8",
              "IPY_MODEL_ee95313186f9486c99c9198a5854d59c"
            ],
            "layout": "IPY_MODEL_65dbc664a18941669908f3b200809529"
          }
        },
        "e380bbe9b15f42fd8cfa239b3379b449": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3081d16b112b495d9e1c371ab1776bbc",
            "placeholder": "​",
            "style": "IPY_MODEL_27370b2f8fca438ab3cd62ea6e121faa",
            "value": ""
          }
        },
        "4eff03d9edec444b8daa57e55e8688b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9462c055aebf44b2a96146983bd119a6",
            "max": 169001437,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a09f4dd072984101bd97fca674ed4bb0",
            "value": 169001437
          }
        },
        "ee95313186f9486c99c9198a5854d59c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce7d6307fb584989a2afd7a1b9e49bfa",
            "placeholder": "​",
            "style": "IPY_MODEL_042e06c739764160aa47460391c05f77",
            "value": " 169001984/? [00:03&lt;00:00, 52703993.42it/s]"
          }
        },
        "65dbc664a18941669908f3b200809529": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3081d16b112b495d9e1c371ab1776bbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27370b2f8fca438ab3cd62ea6e121faa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9462c055aebf44b2a96146983bd119a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a09f4dd072984101bd97fca674ed4bb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ce7d6307fb584989a2afd7a1b9e49bfa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "042e06c739764160aa47460391c05f77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}