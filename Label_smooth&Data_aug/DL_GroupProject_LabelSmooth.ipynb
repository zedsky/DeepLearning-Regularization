{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfGrai_Qt7Ny"
      },
      "outputs": [],
      "source": [
        "# import all libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "6c36c4eadda3424abdeacd3783a1c43b",
            "bb2d457ec6784ab0b52dcfc7073696a2",
            "c50aff42b73b4b6d8141c60dc96b7775",
            "fa0a6677abcf4896a98e98cd2e62c949",
            "6533f4a89d054c6ab1d3e8d6031e4f17",
            "87ff2bb3b61447238bb0ff9a0ea52ba5",
            "a0641be4884b4074a828c71961bb9101",
            "9417521d49204099b144c4c9378cc05e",
            "3b72d049dfec41c6b2900d5d85213e7e",
            "39fe80cc93f24c6a8d964934f467db92",
            "7e589638b6cf444e920d0d753a680ca4"
          ]
        },
        "id": "VgAiImV0uURP",
        "outputId": "9f8322f7-b743-4e7f-b94e-cf6b4112ab88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/169001437 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c36c4eadda3424abdeacd3783a1c43b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# base setting for cifar100\n",
        "transform_train = transforms.Compose([\n",
        "    #transforms.RandomCrop(32, padding=4),\n",
        "    #transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5074,0.4867,0.4411),(0.2011,0.1987,0.2025)),\n",
        "    #transforms.RandomErasing(value = (0.5074,0.4867,0.4411))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5074,0.4867,0.4411),(0.2011,0.1987,0.2025)),\n",
        "])\n",
        "\n",
        "#download CIFAR100 dataset\n",
        "trainset = torchvision.datasets.CIFAR100(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR100(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "# we can use a larger batch size during test, because we do not save \n",
        "# intermediate variables for gradient computation, which leaves more memory\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n",
        "\n",
        "#classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "#           'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for image,label in trainset:\n",
        "    print(\"Image shape: \",image.shape)\n",
        "    print(\"Image tensor: \", image)\n",
        "    print(\"Label: \", label)\n",
        "    break"
      ],
      "metadata": {
        "id": "r9w0GtgVNgIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(trainset.classes)"
      ],
      "metadata": {
        "id": "gvORam6KNxvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_classes_items = dict()\n",
        "\n",
        "for train_item in trainset:\n",
        "    label = trainset.classes[train_item[1]]\n",
        "    if label not in train_classes_items:\n",
        "        train_classes_items[label] = 1\n",
        "    else:\n",
        "        train_classes_items[label] += 1\n",
        "\n",
        "train_classes_items"
      ],
      "metadata": {
        "id": "B_YV8xb0N-NA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_classes_items = dict()\n",
        "for test_item in testset:\n",
        "    label = testset.classes[test_item[1]]\n",
        "    if label not in test_classes_items:\n",
        "        test_classes_items[label] = 1\n",
        "    else:\n",
        "        test_classes_items[label] += 1\n",
        "\n",
        "test_classes_items"
      ],
      "metadata": {
        "id": "yNET6eDxOKm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iP2YfD1gEUC-"
      },
      "source": [
        "See more examples at: https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9e4QhB5B89H"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hldipDVsv-Jt"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "def train(epoch, net, criterion, trainloader, scheduler):\n",
        "    device = 'cuda'\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        if (batch_idx+1) % 50 == 0:\n",
        "          print(\"iteration : %3d, loss : %0.4f, accuracy : %2.2f\" % (batch_idx+1, train_loss/(batch_idx+1), 100.*correct/total))\n",
        "\n",
        "    scheduler.step()\n",
        "    return train_loss/(batch_idx+1), 100.*correct/total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgyCI0U08i2h"
      },
      "source": [
        "Test performance on the test set. Note the use of `torch.inference_mode()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkooK-hQu4a6"
      },
      "outputs": [],
      "source": [
        "def test(epoch, net, criterion, testloader):\n",
        "    device = 'cuda'\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.inference_mode():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    return test_loss/(batch_idx+1), 100.*correct/total\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEj8J7xqwAxD"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(net, acc, epoch):\n",
        "    # Save checkpoint.\n",
        "    print('Saving..')\n",
        "    state = {\n",
        "        'net': net.state_dict(),\n",
        "        'acc': acc,\n",
        "        'epoch': epoch,\n",
        "    }\n",
        "    if not os.path.isdir('checkpoint'):\n",
        "        os.mkdir('checkpoint')\n",
        "    torch.save(state, './checkpoint/ckpt.pth')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlCAjBEWwXNo"
      },
      "outputs": [],
      "source": [
        "# defining resnet models\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=100):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        # This is the \"stem\"\n",
        "        # For CIFAR (32x32 images), it does not perform downsampling\n",
        "        # It should downsample for ImageNet\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        # four stages with three downsampling\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
        "\n",
        "\n",
        "def test_resnet18():\n",
        "    net = ResNet18()\n",
        "    y = net(torch.randn(1, 3, 32, 32))\n",
        "    print(y.size())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9J1Pp8RUVEF",
        "outputId": "dbe14d12-95b3-4dd7-ab8c-b71d33ca9a7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "iteration :  50, loss : 4.4725, accuracy : 4.11\n",
            "iteration : 100, loss : 4.2972, accuracy : 6.53\n",
            "iteration : 150, loss : 4.1769, accuracy : 8.45\n",
            "iteration : 200, loss : 4.0839, accuracy : 9.92\n",
            "iteration : 250, loss : 3.9980, accuracy : 11.50\n",
            "iteration : 300, loss : 3.9288, accuracy : 12.85\n",
            "iteration : 350, loss : 3.8678, accuracy : 14.04\n",
            "Epoch :   1, training loss : 3.8227, training accuracy : 14.88, test loss : 3.5932, test accuracy : 20.84\n",
            "\n",
            "Epoch: 2\n",
            "iteration :  50, loss : 3.2948, accuracy : 26.05\n",
            "iteration : 100, loss : 3.2650, accuracy : 26.67\n",
            "iteration : 150, loss : 3.2442, accuracy : 27.24\n",
            "iteration : 200, loss : 3.2169, accuracy : 27.82\n",
            "iteration : 250, loss : 3.1971, accuracy : 28.27\n",
            "iteration : 300, loss : 3.1664, accuracy : 29.07\n",
            "iteration : 350, loss : 3.1399, accuracy : 29.83\n",
            "Epoch :   2, training loss : 3.1183, training accuracy : 30.35, test loss : 3.2171, test accuracy : 28.73\n",
            "\n",
            "Epoch: 3\n",
            "iteration :  50, loss : 2.7291, accuracy : 40.00\n",
            "iteration : 100, loss : 2.7224, accuracy : 40.23\n",
            "iteration : 150, loss : 2.7132, accuracy : 40.64\n",
            "iteration : 200, loss : 2.7001, accuracy : 41.16\n",
            "iteration : 250, loss : 2.6899, accuracy : 41.34\n",
            "iteration : 300, loss : 2.6752, accuracy : 41.82\n",
            "iteration : 350, loss : 2.6642, accuracy : 42.13\n",
            "Epoch :   3, training loss : 2.6555, training accuracy : 42.36, test loss : 2.7454, test accuracy : 40.59\n",
            "\n",
            "Epoch: 4\n",
            "iteration :  50, loss : 2.3003, accuracy : 52.67\n",
            "iteration : 100, loss : 2.2841, accuracy : 52.94\n",
            "iteration : 150, loss : 2.2832, accuracy : 52.91\n",
            "iteration : 200, loss : 2.2896, accuracy : 52.82\n",
            "iteration : 250, loss : 2.2917, accuracy : 52.79\n",
            "iteration : 300, loss : 2.2928, accuracy : 52.73\n",
            "iteration : 350, loss : 2.2966, accuracy : 52.69\n",
            "Epoch :   4, training loss : 2.2965, training accuracy : 52.73, test loss : 2.6138, test accuracy : 44.46\n",
            "\n",
            "Epoch: 5\n",
            "iteration :  50, loss : 1.9331, accuracy : 64.23\n",
            "iteration : 100, loss : 1.9362, accuracy : 64.16\n",
            "iteration : 150, loss : 1.9411, accuracy : 64.01\n",
            "iteration : 200, loss : 1.9495, accuracy : 63.69\n",
            "iteration : 250, loss : 1.9680, accuracy : 63.09\n",
            "iteration : 300, loss : 1.9808, accuracy : 62.66\n",
            "iteration : 350, loss : 1.9913, accuracy : 62.31\n",
            "Epoch :   5, training loss : 1.9931, training accuracy : 62.22, test loss : 2.5900, test accuracy : 45.97\n",
            "\n",
            "Epoch: 6\n",
            "iteration :  50, loss : 1.6344, accuracy : 75.08\n",
            "iteration : 100, loss : 1.6151, accuracy : 75.85\n",
            "iteration : 150, loss : 1.6282, accuracy : 75.31\n",
            "iteration : 200, loss : 1.6478, accuracy : 74.37\n",
            "iteration : 250, loss : 1.6617, accuracy : 73.86\n",
            "iteration : 300, loss : 1.6746, accuracy : 73.38\n",
            "iteration : 350, loss : 1.6887, accuracy : 72.84\n",
            "Epoch :   6, training loss : 1.6994, training accuracy : 72.38, test loss : 2.6739, test accuracy : 44.93\n",
            "\n",
            "Epoch: 7\n",
            "iteration :  50, loss : 1.4043, accuracy : 84.08\n",
            "iteration : 100, loss : 1.3641, accuracy : 85.61\n",
            "iteration : 150, loss : 1.3634, accuracy : 85.58\n",
            "iteration : 200, loss : 1.3704, accuracy : 85.18\n",
            "iteration : 250, loss : 1.3792, accuracy : 84.62\n",
            "iteration : 300, loss : 1.3927, accuracy : 83.97\n",
            "iteration : 350, loss : 1.4035, accuracy : 83.55\n",
            "Epoch :   7, training loss : 1.4147, training accuracy : 83.05, test loss : 2.6461, test accuracy : 47.28\n",
            "\n",
            "Epoch: 8\n",
            "iteration :  50, loss : 1.2016, accuracy : 91.77\n",
            "iteration : 100, loss : 1.1686, accuracy : 92.81\n",
            "iteration : 150, loss : 1.1608, accuracy : 93.01\n",
            "iteration : 200, loss : 1.1558, accuracy : 93.24\n",
            "iteration : 250, loss : 1.1537, accuracy : 93.34\n",
            "iteration : 300, loss : 1.1558, accuracy : 93.18\n",
            "iteration : 350, loss : 1.1605, accuracy : 92.95\n",
            "Epoch :   8, training loss : 1.1634, training accuracy : 92.82, test loss : 2.5742, test accuracy : 49.12\n",
            "\n",
            "Epoch: 9\n",
            "iteration :  50, loss : 1.0379, accuracy : 97.19\n",
            "iteration : 100, loss : 1.0180, accuracy : 97.81\n",
            "iteration : 150, loss : 1.0108, accuracy : 97.96\n",
            "iteration : 200, loss : 1.0039, accuracy : 98.16\n",
            "iteration : 250, loss : 1.0002, accuracy : 98.25\n",
            "iteration : 300, loss : 0.9973, accuracy : 98.30\n",
            "iteration : 350, loss : 0.9960, accuracy : 98.27\n",
            "Epoch :   9, training loss : 0.9946, training accuracy : 98.27, test loss : 2.4368, test accuracy : 52.35\n",
            "\n",
            "Epoch: 10\n",
            "iteration :  50, loss : 0.9283, accuracy : 99.50\n",
            "iteration : 100, loss : 0.9189, accuracy : 99.56\n",
            "iteration : 150, loss : 0.9142, accuracy : 99.57\n",
            "iteration : 200, loss : 0.9107, accuracy : 99.62\n",
            "iteration : 250, loss : 0.9085, accuracy : 99.64\n",
            "iteration : 300, loss : 0.9068, accuracy : 99.65\n",
            "iteration : 350, loss : 0.9062, accuracy : 99.65\n",
            "Epoch :  10, training loss : 0.9054, training accuracy : 99.66, test loss : 2.3589, test accuracy : 54.54\n",
            "\n",
            "Epoch: 11\n",
            "iteration :  50, loss : 0.8671, accuracy : 99.88\n",
            "iteration : 100, loss : 0.8649, accuracy : 99.89\n",
            "iteration : 150, loss : 0.8634, accuracy : 99.90\n",
            "iteration : 200, loss : 0.8623, accuracy : 99.91\n",
            "iteration : 250, loss : 0.8615, accuracy : 99.90\n",
            "iteration : 300, loss : 0.8610, accuracy : 99.90\n",
            "iteration : 350, loss : 0.8607, accuracy : 99.90\n",
            "Epoch :  11, training loss : 0.8604, training accuracy : 99.90, test loss : 2.3266, test accuracy : 56.56\n",
            "\n",
            "Epoch: 12\n",
            "iteration :  50, loss : 0.8411, accuracy : 99.97\n",
            "iteration : 100, loss : 0.8396, accuracy : 99.96\n",
            "iteration : 150, loss : 0.8390, accuracy : 99.95\n",
            "iteration : 200, loss : 0.8383, accuracy : 99.95\n",
            "iteration : 250, loss : 0.8383, accuracy : 99.93\n",
            "iteration : 300, loss : 0.8381, accuracy : 99.93\n",
            "iteration : 350, loss : 0.8379, accuracy : 99.93\n",
            "Epoch :  12, training loss : 0.8377, training accuracy : 99.93, test loss : 2.3320, test accuracy : 57.01\n",
            "\n",
            "Epoch: 13\n",
            "iteration :  50, loss : 0.8272, accuracy : 99.95\n",
            "iteration : 100, loss : 0.8264, accuracy : 99.96\n",
            "iteration : 150, loss : 0.8267, accuracy : 99.95\n",
            "iteration : 200, loss : 0.8265, accuracy : 99.94\n",
            "iteration : 250, loss : 0.8263, accuracy : 99.94\n",
            "iteration : 300, loss : 0.8259, accuracy : 99.95\n",
            "iteration : 350, loss : 0.8257, accuracy : 99.95\n",
            "Epoch :  13, training loss : 0.8256, training accuracy : 99.95, test loss : 2.3416, test accuracy : 57.41\n",
            "\n",
            "Epoch: 14\n",
            "iteration :  50, loss : 0.8198, accuracy : 99.95\n",
            "iteration : 100, loss : 0.8193, accuracy : 99.97\n",
            "iteration : 150, loss : 0.8192, accuracy : 99.95\n",
            "iteration : 200, loss : 0.8191, accuracy : 99.96\n",
            "iteration : 250, loss : 0.8189, accuracy : 99.95\n",
            "iteration : 300, loss : 0.8187, accuracy : 99.96\n",
            "iteration : 350, loss : 0.8186, accuracy : 99.95\n",
            "Epoch :  14, training loss : 0.8185, training accuracy : 99.95, test loss : 2.3561, test accuracy : 57.30\n",
            "\n",
            "Epoch: 15\n",
            "iteration :  50, loss : 0.8147, accuracy : 99.97\n",
            "iteration : 100, loss : 0.8143, accuracy : 99.97\n",
            "iteration : 150, loss : 0.8140, accuracy : 99.97\n",
            "iteration : 200, loss : 0.8138, accuracy : 99.96\n",
            "iteration : 250, loss : 0.8135, accuracy : 99.97\n",
            "iteration : 300, loss : 0.8134, accuracy : 99.97\n",
            "iteration : 350, loss : 0.8135, accuracy : 99.96\n",
            "Epoch :  15, training loss : 0.8136, training accuracy : 99.95, test loss : 2.3688, test accuracy : 57.01\n",
            "\n",
            "Epoch: 16\n",
            "iteration :  50, loss : 0.8109, accuracy : 99.95\n",
            "iteration : 100, loss : 0.8109, accuracy : 99.95\n",
            "iteration : 150, loss : 0.8111, accuracy : 99.94\n",
            "iteration : 200, loss : 0.8110, accuracy : 99.93\n",
            "iteration : 250, loss : 0.8108, accuracy : 99.94\n",
            "iteration : 300, loss : 0.8107, accuracy : 99.94\n",
            "iteration : 350, loss : 0.8105, accuracy : 99.95\n",
            "Epoch :  16, training loss : 0.8106, training accuracy : 99.95, test loss : 2.3815, test accuracy : 57.62\n",
            "\n",
            "Epoch: 17\n",
            "iteration :  50, loss : 0.8082, accuracy : 99.97\n",
            "iteration : 100, loss : 0.8079, accuracy : 99.98\n",
            "iteration : 150, loss : 0.8080, accuracy : 99.96\n",
            "iteration : 200, loss : 0.8079, accuracy : 99.96\n",
            "iteration : 250, loss : 0.8078, accuracy : 99.97\n",
            "iteration : 300, loss : 0.8079, accuracy : 99.96\n",
            "iteration : 350, loss : 0.8078, accuracy : 99.96\n",
            "Epoch :  17, training loss : 0.8078, training accuracy : 99.96, test loss : 2.3977, test accuracy : 57.23\n",
            "\n",
            "Epoch: 18\n",
            "iteration :  50, loss : 0.8065, accuracy : 99.97\n",
            "iteration : 100, loss : 0.8060, accuracy : 99.97\n",
            "iteration : 150, loss : 0.8056, accuracy : 99.98\n",
            "iteration : 200, loss : 0.8055, accuracy : 99.97\n",
            "iteration : 250, loss : 0.8055, accuracy : 99.97\n",
            "iteration : 300, loss : 0.8056, accuracy : 99.97\n",
            "iteration : 350, loss : 0.8055, accuracy : 99.97\n",
            "Epoch :  18, training loss : 0.8055, training accuracy : 99.97, test loss : 2.4143, test accuracy : 57.05\n",
            "\n",
            "Epoch: 19\n",
            "iteration :  50, loss : 0.8046, accuracy : 99.97\n",
            "iteration : 100, loss : 0.8045, accuracy : 99.95\n",
            "iteration : 150, loss : 0.8047, accuracy : 99.94\n",
            "iteration : 200, loss : 0.8044, accuracy : 99.94\n",
            "iteration : 250, loss : 0.8042, accuracy : 99.95\n",
            "iteration : 300, loss : 0.8041, accuracy : 99.95\n",
            "iteration : 350, loss : 0.8039, accuracy : 99.95\n",
            "Epoch :  19, training loss : 0.8039, training accuracy : 99.96, test loss : 2.4135, test accuracy : 57.29\n",
            "\n",
            "Epoch: 20\n",
            "iteration :  50, loss : 0.8030, accuracy : 99.97\n",
            "iteration : 100, loss : 0.8026, accuracy : 99.97\n",
            "iteration : 150, loss : 0.8024, accuracy : 99.97\n",
            "iteration : 200, loss : 0.8023, accuracy : 99.97\n",
            "iteration : 250, loss : 0.8021, accuracy : 99.98\n",
            "iteration : 300, loss : 0.8021, accuracy : 99.97\n",
            "iteration : 350, loss : 0.8022, accuracy : 99.96\n",
            "Epoch :  20, training loss : 0.8022, training accuracy : 99.96, test loss : 2.4185, test accuracy : 57.21\n",
            "\n",
            "Epoch: 21\n",
            "iteration :  50, loss : 0.8016, accuracy : 99.95\n",
            "iteration : 100, loss : 0.8013, accuracy : 99.95\n",
            "iteration : 150, loss : 0.8012, accuracy : 99.95\n",
            "iteration : 200, loss : 0.8010, accuracy : 99.96\n",
            "iteration : 250, loss : 0.8008, accuracy : 99.97\n",
            "iteration : 300, loss : 0.8009, accuracy : 99.97\n",
            "iteration : 350, loss : 0.8007, accuracy : 99.97\n",
            "Epoch :  21, training loss : 0.8007, training accuracy : 99.97, test loss : 2.4258, test accuracy : 57.01\n",
            "\n",
            "Epoch: 22\n",
            "iteration :  50, loss : 0.7991, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7992, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7993, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7993, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7993, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7994, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7993, accuracy : 99.97\n",
            "Epoch :  22, training loss : 0.7994, training accuracy : 99.97, test loss : 2.4294, test accuracy : 57.35\n",
            "\n",
            "Epoch: 23\n",
            "iteration :  50, loss : 0.7979, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7986, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7983, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7983, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7983, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7983, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7983, accuracy : 99.97\n",
            "Epoch :  23, training loss : 0.7982, training accuracy : 99.97, test loss : 2.4373, test accuracy : 57.45\n",
            "\n",
            "Epoch: 24\n",
            "iteration :  50, loss : 0.7975, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7974, accuracy : 99.99\n",
            "iteration : 150, loss : 0.7976, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7975, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7977, accuracy : 99.96\n",
            "iteration : 300, loss : 0.7976, accuracy : 99.96\n",
            "iteration : 350, loss : 0.7976, accuracy : 99.96\n",
            "Epoch :  24, training loss : 0.7975, training accuracy : 99.96, test loss : 2.4492, test accuracy : 57.18\n",
            "\n",
            "Epoch: 25\n",
            "iteration :  50, loss : 0.7963, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7967, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7967, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7966, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7967, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7966, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7967, accuracy : 99.96\n",
            "Epoch :  25, training loss : 0.7967, training accuracy : 99.96, test loss : 2.4500, test accuracy : 57.19\n",
            "\n",
            "Epoch: 26\n",
            "iteration :  50, loss : 0.7963, accuracy : 99.94\n",
            "iteration : 100, loss : 0.7959, accuracy : 99.96\n",
            "iteration : 150, loss : 0.7960, accuracy : 99.96\n",
            "iteration : 200, loss : 0.7960, accuracy : 99.95\n",
            "iteration : 250, loss : 0.7958, accuracy : 99.96\n",
            "iteration : 300, loss : 0.7958, accuracy : 99.96\n",
            "iteration : 350, loss : 0.7957, accuracy : 99.96\n",
            "Epoch :  26, training loss : 0.7956, training accuracy : 99.97, test loss : 2.4482, test accuracy : 57.28\n",
            "\n",
            "Epoch: 27\n",
            "iteration :  50, loss : 0.7947, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7948, accuracy : 99.99\n",
            "iteration : 150, loss : 0.7946, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7946, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7946, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7947, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7948, accuracy : 99.97\n",
            "Epoch :  27, training loss : 0.7948, training accuracy : 99.97, test loss : 2.4567, test accuracy : 57.35\n",
            "\n",
            "Epoch: 28\n",
            "iteration :  50, loss : 0.7942, accuracy : 99.95\n",
            "iteration : 100, loss : 0.7939, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7940, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7940, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7941, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7942, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7941, accuracy : 99.98\n",
            "Epoch :  28, training loss : 0.7941, training accuracy : 99.97, test loss : 2.4628, test accuracy : 57.53\n",
            "\n",
            "Epoch: 29\n",
            "iteration :  50, loss : 0.7932, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7933, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7934, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7934, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7933, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7934, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7934, accuracy : 99.97\n",
            "Epoch :  29, training loss : 0.7934, training accuracy : 99.97, test loss : 2.4514, test accuracy : 57.12\n",
            "\n",
            "Epoch: 30\n",
            "iteration :  50, loss : 0.7931, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7930, accuracy : 99.99\n",
            "iteration : 150, loss : 0.7929, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7928, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7929, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7929, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7928, accuracy : 99.98\n",
            "Epoch :  30, training loss : 0.7928, training accuracy : 99.97, test loss : 2.4680, test accuracy : 57.36\n",
            "\n",
            "Epoch: 31\n",
            "iteration :  50, loss : 0.7922, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7920, accuracy : 100.00\n",
            "iteration : 150, loss : 0.7921, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7921, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7922, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7922, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7922, accuracy : 99.98\n",
            "Epoch :  31, training loss : 0.7922, training accuracy : 99.98, test loss : 2.4779, test accuracy : 57.24\n",
            "\n",
            "Epoch: 32\n",
            "iteration :  50, loss : 0.7916, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7915, accuracy : 99.99\n",
            "iteration : 150, loss : 0.7914, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7914, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7915, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7915, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7915, accuracy : 99.98\n",
            "Epoch :  32, training loss : 0.7915, training accuracy : 99.97, test loss : 2.4676, test accuracy : 57.64\n",
            "\n",
            "Epoch: 33\n",
            "iteration :  50, loss : 0.7908, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7905, accuracy : 99.99\n",
            "iteration : 150, loss : 0.7905, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7905, accuracy : 100.00\n",
            "iteration : 250, loss : 0.7905, accuracy : 99.99\n",
            "iteration : 300, loss : 0.7906, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7908, accuracy : 99.98\n",
            "Epoch :  33, training loss : 0.7908, training accuracy : 99.97, test loss : 2.4814, test accuracy : 57.17\n",
            "\n",
            "Epoch: 34\n",
            "iteration :  50, loss : 0.7904, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7905, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7905, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7905, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7904, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7905, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7905, accuracy : 99.98\n",
            "Epoch :  34, training loss : 0.7905, training accuracy : 99.97, test loss : 2.4768, test accuracy : 57.31\n",
            "\n",
            "Epoch: 35\n",
            "iteration :  50, loss : 0.7903, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7899, accuracy : 99.99\n",
            "iteration : 150, loss : 0.7899, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7900, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7900, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7901, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7901, accuracy : 99.98\n",
            "Epoch :  35, training loss : 0.7901, training accuracy : 99.97, test loss : 2.4879, test accuracy : 56.98\n",
            "\n",
            "Epoch: 36\n",
            "iteration :  50, loss : 0.7895, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7896, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7895, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7895, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7895, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7895, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7895, accuracy : 99.97\n",
            "Epoch :  36, training loss : 0.7894, training accuracy : 99.97, test loss : 2.4856, test accuracy : 57.19\n",
            "\n",
            "Epoch: 37\n",
            "iteration :  50, loss : 0.7890, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7890, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7890, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7891, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7892, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7891, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7892, accuracy : 99.97\n",
            "Epoch :  37, training loss : 0.7892, training accuracy : 99.97, test loss : 2.4943, test accuracy : 57.38\n",
            "\n",
            "Epoch: 38\n",
            "iteration :  50, loss : 0.7889, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7889, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7887, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7887, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7887, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7887, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7888, accuracy : 99.97\n",
            "Epoch :  38, training loss : 0.7888, training accuracy : 99.97, test loss : 2.4926, test accuracy : 57.52\n",
            "\n",
            "Epoch: 39\n",
            "iteration :  50, loss : 0.7884, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7884, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7884, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7883, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7883, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7883, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7884, accuracy : 99.97\n",
            "Epoch :  39, training loss : 0.7884, training accuracy : 99.97, test loss : 2.4919, test accuracy : 57.25\n",
            "\n",
            "Epoch: 40\n",
            "iteration :  50, loss : 0.7878, accuracy : 99.95\n",
            "iteration : 100, loss : 0.7881, accuracy : 99.95\n",
            "iteration : 150, loss : 0.7880, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7880, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7880, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7881, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7882, accuracy : 99.96\n",
            "Epoch :  40, training loss : 0.7882, training accuracy : 99.97, test loss : 2.4910, test accuracy : 57.28\n",
            "\n",
            "Epoch: 41\n",
            "iteration :  50, loss : 0.7877, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7877, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7877, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7877, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7878, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7877, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7877, accuracy : 99.97\n",
            "Epoch :  41, training loss : 0.7877, training accuracy : 99.97, test loss : 2.4960, test accuracy : 56.97\n",
            "\n",
            "Epoch: 42\n",
            "iteration :  50, loss : 0.7873, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7874, accuracy : 99.99\n",
            "iteration : 150, loss : 0.7874, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7874, accuracy : 99.99\n",
            "iteration : 250, loss : 0.7875, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7875, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7875, accuracy : 99.98\n",
            "Epoch :  42, training loss : 0.7875, training accuracy : 99.98, test loss : 2.5142, test accuracy : 57.06\n",
            "\n",
            "Epoch: 43\n",
            "iteration :  50, loss : 0.7873, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7872, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7872, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7871, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7872, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7873, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7873, accuracy : 99.98\n",
            "Epoch :  43, training loss : 0.7873, training accuracy : 99.97, test loss : 2.5114, test accuracy : 57.17\n",
            "\n",
            "Epoch: 44\n",
            "iteration :  50, loss : 0.7866, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7866, accuracy : 100.00\n",
            "iteration : 150, loss : 0.7867, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7867, accuracy : 99.99\n",
            "iteration : 250, loss : 0.7867, accuracy : 99.99\n",
            "iteration : 300, loss : 0.7868, accuracy : 99.99\n",
            "iteration : 350, loss : 0.7869, accuracy : 99.98\n",
            "Epoch :  44, training loss : 0.7869, training accuracy : 99.98, test loss : 2.5118, test accuracy : 57.22\n",
            "\n",
            "Epoch: 45\n",
            "iteration :  50, loss : 0.7866, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7866, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7865, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7866, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7866, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7866, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7866, accuracy : 99.97\n",
            "Epoch :  45, training loss : 0.7866, training accuracy : 99.97, test loss : 2.5140, test accuracy : 57.22\n",
            "\n",
            "Epoch: 46\n",
            "iteration :  50, loss : 0.7867, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7866, accuracy : 99.97\n",
            "iteration : 150, loss : 0.7864, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7865, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7865, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7864, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7864, accuracy : 99.97\n",
            "Epoch :  46, training loss : 0.7865, training accuracy : 99.97, test loss : 2.5113, test accuracy : 57.11\n",
            "\n",
            "Epoch: 47\n",
            "iteration :  50, loss : 0.7861, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7863, accuracy : 99.97\n",
            "iteration : 150, loss : 0.7863, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7862, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7863, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7863, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7863, accuracy : 99.97\n",
            "Epoch :  47, training loss : 0.7863, training accuracy : 99.97, test loss : 2.5178, test accuracy : 57.22\n",
            "\n",
            "Epoch: 48\n",
            "iteration :  50, loss : 0.7859, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7860, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7858, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7858, accuracy : 99.99\n",
            "iteration : 250, loss : 0.7859, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7858, accuracy : 99.99\n",
            "iteration : 350, loss : 0.7859, accuracy : 99.98\n",
            "Epoch :  48, training loss : 0.7859, training accuracy : 99.98, test loss : 2.5204, test accuracy : 57.14\n",
            "\n",
            "Epoch: 49\n",
            "iteration :  50, loss : 0.7859, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7857, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7857, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7856, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7857, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7857, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7857, accuracy : 99.97\n",
            "Epoch :  49, training loss : 0.7857, training accuracy : 99.97, test loss : 2.5225, test accuracy : 57.26\n",
            "\n",
            "Epoch: 50\n",
            "iteration :  50, loss : 0.7855, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7854, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7855, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7855, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7855, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7855, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7856, accuracy : 99.97\n",
            "Epoch :  50, training loss : 0.7855, training accuracy : 99.97, test loss : 2.5262, test accuracy : 57.15\n",
            "\n",
            "Epoch: 51\n",
            "iteration :  50, loss : 0.7853, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7857, accuracy : 99.96\n",
            "iteration : 150, loss : 0.7854, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7854, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7853, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7854, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7853, accuracy : 99.98\n",
            "Epoch :  51, training loss : 0.7854, training accuracy : 99.97, test loss : 2.5202, test accuracy : 56.71\n",
            "\n",
            "Epoch: 52\n",
            "iteration :  50, loss : 0.7853, accuracy : 99.95\n",
            "iteration : 100, loss : 0.7854, accuracy : 99.95\n",
            "iteration : 150, loss : 0.7853, accuracy : 99.96\n",
            "iteration : 200, loss : 0.7852, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7851, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7852, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7852, accuracy : 99.97\n",
            "Epoch :  52, training loss : 0.7851, training accuracy : 99.97, test loss : 2.5245, test accuracy : 57.03\n",
            "\n",
            "Epoch: 53\n",
            "iteration :  50, loss : 0.7846, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7847, accuracy : 100.00\n",
            "iteration : 150, loss : 0.7849, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7850, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7850, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7849, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7850, accuracy : 99.98\n",
            "Epoch :  53, training loss : 0.7850, training accuracy : 99.98, test loss : 2.5229, test accuracy : 56.77\n",
            "\n",
            "Epoch: 54\n",
            "iteration :  50, loss : 0.7848, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7848, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7850, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7849, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7849, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7849, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7849, accuracy : 99.97\n",
            "Epoch :  54, training loss : 0.7850, training accuracy : 99.97, test loss : 2.5262, test accuracy : 56.76\n",
            "\n",
            "Epoch: 55\n",
            "iteration :  50, loss : 0.7850, accuracy : 99.95\n",
            "iteration : 100, loss : 0.7848, accuracy : 99.97\n",
            "iteration : 150, loss : 0.7846, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7847, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7847, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7847, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7847, accuracy : 99.98\n",
            "Epoch :  55, training loss : 0.7847, training accuracy : 99.97, test loss : 2.5330, test accuracy : 57.01\n",
            "\n",
            "Epoch: 56\n",
            "iteration :  50, loss : 0.7849, accuracy : 99.95\n",
            "iteration : 100, loss : 0.7848, accuracy : 99.97\n",
            "iteration : 150, loss : 0.7847, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7846, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7846, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7846, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7846, accuracy : 99.97\n",
            "Epoch :  56, training loss : 0.7846, training accuracy : 99.97, test loss : 2.5349, test accuracy : 56.90\n",
            "\n",
            "Epoch: 57\n",
            "iteration :  50, loss : 0.7849, accuracy : 99.92\n",
            "iteration : 100, loss : 0.7847, accuracy : 99.95\n",
            "iteration : 150, loss : 0.7845, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7844, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7845, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7845, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7845, accuracy : 99.97\n",
            "Epoch :  57, training loss : 0.7845, training accuracy : 99.97, test loss : 2.5380, test accuracy : 56.88\n",
            "\n",
            "Epoch: 58\n",
            "iteration :  50, loss : 0.7840, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7842, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7844, accuracy : 99.96\n",
            "iteration : 200, loss : 0.7843, accuracy : 99.96\n",
            "iteration : 250, loss : 0.7843, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7843, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7843, accuracy : 99.97\n",
            "Epoch :  58, training loss : 0.7843, training accuracy : 99.97, test loss : 2.5374, test accuracy : 57.04\n",
            "\n",
            "Epoch: 59\n",
            "iteration :  50, loss : 0.7841, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7841, accuracy : 100.00\n",
            "iteration : 150, loss : 0.7841, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7842, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7841, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7842, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7842, accuracy : 99.97\n",
            "Epoch :  59, training loss : 0.7841, training accuracy : 99.97, test loss : 2.5410, test accuracy : 56.76\n",
            "\n",
            "Epoch: 60\n",
            "iteration :  50, loss : 0.7841, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7842, accuracy : 99.97\n",
            "iteration : 150, loss : 0.7843, accuracy : 99.96\n",
            "iteration : 200, loss : 0.7842, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7842, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7842, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7841, accuracy : 99.97\n",
            "Epoch :  60, training loss : 0.7841, training accuracy : 99.97, test loss : 2.5400, test accuracy : 56.82\n",
            "\n",
            "Epoch: 61\n",
            "iteration :  50, loss : 0.7837, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7837, accuracy : 100.00\n",
            "iteration : 150, loss : 0.7839, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7839, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7839, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7839, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7839, accuracy : 99.97\n",
            "Epoch :  61, training loss : 0.7839, training accuracy : 99.97, test loss : 2.5346, test accuracy : 56.46\n",
            "\n",
            "Epoch: 62\n",
            "iteration :  50, loss : 0.7839, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7839, accuracy : 99.99\n",
            "iteration : 150, loss : 0.7839, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7838, accuracy : 99.99\n",
            "iteration : 250, loss : 0.7838, accuracy : 99.99\n",
            "iteration : 300, loss : 0.7837, accuracy : 99.99\n",
            "iteration : 350, loss : 0.7838, accuracy : 99.98\n",
            "Epoch :  62, training loss : 0.7838, training accuracy : 99.98, test loss : 2.5367, test accuracy : 56.85\n",
            "\n",
            "Epoch: 63\n",
            "iteration :  50, loss : 0.7837, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7835, accuracy : 99.99\n",
            "iteration : 150, loss : 0.7835, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7836, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7837, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7837, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7837, accuracy : 99.98\n",
            "Epoch :  63, training loss : 0.7837, training accuracy : 99.97, test loss : 2.5420, test accuracy : 56.85\n",
            "\n",
            "Epoch: 64\n",
            "iteration :  50, loss : 0.7836, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7834, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7836, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7836, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7836, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7835, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7836, accuracy : 99.98\n",
            "Epoch :  64, training loss : 0.7836, training accuracy : 99.97, test loss : 2.5423, test accuracy : 56.53\n",
            "\n",
            "Epoch: 65\n",
            "iteration :  50, loss : 0.7834, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7836, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7836, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7835, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7835, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7835, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7835, accuracy : 99.98\n",
            "Epoch :  65, training loss : 0.7835, training accuracy : 99.97, test loss : 2.5472, test accuracy : 56.83\n",
            "\n",
            "Epoch: 66\n",
            "iteration :  50, loss : 0.7838, accuracy : 99.95\n",
            "iteration : 100, loss : 0.7835, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7834, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7834, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7834, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7834, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7834, accuracy : 99.97\n",
            "Epoch :  66, training loss : 0.7834, training accuracy : 99.97, test loss : 2.5486, test accuracy : 56.57\n",
            "\n",
            "Epoch: 67\n",
            "iteration :  50, loss : 0.7832, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7833, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7833, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7833, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7833, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7833, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7833, accuracy : 99.98\n",
            "Epoch :  67, training loss : 0.7833, training accuracy : 99.97, test loss : 2.5375, test accuracy : 56.56\n",
            "\n",
            "Epoch: 68\n",
            "iteration :  50, loss : 0.7831, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7832, accuracy : 99.97\n",
            "iteration : 150, loss : 0.7832, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7832, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7832, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7832, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7832, accuracy : 99.97\n",
            "Epoch :  68, training loss : 0.7832, training accuracy : 99.97, test loss : 2.5527, test accuracy : 56.47\n",
            "\n",
            "Epoch: 69\n",
            "iteration :  50, loss : 0.7836, accuracy : 99.94\n",
            "iteration : 100, loss : 0.7834, accuracy : 99.95\n",
            "iteration : 150, loss : 0.7833, accuracy : 99.96\n",
            "iteration : 200, loss : 0.7832, accuracy : 99.96\n",
            "iteration : 250, loss : 0.7832, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7831, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7831, accuracy : 99.97\n",
            "Epoch :  69, training loss : 0.7831, training accuracy : 99.97, test loss : 2.5480, test accuracy : 56.85\n",
            "\n",
            "Epoch: 70\n",
            "iteration :  50, loss : 0.7831, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7830, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7829, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7829, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7830, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7830, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7830, accuracy : 99.97\n",
            "Epoch :  70, training loss : 0.7830, training accuracy : 99.97, test loss : 2.5531, test accuracy : 56.72\n",
            "\n",
            "Epoch: 71\n",
            "iteration :  50, loss : 0.7826, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7828, accuracy : 99.99\n",
            "iteration : 150, loss : 0.7828, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7828, accuracy : 99.99\n",
            "iteration : 250, loss : 0.7829, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7829, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7829, accuracy : 99.98\n",
            "Epoch :  71, training loss : 0.7829, training accuracy : 99.98, test loss : 2.5519, test accuracy : 56.67\n",
            "\n",
            "Epoch: 72\n",
            "iteration :  50, loss : 0.7828, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7828, accuracy : 99.97\n",
            "iteration : 150, loss : 0.7828, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7827, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7828, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7827, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7828, accuracy : 99.98\n",
            "Epoch :  72, training loss : 0.7828, training accuracy : 99.97, test loss : 2.5504, test accuracy : 56.67\n",
            "\n",
            "Epoch: 73\n",
            "iteration :  50, loss : 0.7829, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7828, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7827, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7827, accuracy : 99.99\n",
            "iteration : 250, loss : 0.7826, accuracy : 99.99\n",
            "iteration : 300, loss : 0.7827, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7827, accuracy : 99.98\n",
            "Epoch :  73, training loss : 0.7828, training accuracy : 99.98, test loss : 2.5510, test accuracy : 56.80\n",
            "\n",
            "Epoch: 74\n",
            "iteration :  50, loss : 0.7824, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7824, accuracy : 99.99\n",
            "iteration : 150, loss : 0.7826, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7827, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7827, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7826, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7826, accuracy : 99.98\n",
            "Epoch :  74, training loss : 0.7827, training accuracy : 99.97, test loss : 2.5536, test accuracy : 56.66\n",
            "\n",
            "Epoch: 75\n",
            "iteration :  50, loss : 0.7827, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7826, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7825, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7826, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7826, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7826, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7826, accuracy : 99.98\n",
            "Epoch :  75, training loss : 0.7826, training accuracy : 99.98, test loss : 2.5497, test accuracy : 56.65\n",
            "\n",
            "Epoch: 76\n",
            "iteration :  50, loss : 0.7826, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7825, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7826, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7826, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7826, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7826, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7826, accuracy : 99.98\n",
            "Epoch :  76, training loss : 0.7826, training accuracy : 99.97, test loss : 2.5577, test accuracy : 56.72\n",
            "\n",
            "Epoch: 77\n",
            "iteration :  50, loss : 0.7828, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7827, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7826, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7827, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7826, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7826, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7826, accuracy : 99.98\n",
            "Epoch :  77, training loss : 0.7826, training accuracy : 99.98, test loss : 2.5602, test accuracy : 56.63\n",
            "\n",
            "Epoch: 78\n",
            "iteration :  50, loss : 0.7824, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7824, accuracy : 99.99\n",
            "iteration : 150, loss : 0.7824, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7824, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7824, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7824, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7825, accuracy : 99.97\n",
            "Epoch :  78, training loss : 0.7825, training accuracy : 99.97, test loss : 2.5653, test accuracy : 56.47\n",
            "\n",
            "Epoch: 79\n",
            "iteration :  50, loss : 0.7825, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7822, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7822, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7824, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7823, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7824, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7824, accuracy : 99.97\n",
            "Epoch :  79, training loss : 0.7824, training accuracy : 99.97, test loss : 2.5598, test accuracy : 56.31\n",
            "\n",
            "Epoch: 80\n",
            "iteration :  50, loss : 0.7823, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7822, accuracy : 99.99\n",
            "iteration : 150, loss : 0.7822, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7821, accuracy : 99.99\n",
            "iteration : 250, loss : 0.7823, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7822, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7823, accuracy : 99.98\n",
            "Epoch :  80, training loss : 0.7823, training accuracy : 99.98, test loss : 2.5586, test accuracy : 56.65\n",
            "\n",
            "Epoch: 81\n",
            "iteration :  50, loss : 0.7822, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7822, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7823, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7822, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7822, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7822, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7822, accuracy : 99.97\n",
            "Epoch :  81, training loss : 0.7822, training accuracy : 99.97, test loss : 2.5527, test accuracy : 56.37\n",
            "\n",
            "Epoch: 82\n",
            "iteration :  50, loss : 0.7820, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7821, accuracy : 99.99\n",
            "iteration : 150, loss : 0.7820, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7821, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7822, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7822, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7821, accuracy : 99.98\n",
            "Epoch :  82, training loss : 0.7822, training accuracy : 99.97, test loss : 2.5613, test accuracy : 56.31\n",
            "\n",
            "Epoch: 83\n",
            "iteration :  50, loss : 0.7822, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7821, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7820, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7820, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7821, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7821, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7821, accuracy : 99.97\n",
            "Epoch :  83, training loss : 0.7821, training accuracy : 99.97, test loss : 2.5637, test accuracy : 56.23\n",
            "\n",
            "Epoch: 84\n",
            "iteration :  50, loss : 0.7819, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7819, accuracy : 100.00\n",
            "iteration : 150, loss : 0.7819, accuracy : 100.00\n",
            "iteration : 200, loss : 0.7819, accuracy : 99.99\n",
            "iteration : 250, loss : 0.7820, accuracy : 99.99\n",
            "iteration : 300, loss : 0.7820, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7821, accuracy : 99.98\n",
            "Epoch :  84, training loss : 0.7821, training accuracy : 99.98, test loss : 2.5705, test accuracy : 56.43\n",
            "\n",
            "Epoch: 85\n",
            "iteration :  50, loss : 0.7820, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7822, accuracy : 99.97\n",
            "iteration : 150, loss : 0.7820, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7821, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7821, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7820, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7821, accuracy : 99.97\n",
            "Epoch :  85, training loss : 0.7821, training accuracy : 99.97, test loss : 2.5608, test accuracy : 56.55\n",
            "\n",
            "Epoch: 86\n",
            "iteration :  50, loss : 0.7823, accuracy : 99.94\n",
            "iteration : 100, loss : 0.7821, accuracy : 99.96\n",
            "iteration : 150, loss : 0.7821, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7821, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7821, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7820, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7820, accuracy : 99.97\n",
            "Epoch :  86, training loss : 0.7820, training accuracy : 99.97, test loss : 2.5623, test accuracy : 56.50\n",
            "\n",
            "Epoch: 87\n",
            "iteration :  50, loss : 0.7819, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7820, accuracy : 99.96\n",
            "iteration : 150, loss : 0.7819, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7819, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7819, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7819, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7819, accuracy : 99.97\n",
            "Epoch :  87, training loss : 0.7819, training accuracy : 99.97, test loss : 2.5616, test accuracy : 56.48\n",
            "\n",
            "Epoch: 88\n",
            "iteration :  50, loss : 0.7818, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7817, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7819, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7819, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7819, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7819, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7819, accuracy : 99.97\n",
            "Epoch :  88, training loss : 0.7819, training accuracy : 99.97, test loss : 2.5647, test accuracy : 56.40\n",
            "\n",
            "Epoch: 89\n",
            "iteration :  50, loss : 0.7819, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7819, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7818, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7818, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7818, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7818, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7817, accuracy : 99.98\n",
            "Epoch :  89, training loss : 0.7818, training accuracy : 99.97, test loss : 2.5682, test accuracy : 56.27\n",
            "\n",
            "Epoch: 90\n",
            "iteration :  50, loss : 0.7815, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7817, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7817, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7817, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7817, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7817, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7818, accuracy : 99.97\n",
            "Epoch :  90, training loss : 0.7818, training accuracy : 99.97, test loss : 2.5631, test accuracy : 56.54\n",
            "\n",
            "Epoch: 91\n",
            "iteration :  50, loss : 0.7816, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7817, accuracy : 99.99\n",
            "iteration : 150, loss : 0.7817, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7817, accuracy : 99.99\n",
            "iteration : 250, loss : 0.7817, accuracy : 99.99\n",
            "iteration : 300, loss : 0.7817, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7818, accuracy : 99.97\n",
            "Epoch :  91, training loss : 0.7818, training accuracy : 99.97, test loss : 2.5692, test accuracy : 56.29\n",
            "\n",
            "Epoch: 92\n",
            "iteration :  50, loss : 0.7816, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7817, accuracy : 99.97\n",
            "iteration : 150, loss : 0.7817, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7817, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7816, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7816, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7817, accuracy : 99.98\n",
            "Epoch :  92, training loss : 0.7817, training accuracy : 99.97, test loss : 2.5756, test accuracy : 56.37\n",
            "\n",
            "Epoch: 93\n",
            "iteration :  50, loss : 0.7818, accuracy : 99.95\n",
            "iteration : 100, loss : 0.7815, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7814, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7815, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7816, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7816, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7817, accuracy : 99.97\n",
            "Epoch :  93, training loss : 0.7816, training accuracy : 99.97, test loss : 2.5731, test accuracy : 56.27\n",
            "\n",
            "Epoch: 94\n",
            "iteration :  50, loss : 0.7816, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7818, accuracy : 99.96\n",
            "iteration : 150, loss : 0.7816, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7816, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7815, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7816, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7816, accuracy : 99.98\n",
            "Epoch :  94, training loss : 0.7816, training accuracy : 99.98, test loss : 2.5706, test accuracy : 56.30\n",
            "\n",
            "Epoch: 95\n",
            "iteration :  50, loss : 0.7818, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7818, accuracy : 99.97\n",
            "iteration : 150, loss : 0.7818, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7817, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7816, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7817, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7816, accuracy : 99.97\n",
            "Epoch :  95, training loss : 0.7816, training accuracy : 99.97, test loss : 2.5785, test accuracy : 56.14\n",
            "\n",
            "Epoch: 96\n",
            "iteration :  50, loss : 0.7817, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7815, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7815, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7815, accuracy : 99.99\n",
            "iteration : 250, loss : 0.7815, accuracy : 99.99\n",
            "iteration : 300, loss : 0.7814, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7815, accuracy : 99.98\n",
            "Epoch :  96, training loss : 0.7815, training accuracy : 99.98, test loss : 2.5736, test accuracy : 56.13\n",
            "\n",
            "Epoch: 97\n",
            "iteration :  50, loss : 0.7813, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7814, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7813, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7813, accuracy : 99.99\n",
            "iteration : 250, loss : 0.7814, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7814, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7815, accuracy : 99.97\n",
            "Epoch :  97, training loss : 0.7815, training accuracy : 99.97, test loss : 2.5763, test accuracy : 56.36\n",
            "\n",
            "Epoch: 98\n",
            "iteration :  50, loss : 0.7815, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7814, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7815, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7815, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7814, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7815, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7815, accuracy : 99.97\n",
            "Epoch :  98, training loss : 0.7815, training accuracy : 99.97, test loss : 2.5768, test accuracy : 56.24\n",
            "\n",
            "Epoch: 99\n",
            "iteration :  50, loss : 0.7814, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7814, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7815, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7815, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7814, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7814, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7814, accuracy : 99.98\n",
            "Epoch :  99, training loss : 0.7814, training accuracy : 99.98, test loss : 2.5793, test accuracy : 56.18\n",
            "\n",
            "Epoch: 100\n",
            "iteration :  50, loss : 0.7815, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7818, accuracy : 99.95\n",
            "iteration : 150, loss : 0.7816, accuracy : 99.96\n",
            "iteration : 200, loss : 0.7815, accuracy : 99.96\n",
            "iteration : 250, loss : 0.7815, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7814, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7814, accuracy : 99.97\n",
            "Epoch : 100, training loss : 0.7814, training accuracy : 99.97, test loss : 2.5772, test accuracy : 56.38\n",
            "\n",
            "Epoch: 101\n",
            "iteration :  50, loss : 0.7811, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7812, accuracy : 99.99\n",
            "iteration : 150, loss : 0.7813, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7814, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7813, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7813, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7813, accuracy : 99.98\n",
            "Epoch : 101, training loss : 0.7814, training accuracy : 99.97, test loss : 2.5822, test accuracy : 56.22\n",
            "\n",
            "Epoch: 102\n",
            "iteration :  50, loss : 0.7815, accuracy : 99.95\n",
            "iteration : 100, loss : 0.7813, accuracy : 99.97\n",
            "iteration : 150, loss : 0.7814, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7813, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7814, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7814, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7814, accuracy : 99.97\n",
            "Epoch : 102, training loss : 0.7814, training accuracy : 99.97, test loss : 2.5846, test accuracy : 56.06\n",
            "\n",
            "Epoch: 103\n",
            "iteration :  50, loss : 0.7810, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7813, accuracy : 99.95\n",
            "iteration : 150, loss : 0.7813, accuracy : 99.96\n",
            "iteration : 200, loss : 0.7813, accuracy : 99.96\n",
            "iteration : 250, loss : 0.7813, accuracy : 99.96\n",
            "iteration : 300, loss : 0.7813, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7813, accuracy : 99.97\n",
            "Epoch : 103, training loss : 0.7813, training accuracy : 99.97, test loss : 2.5801, test accuracy : 56.41\n",
            "\n",
            "Epoch: 104\n",
            "iteration :  50, loss : 0.7818, accuracy : 99.94\n",
            "iteration : 100, loss : 0.7816, accuracy : 99.95\n",
            "iteration : 150, loss : 0.7814, accuracy : 99.96\n",
            "iteration : 200, loss : 0.7814, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7813, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7813, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7813, accuracy : 99.97\n",
            "Epoch : 104, training loss : 0.7813, training accuracy : 99.97, test loss : 2.5832, test accuracy : 56.00\n",
            "\n",
            "Epoch: 105\n",
            "iteration :  50, loss : 0.7813, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7813, accuracy : 99.97\n",
            "iteration : 150, loss : 0.7813, accuracy : 99.96\n",
            "iteration : 200, loss : 0.7813, accuracy : 99.96\n",
            "iteration : 250, loss : 0.7813, accuracy : 99.96\n",
            "iteration : 300, loss : 0.7813, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7813, accuracy : 99.97\n",
            "Epoch : 105, training loss : 0.7812, training accuracy : 99.97, test loss : 2.5777, test accuracy : 56.27\n",
            "\n",
            "Epoch: 106\n",
            "iteration :  50, loss : 0.7814, accuracy : 99.95\n",
            "iteration : 100, loss : 0.7813, accuracy : 99.97\n",
            "iteration : 150, loss : 0.7812, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7812, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7812, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7812, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7812, accuracy : 99.98\n",
            "Epoch : 106, training loss : 0.7812, training accuracy : 99.97, test loss : 2.5823, test accuracy : 56.18\n",
            "\n",
            "Epoch: 107\n",
            "iteration :  50, loss : 0.7810, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7811, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7811, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7810, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7811, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7811, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7811, accuracy : 99.97\n",
            "Epoch : 107, training loss : 0.7812, training accuracy : 99.97, test loss : 2.5908, test accuracy : 56.01\n",
            "\n",
            "Epoch: 108\n",
            "iteration :  50, loss : 0.7812, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7815, accuracy : 99.95\n",
            "iteration : 150, loss : 0.7814, accuracy : 99.95\n",
            "iteration : 200, loss : 0.7813, accuracy : 99.96\n",
            "iteration : 250, loss : 0.7813, accuracy : 99.96\n",
            "iteration : 300, loss : 0.7813, accuracy : 99.96\n",
            "iteration : 350, loss : 0.7812, accuracy : 99.97\n",
            "Epoch : 108, training loss : 0.7812, training accuracy : 99.97, test loss : 2.5845, test accuracy : 56.14\n",
            "\n",
            "Epoch: 109\n",
            "iteration :  50, loss : 0.7811, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7810, accuracy : 99.99\n",
            "iteration : 150, loss : 0.7811, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7811, accuracy : 99.99\n",
            "iteration : 250, loss : 0.7811, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7811, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7811, accuracy : 99.98\n",
            "Epoch : 109, training loss : 0.7811, training accuracy : 99.98, test loss : 2.5879, test accuracy : 56.05\n",
            "\n",
            "Epoch: 110\n",
            "iteration :  50, loss : 0.7811, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7812, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7812, accuracy : 99.96\n",
            "iteration : 200, loss : 0.7812, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7812, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7811, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7812, accuracy : 99.97\n",
            "Epoch : 110, training loss : 0.7811, training accuracy : 99.97, test loss : 2.5870, test accuracy : 56.15\n",
            "\n",
            "Epoch: 111\n",
            "iteration :  50, loss : 0.7808, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7811, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7810, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7810, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7810, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7811, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7811, accuracy : 99.97\n",
            "Epoch : 111, training loss : 0.7810, training accuracy : 99.97, test loss : 2.5888, test accuracy : 56.20\n",
            "\n",
            "Epoch: 112\n",
            "iteration :  50, loss : 0.7812, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7810, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7810, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7809, accuracy : 99.99\n",
            "iteration : 250, loss : 0.7810, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7810, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7810, accuracy : 99.98\n",
            "Epoch : 112, training loss : 0.7811, training accuracy : 99.97, test loss : 2.5859, test accuracy : 56.07\n",
            "\n",
            "Epoch: 113\n",
            "iteration :  50, loss : 0.7810, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7811, accuracy : 99.96\n",
            "iteration : 150, loss : 0.7811, accuracy : 99.96\n",
            "iteration : 200, loss : 0.7810, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7811, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7810, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7810, accuracy : 99.97\n",
            "Epoch : 113, training loss : 0.7810, training accuracy : 99.97, test loss : 2.5815, test accuracy : 56.20\n",
            "\n",
            "Epoch: 114\n",
            "iteration :  50, loss : 0.7811, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7809, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7809, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7809, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7809, accuracy : 99.99\n",
            "iteration : 300, loss : 0.7809, accuracy : 99.99\n",
            "iteration : 350, loss : 0.7810, accuracy : 99.98\n",
            "Epoch : 114, training loss : 0.7810, training accuracy : 99.98, test loss : 2.5905, test accuracy : 56.09\n",
            "\n",
            "Epoch: 115\n",
            "iteration :  50, loss : 0.7807, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7809, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7809, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7809, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7809, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7809, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7809, accuracy : 99.97\n",
            "Epoch : 115, training loss : 0.7809, training accuracy : 99.97, test loss : 2.5884, test accuracy : 56.02\n",
            "\n",
            "Epoch: 116\n",
            "iteration :  50, loss : 0.7810, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7810, accuracy : 99.97\n",
            "iteration : 150, loss : 0.7810, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7810, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7810, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7809, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7810, accuracy : 99.97\n",
            "Epoch : 116, training loss : 0.7809, training accuracy : 99.97, test loss : 2.5909, test accuracy : 56.12\n",
            "\n",
            "Epoch: 117\n",
            "iteration :  50, loss : 0.7810, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7810, accuracy : 99.96\n",
            "iteration : 150, loss : 0.7809, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7809, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7809, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7809, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7810, accuracy : 99.97\n",
            "Epoch : 117, training loss : 0.7809, training accuracy : 99.97, test loss : 2.5927, test accuracy : 55.87\n",
            "\n",
            "Epoch: 118\n",
            "iteration :  50, loss : 0.7808, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7808, accuracy : 100.00\n",
            "iteration : 150, loss : 0.7808, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7808, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7808, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7808, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7809, accuracy : 99.98\n",
            "Epoch : 118, training loss : 0.7809, training accuracy : 99.98, test loss : 2.5875, test accuracy : 55.90\n",
            "\n",
            "Epoch: 119\n",
            "iteration :  50, loss : 0.7807, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7808, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7809, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7810, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7809, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7809, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7809, accuracy : 99.97\n",
            "Epoch : 119, training loss : 0.7809, training accuracy : 99.97, test loss : 2.5867, test accuracy : 56.13\n",
            "\n",
            "Epoch: 120\n",
            "iteration :  50, loss : 0.7810, accuracy : 99.94\n",
            "iteration : 100, loss : 0.7810, accuracy : 99.95\n",
            "iteration : 150, loss : 0.7808, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7809, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7809, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7808, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7808, accuracy : 99.97\n",
            "Epoch : 120, training loss : 0.7808, training accuracy : 99.97, test loss : 2.5868, test accuracy : 56.25\n",
            "\n",
            "Epoch: 121\n",
            "iteration :  50, loss : 0.7807, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7807, accuracy : 100.00\n",
            "iteration : 150, loss : 0.7808, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7809, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7809, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7808, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7808, accuracy : 99.98\n",
            "Epoch : 121, training loss : 0.7808, training accuracy : 99.98, test loss : 2.5952, test accuracy : 55.91\n",
            "\n",
            "Epoch: 122\n",
            "iteration :  50, loss : 0.7807, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7808, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7807, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7808, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7808, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7808, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7808, accuracy : 99.97\n",
            "Epoch : 122, training loss : 0.7808, training accuracy : 99.97, test loss : 2.5915, test accuracy : 55.87\n",
            "\n",
            "Epoch: 123\n",
            "iteration :  50, loss : 0.7807, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7807, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7807, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7808, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7808, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7807, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7808, accuracy : 99.98\n",
            "Epoch : 123, training loss : 0.7808, training accuracy : 99.98, test loss : 2.5931, test accuracy : 55.99\n",
            "\n",
            "Epoch: 124\n",
            "iteration :  50, loss : 0.7806, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7807, accuracy : 99.99\n",
            "iteration : 150, loss : 0.7807, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7807, accuracy : 99.99\n",
            "iteration : 250, loss : 0.7807, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7808, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7808, accuracy : 99.97\n",
            "Epoch : 124, training loss : 0.7808, training accuracy : 99.97, test loss : 2.5906, test accuracy : 55.93\n",
            "\n",
            "Epoch: 125\n",
            "iteration :  50, loss : 0.7808, accuracy : 99.95\n",
            "iteration : 100, loss : 0.7806, accuracy : 99.97\n",
            "iteration : 150, loss : 0.7807, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7807, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7807, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7807, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7807, accuracy : 99.97\n",
            "Epoch : 125, training loss : 0.7807, training accuracy : 99.97, test loss : 2.5954, test accuracy : 55.94\n",
            "\n",
            "Epoch: 126\n",
            "iteration :  50, loss : 0.7807, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7808, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7807, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7807, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7807, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7807, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7807, accuracy : 99.98\n",
            "Epoch : 126, training loss : 0.7807, training accuracy : 99.98, test loss : 2.5950, test accuracy : 56.00\n",
            "\n",
            "Epoch: 127\n",
            "iteration :  50, loss : 0.7807, accuracy : 99.95\n",
            "iteration : 100, loss : 0.7807, accuracy : 99.95\n",
            "iteration : 150, loss : 0.7808, accuracy : 99.96\n",
            "iteration : 200, loss : 0.7807, accuracy : 99.96\n",
            "iteration : 250, loss : 0.7807, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7807, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7807, accuracy : 99.97\n",
            "Epoch : 127, training loss : 0.7807, training accuracy : 99.97, test loss : 2.5985, test accuracy : 55.68\n",
            "\n",
            "Epoch: 128\n",
            "iteration :  50, loss : 0.7806, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7806, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7806, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7806, accuracy : 99.99\n",
            "iteration : 250, loss : 0.7807, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7807, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7806, accuracy : 99.98\n",
            "Epoch : 128, training loss : 0.7807, training accuracy : 99.98, test loss : 2.5954, test accuracy : 55.75\n",
            "\n",
            "Epoch: 129\n",
            "iteration :  50, loss : 0.7805, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7806, accuracy : 99.97\n",
            "iteration : 150, loss : 0.7806, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7806, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7806, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7806, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7806, accuracy : 99.98\n",
            "Epoch : 129, training loss : 0.7807, training accuracy : 99.97, test loss : 2.5932, test accuracy : 55.90\n",
            "\n",
            "Epoch: 130\n",
            "iteration :  50, loss : 0.7804, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7805, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7805, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7805, accuracy : 99.99\n",
            "iteration : 250, loss : 0.7805, accuracy : 99.99\n",
            "iteration : 300, loss : 0.7805, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7806, accuracy : 99.98\n",
            "Epoch : 130, training loss : 0.7806, training accuracy : 99.97, test loss : 2.6026, test accuracy : 55.87\n",
            "\n",
            "Epoch: 131\n",
            "iteration :  50, loss : 0.7806, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7806, accuracy : 99.97\n",
            "iteration : 150, loss : 0.7806, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7806, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7806, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7806, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7806, accuracy : 99.97\n",
            "Epoch : 131, training loss : 0.7806, training accuracy : 99.97, test loss : 2.5985, test accuracy : 55.93\n",
            "\n",
            "Epoch: 132\n",
            "iteration :  50, loss : 0.7806, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7806, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7806, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7806, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7805, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7805, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7806, accuracy : 99.98\n",
            "Epoch : 132, training loss : 0.7806, training accuracy : 99.97, test loss : 2.6009, test accuracy : 55.87\n",
            "\n",
            "Epoch: 133\n",
            "iteration :  50, loss : 0.7806, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7806, accuracy : 99.96\n",
            "iteration : 150, loss : 0.7807, accuracy : 99.95\n",
            "iteration : 200, loss : 0.7806, accuracy : 99.96\n",
            "iteration : 250, loss : 0.7806, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7806, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7806, accuracy : 99.97\n",
            "Epoch : 133, training loss : 0.7806, training accuracy : 99.97, test loss : 2.6014, test accuracy : 55.68\n",
            "\n",
            "Epoch: 134\n",
            "iteration :  50, loss : 0.7808, accuracy : 99.95\n",
            "iteration : 100, loss : 0.7807, accuracy : 99.97\n",
            "iteration : 150, loss : 0.7806, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7806, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7805, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7805, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7806, accuracy : 99.97\n",
            "Epoch : 134, training loss : 0.7806, training accuracy : 99.97, test loss : 2.5999, test accuracy : 56.05\n",
            "\n",
            "Epoch: 135\n",
            "iteration :  50, loss : 0.7804, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7805, accuracy : 99.97\n",
            "iteration : 150, loss : 0.7806, accuracy : 99.96\n",
            "iteration : 200, loss : 0.7806, accuracy : 99.96\n",
            "iteration : 250, loss : 0.7806, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7806, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7806, accuracy : 99.97\n",
            "Epoch : 135, training loss : 0.7806, training accuracy : 99.97, test loss : 2.5945, test accuracy : 55.88\n",
            "\n",
            "Epoch: 136\n",
            "iteration :  50, loss : 0.7808, accuracy : 99.95\n",
            "iteration : 100, loss : 0.7807, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7806, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7806, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7806, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7806, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7805, accuracy : 99.97\n",
            "Epoch : 136, training loss : 0.7805, training accuracy : 99.97, test loss : 2.6046, test accuracy : 55.72\n",
            "\n",
            "Epoch: 137\n",
            "iteration :  50, loss : 0.7806, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7806, accuracy : 99.97\n",
            "iteration : 150, loss : 0.7806, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7805, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7806, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7805, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7805, accuracy : 99.97\n",
            "Epoch : 137, training loss : 0.7805, training accuracy : 99.97, test loss : 2.6036, test accuracy : 56.01\n",
            "\n",
            "Epoch: 138\n",
            "iteration :  50, loss : 0.7808, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7806, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7806, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7805, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7805, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7805, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7805, accuracy : 99.98\n",
            "Epoch : 138, training loss : 0.7805, training accuracy : 99.98, test loss : 2.6053, test accuracy : 55.69\n",
            "\n",
            "Epoch: 139\n",
            "iteration :  50, loss : 0.7804, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7804, accuracy : 99.97\n",
            "iteration : 150, loss : 0.7804, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7805, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7804, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7804, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7804, accuracy : 99.98\n",
            "Epoch : 139, training loss : 0.7805, training accuracy : 99.97, test loss : 2.6000, test accuracy : 55.56\n",
            "\n",
            "Epoch: 140\n",
            "iteration :  50, loss : 0.7807, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7805, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7806, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7806, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7805, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7805, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7805, accuracy : 99.97\n",
            "Epoch : 140, training loss : 0.7805, training accuracy : 99.97, test loss : 2.5981, test accuracy : 55.94\n",
            "\n",
            "Epoch: 141\n",
            "iteration :  50, loss : 0.7806, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7805, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7804, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7805, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7805, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7804, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7804, accuracy : 99.98\n",
            "Epoch : 141, training loss : 0.7805, training accuracy : 99.98, test loss : 2.6048, test accuracy : 55.71\n",
            "\n",
            "Epoch: 142\n",
            "iteration :  50, loss : 0.7806, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7804, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7804, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7804, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7805, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7804, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7805, accuracy : 99.98\n",
            "Epoch : 142, training loss : 0.7805, training accuracy : 99.97, test loss : 2.6043, test accuracy : 55.51\n",
            "\n",
            "Epoch: 143\n",
            "iteration :  50, loss : 0.7805, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7805, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7804, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7804, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7805, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7805, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7805, accuracy : 99.98\n",
            "Epoch : 143, training loss : 0.7805, training accuracy : 99.97, test loss : 2.6037, test accuracy : 55.65\n",
            "\n",
            "Epoch: 144\n",
            "iteration :  50, loss : 0.7803, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7803, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7803, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7804, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7804, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7805, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7804, accuracy : 99.98\n",
            "Epoch : 144, training loss : 0.7804, training accuracy : 99.98, test loss : 2.6007, test accuracy : 55.72\n",
            "\n",
            "Epoch: 145\n",
            "iteration :  50, loss : 0.7802, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7803, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7804, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7803, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7804, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7804, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7804, accuracy : 99.97\n",
            "Epoch : 145, training loss : 0.7804, training accuracy : 99.97, test loss : 2.6037, test accuracy : 55.76\n",
            "\n",
            "Epoch: 146\n",
            "iteration :  50, loss : 0.7807, accuracy : 99.95\n",
            "iteration : 100, loss : 0.7804, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7804, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7804, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7804, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7804, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7804, accuracy : 99.97\n",
            "Epoch : 146, training loss : 0.7804, training accuracy : 99.97, test loss : 2.6065, test accuracy : 55.58\n",
            "\n",
            "Epoch: 147\n",
            "iteration :  50, loss : 0.7803, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7802, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7803, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7803, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7803, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7803, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7804, accuracy : 99.98\n",
            "Epoch : 147, training loss : 0.7804, training accuracy : 99.97, test loss : 2.6090, test accuracy : 55.68\n",
            "\n",
            "Epoch: 148\n",
            "iteration :  50, loss : 0.7805, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7805, accuracy : 99.97\n",
            "iteration : 150, loss : 0.7804, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7804, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7804, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7804, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7804, accuracy : 99.98\n",
            "Epoch : 148, training loss : 0.7804, training accuracy : 99.97, test loss : 2.6002, test accuracy : 55.67\n",
            "\n",
            "Epoch: 149\n",
            "iteration :  50, loss : 0.7806, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7805, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7805, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7804, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7804, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7804, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7804, accuracy : 99.98\n",
            "Epoch : 149, training loss : 0.7804, training accuracy : 99.98, test loss : 2.6071, test accuracy : 55.66\n",
            "\n",
            "Epoch: 150\n",
            "iteration :  50, loss : 0.7803, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7804, accuracy : 99.97\n",
            "iteration : 150, loss : 0.7804, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7804, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7803, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7803, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7803, accuracy : 99.98\n",
            "Epoch : 150, training loss : 0.7804, training accuracy : 99.97, test loss : 2.6114, test accuracy : 55.64\n",
            "\n",
            "Epoch: 151\n",
            "iteration :  50, loss : 0.7802, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7802, accuracy : 99.99\n",
            "iteration : 150, loss : 0.7802, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7802, accuracy : 100.00\n",
            "iteration : 250, loss : 0.7802, accuracy : 99.99\n",
            "iteration : 300, loss : 0.7803, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7803, accuracy : 99.98\n",
            "Epoch : 151, training loss : 0.7803, training accuracy : 99.98, test loss : 2.6122, test accuracy : 55.64\n",
            "\n",
            "Epoch: 152\n",
            "iteration :  50, loss : 0.7802, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7802, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7802, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7803, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7803, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7803, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7803, accuracy : 99.98\n",
            "Epoch : 152, training loss : 0.7804, training accuracy : 99.97, test loss : 2.6074, test accuracy : 55.70\n",
            "\n",
            "Epoch: 153\n",
            "iteration :  50, loss : 0.7803, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7803, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7803, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7804, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7804, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7804, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7804, accuracy : 99.97\n",
            "Epoch : 153, training loss : 0.7803, training accuracy : 99.97, test loss : 2.6042, test accuracy : 55.80\n",
            "\n",
            "Epoch: 154\n",
            "iteration :  50, loss : 0.7802, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7803, accuracy : 99.97\n",
            "iteration : 150, loss : 0.7803, accuracy : 99.96\n",
            "iteration : 200, loss : 0.7804, accuracy : 99.96\n",
            "iteration : 250, loss : 0.7803, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7803, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7803, accuracy : 99.96\n",
            "Epoch : 154, training loss : 0.7803, training accuracy : 99.97, test loss : 2.6066, test accuracy : 55.60\n",
            "\n",
            "Epoch: 155\n",
            "iteration :  50, loss : 0.7803, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7803, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7803, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7803, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7803, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7803, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7803, accuracy : 99.98\n",
            "Epoch : 155, training loss : 0.7803, training accuracy : 99.98, test loss : 2.6023, test accuracy : 55.52\n",
            "\n",
            "Epoch: 156\n",
            "iteration :  50, loss : 0.7804, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7803, accuracy : 99.97\n",
            "iteration : 150, loss : 0.7802, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7803, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7802, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7803, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7803, accuracy : 99.97\n",
            "Epoch : 156, training loss : 0.7803, training accuracy : 99.97, test loss : 2.6101, test accuracy : 55.88\n",
            "\n",
            "Epoch: 157\n",
            "iteration :  50, loss : 0.7802, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7802, accuracy : 99.99\n",
            "iteration : 150, loss : 0.7802, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7803, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7802, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7803, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7803, accuracy : 99.98\n",
            "Epoch : 157, training loss : 0.7803, training accuracy : 99.97, test loss : 2.6099, test accuracy : 55.71\n",
            "\n",
            "Epoch: 158\n",
            "iteration :  50, loss : 0.7802, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7801, accuracy : 99.99\n",
            "iteration : 150, loss : 0.7801, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7801, accuracy : 99.99\n",
            "iteration : 250, loss : 0.7801, accuracy : 99.99\n",
            "iteration : 300, loss : 0.7802, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7802, accuracy : 99.98\n",
            "Epoch : 158, training loss : 0.7803, training accuracy : 99.97, test loss : 2.6065, test accuracy : 55.66\n",
            "\n",
            "Epoch: 159\n",
            "iteration :  50, loss : 0.7801, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7800, accuracy : 100.00\n",
            "iteration : 150, loss : 0.7801, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7801, accuracy : 99.99\n",
            "iteration : 250, loss : 0.7803, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7803, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7802, accuracy : 99.98\n",
            "Epoch : 159, training loss : 0.7802, training accuracy : 99.98, test loss : 2.6081, test accuracy : 55.69\n",
            "\n",
            "Epoch: 160\n",
            "iteration :  50, loss : 0.7803, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7803, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7803, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7803, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7803, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7802, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7803, accuracy : 99.97\n",
            "Epoch : 160, training loss : 0.7802, training accuracy : 99.97, test loss : 2.6101, test accuracy : 55.52\n",
            "\n",
            "Epoch: 161\n",
            "iteration :  50, loss : 0.7804, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7803, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7802, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7802, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7802, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7802, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7802, accuracy : 99.98\n",
            "Epoch : 161, training loss : 0.7802, training accuracy : 99.97, test loss : 2.6134, test accuracy : 55.46\n",
            "\n",
            "Epoch: 162\n",
            "iteration :  50, loss : 0.7803, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7802, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7802, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7802, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7802, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7802, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7802, accuracy : 99.98\n",
            "Epoch : 162, training loss : 0.7802, training accuracy : 99.98, test loss : 2.6078, test accuracy : 55.61\n",
            "\n",
            "Epoch: 163\n",
            "iteration :  50, loss : 0.7802, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7802, accuracy : 99.97\n",
            "iteration : 150, loss : 0.7802, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7802, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7803, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7802, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7802, accuracy : 99.97\n",
            "Epoch : 163, training loss : 0.7802, training accuracy : 99.97, test loss : 2.6148, test accuracy : 55.54\n",
            "\n",
            "Epoch: 164\n",
            "iteration :  50, loss : 0.7800, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7801, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7802, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7801, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7802, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7802, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7802, accuracy : 99.97\n",
            "Epoch : 164, training loss : 0.7802, training accuracy : 99.97, test loss : 2.6103, test accuracy : 55.33\n",
            "\n",
            "Epoch: 165\n",
            "iteration :  50, loss : 0.7804, accuracy : 99.95\n",
            "iteration : 100, loss : 0.7803, accuracy : 99.96\n",
            "iteration : 150, loss : 0.7802, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7802, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7802, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7802, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7802, accuracy : 99.97\n",
            "Epoch : 165, training loss : 0.7802, training accuracy : 99.97, test loss : 2.6125, test accuracy : 55.64\n",
            "\n",
            "Epoch: 166\n",
            "iteration :  50, loss : 0.7801, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7802, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7802, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7802, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7802, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7801, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7802, accuracy : 99.97\n",
            "Epoch : 166, training loss : 0.7802, training accuracy : 99.97, test loss : 2.6103, test accuracy : 55.30\n",
            "\n",
            "Epoch: 167\n",
            "iteration :  50, loss : 0.7801, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7800, accuracy : 100.00\n",
            "iteration : 150, loss : 0.7800, accuracy : 100.00\n",
            "iteration : 200, loss : 0.7801, accuracy : 100.00\n",
            "iteration : 250, loss : 0.7801, accuracy : 99.99\n",
            "iteration : 300, loss : 0.7801, accuracy : 99.99\n",
            "iteration : 350, loss : 0.7801, accuracy : 99.99\n",
            "Epoch : 167, training loss : 0.7802, training accuracy : 99.98, test loss : 2.6112, test accuracy : 55.65\n",
            "\n",
            "Epoch: 168\n",
            "iteration :  50, loss : 0.7800, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7802, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7802, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7801, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7801, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7801, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7801, accuracy : 99.98\n",
            "Epoch : 168, training loss : 0.7801, training accuracy : 99.98, test loss : 2.6112, test accuracy : 55.42\n",
            "\n",
            "Epoch: 169\n",
            "iteration :  50, loss : 0.7803, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7802, accuracy : 99.97\n",
            "iteration : 150, loss : 0.7802, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7801, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7801, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7801, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7802, accuracy : 99.97\n",
            "Epoch : 169, training loss : 0.7802, training accuracy : 99.97, test loss : 2.6105, test accuracy : 55.60\n",
            "\n",
            "Epoch: 170\n",
            "iteration :  50, loss : 0.7803, accuracy : 99.95\n",
            "iteration : 100, loss : 0.7802, accuracy : 99.96\n",
            "iteration : 150, loss : 0.7803, accuracy : 99.95\n",
            "iteration : 200, loss : 0.7802, accuracy : 99.96\n",
            "iteration : 250, loss : 0.7802, accuracy : 99.96\n",
            "iteration : 300, loss : 0.7802, accuracy : 99.96\n",
            "iteration : 350, loss : 0.7802, accuracy : 99.96\n",
            "Epoch : 170, training loss : 0.7802, training accuracy : 99.97, test loss : 2.6128, test accuracy : 55.45\n",
            "\n",
            "Epoch: 171\n",
            "iteration :  50, loss : 0.7802, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7802, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7802, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7801, accuracy : 99.99\n",
            "iteration : 250, loss : 0.7801, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7802, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7802, accuracy : 99.98\n",
            "Epoch : 171, training loss : 0.7801, training accuracy : 99.98, test loss : 2.6121, test accuracy : 55.50\n",
            "\n",
            "Epoch: 172\n",
            "iteration :  50, loss : 0.7801, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7801, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7801, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7801, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7801, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7801, accuracy : 99.97\n",
            "Epoch : 172, training loss : 0.7801, training accuracy : 99.97, test loss : 2.6128, test accuracy : 55.68\n",
            "\n",
            "Epoch: 173\n",
            "iteration :  50, loss : 0.7803, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7802, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7802, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7802, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7802, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7802, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7801, accuracy : 99.97\n",
            "Epoch : 173, training loss : 0.7802, training accuracy : 99.97, test loss : 2.6135, test accuracy : 55.38\n",
            "\n",
            "Epoch: 174\n",
            "iteration :  50, loss : 0.7799, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7801, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7801, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7800, accuracy : 99.99\n",
            "iteration : 300, loss : 0.7801, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7801, accuracy : 99.98\n",
            "Epoch : 174, training loss : 0.7801, training accuracy : 99.98, test loss : 2.6206, test accuracy : 55.35\n",
            "\n",
            "Epoch: 175\n",
            "iteration :  50, loss : 0.7799, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7800, accuracy : 99.99\n",
            "iteration : 150, loss : 0.7801, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7801, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7801, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7802, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7801, accuracy : 99.98\n",
            "Epoch : 175, training loss : 0.7801, training accuracy : 99.98, test loss : 2.6164, test accuracy : 55.61\n",
            "\n",
            "Epoch: 176\n",
            "iteration :  50, loss : 0.7800, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7800, accuracy : 99.97\n",
            "iteration : 150, loss : 0.7801, accuracy : 99.96\n",
            "iteration : 200, loss : 0.7802, accuracy : 99.96\n",
            "iteration : 250, loss : 0.7801, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7801, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7801, accuracy : 99.97\n",
            "Epoch : 176, training loss : 0.7801, training accuracy : 99.97, test loss : 2.6173, test accuracy : 55.55\n",
            "\n",
            "Epoch: 177\n",
            "iteration :  50, loss : 0.7800, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7801, accuracy : 99.99\n",
            "iteration : 150, loss : 0.7800, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7801, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7801, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7801, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7801, accuracy : 99.98\n",
            "Epoch : 177, training loss : 0.7801, training accuracy : 99.98, test loss : 2.6156, test accuracy : 55.41\n",
            "\n",
            "Epoch: 178\n",
            "iteration :  50, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7800, accuracy : 99.99\n",
            "iteration : 250, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7801, accuracy : 99.98\n",
            "Epoch : 178, training loss : 0.7801, training accuracy : 99.98, test loss : 2.6198, test accuracy : 55.30\n",
            "\n",
            "Epoch: 179\n",
            "iteration :  50, loss : 0.7801, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7800, accuracy : 99.99\n",
            "iteration : 150, loss : 0.7800, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7800, accuracy : 99.98\n",
            "Epoch : 179, training loss : 0.7801, training accuracy : 99.97, test loss : 2.6172, test accuracy : 55.50\n",
            "\n",
            "Epoch: 180\n",
            "iteration :  50, loss : 0.7802, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7801, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7800, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7801, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7801, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7801, accuracy : 99.98\n",
            "Epoch : 180, training loss : 0.7801, training accuracy : 99.98, test loss : 2.6151, test accuracy : 55.55\n",
            "\n",
            "Epoch: 181\n",
            "iteration :  50, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7800, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7800, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7801, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7801, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7800, accuracy : 99.97\n",
            "Epoch : 181, training loss : 0.7800, training accuracy : 99.97, test loss : 2.6176, test accuracy : 55.46\n",
            "\n",
            "Epoch: 182\n",
            "iteration :  50, loss : 0.7801, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7802, accuracy : 99.96\n",
            "iteration : 150, loss : 0.7801, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7801, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7800, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7800, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7800, accuracy : 99.98\n",
            "Epoch : 182, training loss : 0.7800, training accuracy : 99.97, test loss : 2.6162, test accuracy : 55.53\n",
            "\n",
            "Epoch: 183\n",
            "iteration :  50, loss : 0.7799, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7799, accuracy : 100.00\n",
            "iteration : 150, loss : 0.7800, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7801, accuracy : 99.98\n",
            "Epoch : 183, training loss : 0.7801, training accuracy : 99.97, test loss : 2.6189, test accuracy : 55.37\n",
            "\n",
            "Epoch: 184\n",
            "iteration :  50, loss : 0.7799, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7800, accuracy : 99.98\n",
            "Epoch : 184, training loss : 0.7800, training accuracy : 99.98, test loss : 2.6182, test accuracy : 55.47\n",
            "\n",
            "Epoch: 185\n",
            "iteration :  50, loss : 0.7799, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7799, accuracy : 100.00\n",
            "iteration : 150, loss : 0.7798, accuracy : 100.00\n",
            "iteration : 200, loss : 0.7799, accuracy : 100.00\n",
            "iteration : 250, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7800, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7800, accuracy : 99.98\n",
            "Epoch : 185, training loss : 0.7800, training accuracy : 99.97, test loss : 2.6183, test accuracy : 55.37\n",
            "\n",
            "Epoch: 186\n",
            "iteration :  50, loss : 0.7803, accuracy : 99.94\n",
            "iteration : 100, loss : 0.7801, accuracy : 99.97\n",
            "iteration : 150, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7800, accuracy : 99.98\n",
            "Epoch : 186, training loss : 0.7800, training accuracy : 99.97, test loss : 2.6204, test accuracy : 55.43\n",
            "\n",
            "Epoch: 187\n",
            "iteration :  50, loss : 0.7801, accuracy : 99.95\n",
            "iteration : 100, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7801, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7800, accuracy : 99.97\n",
            "Epoch : 187, training loss : 0.7800, training accuracy : 99.98, test loss : 2.6159, test accuracy : 55.33\n",
            "\n",
            "Epoch: 188\n",
            "iteration :  50, loss : 0.7799, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7799, accuracy : 100.00\n",
            "iteration : 150, loss : 0.7800, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7800, accuracy : 99.97\n",
            "Epoch : 188, training loss : 0.7800, training accuracy : 99.97, test loss : 2.6190, test accuracy : 55.40\n",
            "\n",
            "Epoch: 189\n",
            "iteration :  50, loss : 0.7800, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7800, accuracy : 99.98\n",
            "Epoch : 189, training loss : 0.7800, training accuracy : 99.98, test loss : 2.6169, test accuracy : 55.39\n",
            "\n",
            "Epoch: 190\n",
            "iteration :  50, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7799, accuracy : 99.99\n",
            "iteration : 150, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7799, accuracy : 99.99\n",
            "iteration : 250, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7800, accuracy : 99.98\n",
            "Epoch : 190, training loss : 0.7800, training accuracy : 99.97, test loss : 2.6235, test accuracy : 55.26\n",
            "\n",
            "Epoch: 191\n",
            "iteration :  50, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7801, accuracy : 99.97\n",
            "iteration : 150, loss : 0.7801, accuracy : 99.96\n",
            "iteration : 200, loss : 0.7801, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7801, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7800, accuracy : 99.97\n",
            "Epoch : 191, training loss : 0.7800, training accuracy : 99.97, test loss : 2.6161, test accuracy : 55.39\n",
            "\n",
            "Epoch: 192\n",
            "iteration :  50, loss : 0.7798, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7800, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7800, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7800, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7800, accuracy : 99.97\n",
            "Epoch : 192, training loss : 0.7800, training accuracy : 99.97, test loss : 2.6165, test accuracy : 55.42\n",
            "\n",
            "Epoch: 193\n",
            "iteration :  50, loss : 0.7802, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7800, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7800, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7800, accuracy : 99.98\n",
            "Epoch : 193, training loss : 0.7800, training accuracy : 99.98, test loss : 2.6208, test accuracy : 55.32\n",
            "\n",
            "Epoch: 194\n",
            "iteration :  50, loss : 0.7801, accuracy : 99.95\n",
            "iteration : 100, loss : 0.7800, accuracy : 99.96\n",
            "iteration : 150, loss : 0.7800, accuracy : 99.96\n",
            "iteration : 200, loss : 0.7800, accuracy : 99.96\n",
            "iteration : 250, loss : 0.7800, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7799, accuracy : 99.98\n",
            "Epoch : 194, training loss : 0.7800, training accuracy : 99.97, test loss : 2.6177, test accuracy : 55.41\n",
            "\n",
            "Epoch: 195\n",
            "iteration :  50, loss : 0.7798, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7800, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7799, accuracy : 99.98\n",
            "Epoch : 195, training loss : 0.7800, training accuracy : 99.97, test loss : 2.6209, test accuracy : 55.17\n",
            "\n",
            "Epoch: 196\n",
            "iteration :  50, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7800, accuracy : 99.97\n",
            "iteration : 150, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7800, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7800, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7800, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7800, accuracy : 99.97\n",
            "Epoch : 196, training loss : 0.7800, training accuracy : 99.97, test loss : 2.6161, test accuracy : 55.44\n",
            "\n",
            "Epoch: 197\n",
            "iteration :  50, loss : 0.7801, accuracy : 99.95\n",
            "iteration : 100, loss : 0.7800, accuracy : 99.96\n",
            "iteration : 150, loss : 0.7800, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7800, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7799, accuracy : 99.97\n",
            "Epoch : 197, training loss : 0.7799, training accuracy : 99.97, test loss : 2.6183, test accuracy : 55.54\n",
            "\n",
            "Epoch: 198\n",
            "iteration :  50, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7798, accuracy : 99.99\n",
            "iteration : 150, loss : 0.7799, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7800, accuracy : 99.99\n",
            "iteration : 250, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7800, accuracy : 99.98\n",
            "Epoch : 198, training loss : 0.7800, training accuracy : 99.98, test loss : 2.6224, test accuracy : 55.38\n",
            "\n",
            "Epoch: 199\n",
            "iteration :  50, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7799, accuracy : 99.98\n",
            "Epoch : 199, training loss : 0.7800, training accuracy : 99.97, test loss : 2.6222, test accuracy : 55.39\n",
            "\n",
            "Epoch: 200\n",
            "iteration :  50, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7799, accuracy : 99.99\n",
            "iteration : 150, loss : 0.7800, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7799, accuracy : 99.98\n",
            "Epoch : 200, training loss : 0.7799, training accuracy : 99.98, test loss : 2.6176, test accuracy : 55.42\n",
            "\n",
            "Epoch: 201\n",
            "iteration :  50, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7800, accuracy : 99.96\n",
            "iteration : 200, loss : 0.7800, accuracy : 99.96\n",
            "iteration : 250, loss : 0.7800, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7800, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7800, accuracy : 99.97\n",
            "Epoch : 201, training loss : 0.7799, training accuracy : 99.97, test loss : 2.6238, test accuracy : 55.49\n",
            "\n",
            "Epoch: 202\n",
            "iteration :  50, loss : 0.7798, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7797, accuracy : 100.00\n",
            "iteration : 150, loss : 0.7798, accuracy : 100.00\n",
            "iteration : 200, loss : 0.7798, accuracy : 100.00\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.99\n",
            "iteration : 300, loss : 0.7799, accuracy : 99.99\n",
            "iteration : 350, loss : 0.7799, accuracy : 99.98\n",
            "Epoch : 202, training loss : 0.7799, training accuracy : 99.98, test loss : 2.6267, test accuracy : 55.25\n",
            "\n",
            "Epoch: 203\n",
            "iteration :  50, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7800, accuracy : 99.96\n",
            "iteration : 250, loss : 0.7800, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7800, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7799, accuracy : 99.97\n",
            "Epoch : 203, training loss : 0.7799, training accuracy : 99.97, test loss : 2.6219, test accuracy : 55.22\n",
            "\n",
            "Epoch: 204\n",
            "iteration :  50, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7800, accuracy : 99.98\n",
            "Epoch : 204, training loss : 0.7799, training accuracy : 99.98, test loss : 2.6194, test accuracy : 55.33\n",
            "\n",
            "Epoch: 205\n",
            "iteration :  50, loss : 0.7800, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7800, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7800, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7800, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7799, accuracy : 99.98\n",
            "Epoch : 205, training loss : 0.7799, training accuracy : 99.98, test loss : 2.6223, test accuracy : 55.33\n",
            "\n",
            "Epoch: 206\n",
            "iteration :  50, loss : 0.7802, accuracy : 99.95\n",
            "iteration : 100, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7800, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7800, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7800, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7800, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7799, accuracy : 99.97\n",
            "Epoch : 206, training loss : 0.7799, training accuracy : 99.97, test loss : 2.6183, test accuracy : 55.43\n",
            "\n",
            "Epoch: 207\n",
            "iteration :  50, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7799, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7799, accuracy : 99.97\n",
            "Epoch : 207, training loss : 0.7799, training accuracy : 99.97, test loss : 2.6206, test accuracy : 55.56\n",
            "\n",
            "Epoch: 208\n",
            "iteration :  50, loss : 0.7802, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7799, accuracy : 99.98\n",
            "Epoch : 208, training loss : 0.7799, training accuracy : 99.98, test loss : 2.6184, test accuracy : 55.63\n",
            "\n",
            "Epoch: 209\n",
            "iteration :  50, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7797, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7799, accuracy : 99.97\n",
            "Epoch : 209, training loss : 0.7799, training accuracy : 99.97, test loss : 2.6214, test accuracy : 55.42\n",
            "\n",
            "Epoch: 210\n",
            "iteration :  50, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7799, accuracy : 99.98\n",
            "Epoch : 210, training loss : 0.7799, training accuracy : 99.98, test loss : 2.6195, test accuracy : 55.32\n",
            "\n",
            "Epoch: 211\n",
            "iteration :  50, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.99\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7799, accuracy : 99.97\n",
            "Epoch : 211, training loss : 0.7799, training accuracy : 99.97, test loss : 2.6200, test accuracy : 55.40\n",
            "\n",
            "Epoch: 212\n",
            "iteration :  50, loss : 0.7799, accuracy : 99.95\n",
            "iteration : 100, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 150, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7799, accuracy : 99.97\n",
            "Epoch : 212, training loss : 0.7799, training accuracy : 99.97, test loss : 2.6196, test accuracy : 55.47\n",
            "\n",
            "Epoch: 213\n",
            "iteration :  50, loss : 0.7801, accuracy : 99.94\n",
            "iteration : 100, loss : 0.7800, accuracy : 99.96\n",
            "iteration : 150, loss : 0.7799, accuracy : 99.96\n",
            "iteration : 200, loss : 0.7799, accuracy : 99.96\n",
            "iteration : 250, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 213, training loss : 0.7799, training accuracy : 99.97, test loss : 2.6234, test accuracy : 55.55\n",
            "\n",
            "Epoch: 214\n",
            "iteration :  50, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7799, accuracy : 99.98\n",
            "Epoch : 214, training loss : 0.7799, training accuracy : 99.98, test loss : 2.6231, test accuracy : 55.36\n",
            "\n",
            "Epoch: 215\n",
            "iteration :  50, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7799, accuracy : 99.97\n",
            "Epoch : 215, training loss : 0.7799, training accuracy : 99.97, test loss : 2.6215, test accuracy : 55.45\n",
            "\n",
            "Epoch: 216\n",
            "iteration :  50, loss : 0.7800, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7799, accuracy : 99.97\n",
            "Epoch : 216, training loss : 0.7799, training accuracy : 99.97, test loss : 2.6253, test accuracy : 55.47\n",
            "\n",
            "Epoch: 217\n",
            "iteration :  50, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 217, training loss : 0.7799, training accuracy : 99.97, test loss : 2.6232, test accuracy : 55.34\n",
            "\n",
            "Epoch: 218\n",
            "iteration :  50, loss : 0.7796, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7799, accuracy : 99.98\n",
            "Epoch : 218, training loss : 0.7799, training accuracy : 99.98, test loss : 2.6291, test accuracy : 55.29\n",
            "\n",
            "Epoch: 219\n",
            "iteration :  50, loss : 0.7800, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7799, accuracy : 99.98\n",
            "Epoch : 219, training loss : 0.7799, training accuracy : 99.97, test loss : 2.6233, test accuracy : 55.35\n",
            "\n",
            "Epoch: 220\n",
            "iteration :  50, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7799, accuracy : 99.98\n",
            "Epoch : 220, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6235, test accuracy : 55.33\n",
            "\n",
            "Epoch: 221\n",
            "iteration :  50, loss : 0.7799, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7799, accuracy : 99.97\n",
            "Epoch : 221, training loss : 0.7799, training accuracy : 99.97, test loss : 2.6236, test accuracy : 55.37\n",
            "\n",
            "Epoch: 222\n",
            "iteration :  50, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7797, accuracy : 99.99\n",
            "iteration : 150, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7799, accuracy : 99.97\n",
            "Epoch : 222, training loss : 0.7799, training accuracy : 99.97, test loss : 2.6248, test accuracy : 55.45\n",
            "\n",
            "Epoch: 223\n",
            "iteration :  50, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7799, accuracy : 99.96\n",
            "iteration : 200, loss : 0.7799, accuracy : 99.96\n",
            "iteration : 250, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7799, accuracy : 99.97\n",
            "Epoch : 223, training loss : 0.7799, training accuracy : 99.97, test loss : 2.6254, test accuracy : 55.45\n",
            "\n",
            "Epoch: 224\n",
            "iteration :  50, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7798, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.99\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 224, training loss : 0.7799, training accuracy : 99.98, test loss : 2.6254, test accuracy : 55.48\n",
            "\n",
            "Epoch: 225\n",
            "iteration :  50, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.99\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7799, accuracy : 99.98\n",
            "Epoch : 225, training loss : 0.7799, training accuracy : 99.98, test loss : 2.6240, test accuracy : 55.50\n",
            "\n",
            "Epoch: 226\n",
            "iteration :  50, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7800, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7800, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7800, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7799, accuracy : 99.98\n",
            "Epoch : 226, training loss : 0.7799, training accuracy : 99.98, test loss : 2.6273, test accuracy : 55.32\n",
            "\n",
            "Epoch: 227\n",
            "iteration :  50, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7799, accuracy : 99.97\n",
            "Epoch : 227, training loss : 0.7799, training accuracy : 99.97, test loss : 2.6272, test accuracy : 55.29\n",
            "\n",
            "Epoch: 228\n",
            "iteration :  50, loss : 0.7800, accuracy : 99.95\n",
            "iteration : 100, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7799, accuracy : 99.97\n",
            "Epoch : 228, training loss : 0.7798, training accuracy : 99.97, test loss : 2.6284, test accuracy : 55.28\n",
            "\n",
            "Epoch: 229\n",
            "iteration :  50, loss : 0.7796, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7796, accuracy : 100.00\n",
            "iteration : 150, loss : 0.7797, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.99\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.97\n",
            "Epoch : 229, training loss : 0.7798, training accuracy : 99.97, test loss : 2.6279, test accuracy : 55.39\n",
            "\n",
            "Epoch: 230\n",
            "iteration :  50, loss : 0.7796, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 150, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.97\n",
            "Epoch : 230, training loss : 0.7798, training accuracy : 99.97, test loss : 2.6256, test accuracy : 55.57\n",
            "\n",
            "Epoch: 231\n",
            "iteration :  50, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7797, accuracy : 99.99\n",
            "iteration : 150, loss : 0.7798, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7797, accuracy : 99.99\n",
            "iteration : 250, loss : 0.7797, accuracy : 99.99\n",
            "iteration : 300, loss : 0.7797, accuracy : 99.99\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 231, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6270, test accuracy : 55.28\n",
            "\n",
            "Epoch: 232\n",
            "iteration :  50, loss : 0.7796, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7799, accuracy : 99.98\n",
            "Epoch : 232, training loss : 0.7799, training accuracy : 99.98, test loss : 2.6259, test accuracy : 55.43\n",
            "\n",
            "Epoch: 233\n",
            "iteration :  50, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7798, accuracy : 99.99\n",
            "iteration : 150, loss : 0.7797, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.99\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.99\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 233, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6242, test accuracy : 55.32\n",
            "\n",
            "Epoch: 234\n",
            "iteration :  50, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 234, training loss : 0.7798, training accuracy : 99.97, test loss : 2.6286, test accuracy : 55.28\n",
            "\n",
            "Epoch: 235\n",
            "iteration :  50, loss : 0.7795, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7798, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 235, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6261, test accuracy : 55.34\n",
            "\n",
            "Epoch: 236\n",
            "iteration :  50, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 236, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6233, test accuracy : 55.42\n",
            "\n",
            "Epoch: 237\n",
            "iteration :  50, loss : 0.7800, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.97\n",
            "Epoch : 237, training loss : 0.7798, training accuracy : 99.97, test loss : 2.6257, test accuracy : 55.37\n",
            "\n",
            "Epoch: 238\n",
            "iteration :  50, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.97\n",
            "Epoch : 238, training loss : 0.7798, training accuracy : 99.97, test loss : 2.6249, test accuracy : 55.30\n",
            "\n",
            "Epoch: 239\n",
            "iteration :  50, loss : 0.7796, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7797, accuracy : 100.00\n",
            "iteration : 150, loss : 0.7798, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 239, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6256, test accuracy : 55.35\n",
            "\n",
            "Epoch: 240\n",
            "iteration :  50, loss : 0.7796, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 240, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6260, test accuracy : 55.44\n",
            "\n",
            "Epoch: 241\n",
            "iteration :  50, loss : 0.7798, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7798, accuracy : 99.99\n",
            "iteration : 150, loss : 0.7798, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.99\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.99\n",
            "iteration : 300, loss : 0.7797, accuracy : 99.99\n",
            "iteration : 350, loss : 0.7797, accuracy : 99.99\n",
            "Epoch : 241, training loss : 0.7798, training accuracy : 99.99, test loss : 2.6285, test accuracy : 55.37\n",
            "\n",
            "Epoch: 242\n",
            "iteration :  50, loss : 0.7800, accuracy : 99.95\n",
            "iteration : 100, loss : 0.7800, accuracy : 99.95\n",
            "iteration : 150, loss : 0.7799, accuracy : 99.96\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 242, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6253, test accuracy : 55.33\n",
            "\n",
            "Epoch: 243\n",
            "iteration :  50, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 243, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6276, test accuracy : 55.43\n",
            "\n",
            "Epoch: 244\n",
            "iteration :  50, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7798, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7797, accuracy : 99.99\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 244, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6287, test accuracy : 55.32\n",
            "\n",
            "Epoch: 245\n",
            "iteration :  50, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 150, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 245, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6295, test accuracy : 55.29\n",
            "\n",
            "Epoch: 246\n",
            "iteration :  50, loss : 0.7800, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.97\n",
            "Epoch : 246, training loss : 0.7798, training accuracy : 99.97, test loss : 2.6307, test accuracy : 55.24\n",
            "\n",
            "Epoch: 247\n",
            "iteration :  50, loss : 0.7801, accuracy : 99.92\n",
            "iteration : 100, loss : 0.7800, accuracy : 99.95\n",
            "iteration : 150, loss : 0.7800, accuracy : 99.95\n",
            "iteration : 200, loss : 0.7799, accuracy : 99.95\n",
            "iteration : 250, loss : 0.7799, accuracy : 99.96\n",
            "iteration : 300, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.97\n",
            "Epoch : 247, training loss : 0.7798, training accuracy : 99.97, test loss : 2.6275, test accuracy : 55.41\n",
            "\n",
            "Epoch: 248\n",
            "iteration :  50, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.97\n",
            "Epoch : 248, training loss : 0.7798, training accuracy : 99.97, test loss : 2.6285, test accuracy : 55.19\n",
            "\n",
            "Epoch: 249\n",
            "iteration :  50, loss : 0.7796, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7797, accuracy : 99.99\n",
            "iteration : 150, loss : 0.7797, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 249, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6264, test accuracy : 55.25\n",
            "\n",
            "Epoch: 250\n",
            "iteration :  50, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 150, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.97\n",
            "Epoch : 250, training loss : 0.7798, training accuracy : 99.97, test loss : 2.6272, test accuracy : 55.45\n",
            "\n",
            "Epoch: 251\n",
            "iteration :  50, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.97\n",
            "Epoch : 251, training loss : 0.7798, training accuracy : 99.97, test loss : 2.6305, test accuracy : 55.32\n",
            "\n",
            "Epoch: 252\n",
            "iteration :  50, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.99\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.99\n",
            "iteration : 300, loss : 0.7797, accuracy : 99.99\n",
            "iteration : 350, loss : 0.7797, accuracy : 99.99\n",
            "Epoch : 252, training loss : 0.7798, training accuracy : 99.99, test loss : 2.6272, test accuracy : 55.38\n",
            "\n",
            "Epoch: 253\n",
            "iteration :  50, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7799, accuracy : 99.96\n",
            "iteration : 150, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 253, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6291, test accuracy : 55.43\n",
            "\n",
            "Epoch: 254\n",
            "iteration :  50, loss : 0.7799, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7798, accuracy : 100.00\n",
            "iteration : 150, loss : 0.7797, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7797, accuracy : 100.00\n",
            "iteration : 250, loss : 0.7797, accuracy : 99.99\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.99\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 254, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6258, test accuracy : 55.59\n",
            "\n",
            "Epoch: 255\n",
            "iteration :  50, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 255, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6308, test accuracy : 55.28\n",
            "\n",
            "Epoch: 256\n",
            "iteration :  50, loss : 0.7796, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7796, accuracy : 99.99\n",
            "iteration : 150, loss : 0.7797, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7797, accuracy : 99.98\n",
            "Epoch : 256, training loss : 0.7798, training accuracy : 99.97, test loss : 2.6271, test accuracy : 55.40\n",
            "\n",
            "Epoch: 257\n",
            "iteration :  50, loss : 0.7795, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7797, accuracy : 99.99\n",
            "iteration : 150, loss : 0.7797, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7797, accuracy : 99.99\n",
            "iteration : 250, loss : 0.7797, accuracy : 99.99\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 257, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6280, test accuracy : 55.22\n",
            "\n",
            "Epoch: 258\n",
            "iteration :  50, loss : 0.7801, accuracy : 99.95\n",
            "iteration : 100, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 258, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6287, test accuracy : 55.36\n",
            "\n",
            "Epoch: 259\n",
            "iteration :  50, loss : 0.7800, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7797, accuracy : 99.98\n",
            "Epoch : 259, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6271, test accuracy : 55.27\n",
            "\n",
            "Epoch: 260\n",
            "iteration :  50, loss : 0.7795, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 260, training loss : 0.7798, training accuracy : 99.97, test loss : 2.6309, test accuracy : 55.28\n",
            "\n",
            "Epoch: 261\n",
            "iteration :  50, loss : 0.7796, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 261, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6264, test accuracy : 55.46\n",
            "\n",
            "Epoch: 262\n",
            "iteration :  50, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7798, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.99\n",
            "Epoch : 262, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6263, test accuracy : 55.27\n",
            "\n",
            "Epoch: 263\n",
            "iteration :  50, loss : 0.7796, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 263, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6275, test accuracy : 55.10\n",
            "\n",
            "Epoch: 264\n",
            "iteration :  50, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7797, accuracy : 99.99\n",
            "iteration : 150, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.97\n",
            "Epoch : 264, training loss : 0.7798, training accuracy : 99.97, test loss : 2.6294, test accuracy : 55.42\n",
            "\n",
            "Epoch: 265\n",
            "iteration :  50, loss : 0.7797, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 150, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 265, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6275, test accuracy : 55.45\n",
            "\n",
            "Epoch: 266\n",
            "iteration :  50, loss : 0.7797, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7798, accuracy : 100.00\n",
            "iteration : 150, loss : 0.7797, accuracy : 100.00\n",
            "iteration : 200, loss : 0.7797, accuracy : 100.00\n",
            "iteration : 250, loss : 0.7797, accuracy : 100.00\n",
            "iteration : 300, loss : 0.7797, accuracy : 99.99\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.99\n",
            "Epoch : 266, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6284, test accuracy : 55.34\n",
            "\n",
            "Epoch: 267\n",
            "iteration :  50, loss : 0.7797, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.97\n",
            "Epoch : 267, training loss : 0.7798, training accuracy : 99.97, test loss : 2.6253, test accuracy : 55.50\n",
            "\n",
            "Epoch: 268\n",
            "iteration :  50, loss : 0.7796, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 268, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6267, test accuracy : 55.21\n",
            "\n",
            "Epoch: 269\n",
            "iteration :  50, loss : 0.7795, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7796, accuracy : 100.00\n",
            "iteration : 150, loss : 0.7797, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7797, accuracy : 99.99\n",
            "iteration : 250, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 269, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6264, test accuracy : 55.29\n",
            "\n",
            "Epoch: 270\n",
            "iteration :  50, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7799, accuracy : 99.96\n",
            "iteration : 150, loss : 0.7797, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 270, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6300, test accuracy : 55.24\n",
            "\n",
            "Epoch: 271\n",
            "iteration :  50, loss : 0.7796, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7797, accuracy : 100.00\n",
            "iteration : 150, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 271, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6258, test accuracy : 55.20\n",
            "\n",
            "Epoch: 272\n",
            "iteration :  50, loss : 0.7798, accuracy : 99.95\n",
            "iteration : 100, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 272, training loss : 0.7797, training accuracy : 99.98, test loss : 2.6329, test accuracy : 55.16\n",
            "\n",
            "Epoch: 273\n",
            "iteration :  50, loss : 0.7796, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 150, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 273, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6311, test accuracy : 55.32\n",
            "\n",
            "Epoch: 274\n",
            "iteration :  50, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 150, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 274, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6262, test accuracy : 55.28\n",
            "\n",
            "Epoch: 275\n",
            "iteration :  50, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7797, accuracy : 99.99\n",
            "iteration : 150, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 275, training loss : 0.7798, training accuracy : 99.97, test loss : 2.6269, test accuracy : 55.37\n",
            "\n",
            "Epoch: 276\n",
            "iteration :  50, loss : 0.7796, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.99\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 276, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6313, test accuracy : 55.38\n",
            "\n",
            "Epoch: 277\n",
            "iteration :  50, loss : 0.7797, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7797, accuracy : 99.99\n",
            "iteration : 350, loss : 0.7797, accuracy : 99.99\n",
            "Epoch : 277, training loss : 0.7798, training accuracy : 99.99, test loss : 2.6279, test accuracy : 55.50\n",
            "\n",
            "Epoch: 278\n",
            "iteration :  50, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7797, accuracy : 99.98\n",
            "Epoch : 278, training loss : 0.7797, training accuracy : 99.98, test loss : 2.6284, test accuracy : 55.42\n",
            "\n",
            "Epoch: 279\n",
            "iteration :  50, loss : 0.7797, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7798, accuracy : 99.99\n",
            "iteration : 150, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 279, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6284, test accuracy : 55.26\n",
            "\n",
            "Epoch: 280\n",
            "iteration :  50, loss : 0.7797, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7799, accuracy : 99.95\n",
            "iteration : 150, loss : 0.7798, accuracy : 99.96\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7797, accuracy : 99.98\n",
            "Epoch : 280, training loss : 0.7798, training accuracy : 99.97, test loss : 2.6296, test accuracy : 55.23\n",
            "\n",
            "Epoch: 281\n",
            "iteration :  50, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 150, loss : 0.7797, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7797, accuracy : 99.98\n",
            "Epoch : 281, training loss : 0.7798, training accuracy : 99.99, test loss : 2.6298, test accuracy : 55.34\n",
            "\n",
            "Epoch: 282\n",
            "iteration :  50, loss : 0.7797, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7798, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.99\n",
            "iteration : 250, loss : 0.7797, accuracy : 99.99\n",
            "iteration : 300, loss : 0.7797, accuracy : 99.99\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.99\n",
            "Epoch : 282, training loss : 0.7798, training accuracy : 99.99, test loss : 2.6315, test accuracy : 55.18\n",
            "\n",
            "Epoch: 283\n",
            "iteration :  50, loss : 0.7801, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 283, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6292, test accuracy : 55.42\n",
            "\n",
            "Epoch: 284\n",
            "iteration :  50, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7798, accuracy : 99.99\n",
            "iteration : 150, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7797, accuracy : 99.99\n",
            "Epoch : 284, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6296, test accuracy : 55.33\n",
            "\n",
            "Epoch: 285\n",
            "iteration :  50, loss : 0.7799, accuracy : 99.97\n",
            "iteration : 100, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7797, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7797, accuracy : 99.99\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7797, accuracy : 99.99\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 285, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6279, test accuracy : 55.21\n",
            "\n",
            "Epoch: 286\n",
            "iteration :  50, loss : 0.7796, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 286, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6277, test accuracy : 55.37\n",
            "\n",
            "Epoch: 287\n",
            "iteration :  50, loss : 0.7796, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7796, accuracy : 99.99\n",
            "iteration : 150, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7797, accuracy : 99.98\n",
            "Epoch : 287, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6291, test accuracy : 55.23\n",
            "\n",
            "Epoch: 288\n",
            "iteration :  50, loss : 0.7795, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7796, accuracy : 100.00\n",
            "iteration : 150, loss : 0.7797, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.99\n",
            "iteration : 350, loss : 0.7797, accuracy : 99.99\n",
            "Epoch : 288, training loss : 0.7798, training accuracy : 99.99, test loss : 2.6277, test accuracy : 55.33\n",
            "\n",
            "Epoch: 289\n",
            "iteration :  50, loss : 0.7800, accuracy : 99.95\n",
            "iteration : 100, loss : 0.7799, accuracy : 99.96\n",
            "iteration : 150, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 289, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6309, test accuracy : 55.23\n",
            "\n",
            "Epoch: 290\n",
            "iteration :  50, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7797, accuracy : 99.98\n",
            "Epoch : 290, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6263, test accuracy : 55.35\n",
            "\n",
            "Epoch: 291\n",
            "iteration :  50, loss : 0.7797, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 291, training loss : 0.7798, training accuracy : 99.97, test loss : 2.6281, test accuracy : 55.36\n",
            "\n",
            "Epoch: 292\n",
            "iteration :  50, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7797, accuracy : 99.99\n",
            "iteration : 200, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 292, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6286, test accuracy : 55.35\n",
            "\n",
            "Epoch: 293\n",
            "iteration :  50, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7797, accuracy : 99.98\n",
            "Epoch : 293, training loss : 0.7797, training accuracy : 99.98, test loss : 2.6257, test accuracy : 55.39\n",
            "\n",
            "Epoch: 294\n",
            "iteration :  50, loss : 0.7796, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7796, accuracy : 100.00\n",
            "iteration : 150, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 294, training loss : 0.7797, training accuracy : 99.98, test loss : 2.6298, test accuracy : 55.27\n",
            "\n",
            "Epoch: 295\n",
            "iteration :  50, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 295, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6316, test accuracy : 55.02\n",
            "\n",
            "Epoch: 296\n",
            "iteration :  50, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7797, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 296, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6287, test accuracy : 55.18\n",
            "\n",
            "Epoch: 297\n",
            "iteration :  50, loss : 0.7802, accuracy : 99.91\n",
            "iteration : 100, loss : 0.7799, accuracy : 99.95\n",
            "iteration : 150, loss : 0.7798, accuracy : 99.96\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.96\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.96\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.97\n",
            "iteration : 350, loss : 0.7797, accuracy : 99.97\n",
            "Epoch : 297, training loss : 0.7798, training accuracy : 99.97, test loss : 2.6331, test accuracy : 55.14\n",
            "\n",
            "Epoch: 298\n",
            "iteration :  50, loss : 0.7797, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7797, accuracy : 100.00\n",
            "iteration : 150, loss : 0.7797, accuracy : 100.00\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.99\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.99\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.99\n",
            "iteration : 350, loss : 0.7797, accuracy : 99.99\n",
            "Epoch : 298, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6292, test accuracy : 55.48\n",
            "\n",
            "Epoch: 299\n",
            "iteration :  50, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 100, loss : 0.7799, accuracy : 99.98\n",
            "iteration : 150, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 300, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 350, loss : 0.7798, accuracy : 99.98\n",
            "Epoch : 299, training loss : 0.7798, training accuracy : 99.98, test loss : 2.6271, test accuracy : 55.45\n",
            "\n",
            "Epoch: 300\n",
            "iteration :  50, loss : 0.7797, accuracy : 100.00\n",
            "iteration : 100, loss : 0.7797, accuracy : 99.99\n",
            "iteration : 150, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 200, loss : 0.7798, accuracy : 99.98\n",
            "iteration : 250, loss : 0.7798, accuracy : 99.99\n",
            "iteration : 300, loss : 0.7797, accuracy : 99.99\n",
            "iteration : 350, loss : 0.7797, accuracy : 99.99\n",
            "Epoch : 300, training loss : 0.7798, training accuracy : 99.99, test loss : 2.6283, test accuracy : 55.46\n"
          ]
        }
      ],
      "source": [
        "#------------------------------Main---------------------------------------------\n",
        "config = {\n",
        "    'lr': 0.01,\n",
        "    'momentum': 0.9,\n",
        "}\n",
        "train_loss_ = []\n",
        "train_acc_ = []\n",
        "test_loss_ = []\n",
        "test_acc_ = []\n",
        "#ResNet 34\n",
        "net = ResNet34().to('cuda')\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing = 0.1).to('cuda')\n",
        "optimizer = optim.SGD(net.parameters(), lr=config['lr'],\n",
        "                      momentum=config['momentum'])\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\n",
        "\n",
        "for epoch in range(1, 301):\n",
        "    train_loss, train_acc = train(epoch, net, criterion, trainloader, scheduler)\n",
        "    test_loss, test_acc = test(epoch, net, criterion, testloader)\n",
        "    \n",
        "    print((\"Epoch : %3d, training loss : %0.4f, training accuracy : %2.2f, test loss \" + \\\n",
        "      \": %0.4f, test accuracy : %2.2f\") % (epoch, train_loss, train_acc, test_loss, test_acc))\n",
        "    \n",
        "    train_loss_.append(train_loss)\n",
        "    test_loss_.append(test_loss)\n",
        "    train_acc_.append(train_acc)\n",
        "    test_acc_.append(test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "81l23SSrYG7r",
        "outputId": "452252d7-c93f-4094-8062-5fdb1a0f8e8f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(range(len(train_loss_)), train_loss_, 'b')\n",
        "plt.plot(range(len(test_loss_)), test_loss_, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"ResNet34: Loss vs Number of epochs\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "PgRzBZnLb9oq",
        "outputId": "9b060e50-ae30-4cf5-c49f-0ab051ba34ed"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(range(len(train_acc_)), train_acc_, 'b')\n",
        "plt.plot(range(len(test_acc_)), test_acc_, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"ResNet34: Accuracy vs Number of epochs\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "id": "ArgupDVRwB8i",
        "outputId": "7a6857e2-4b87-48e3-ec4c-82c2da11c1b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 1\n",
            "iteration :  50, loss : 1.9576, accuracy : 27.44\n",
            "iteration : 100, loss : 1.8206, accuracy : 32.25\n",
            "iteration : 150, loss : 1.7145, accuracy : 36.39\n",
            "iteration : 200, loss : 1.6430, accuracy : 39.11\n",
            "iteration : 250, loss : 1.5699, accuracy : 42.02\n",
            "iteration : 300, loss : 1.5136, accuracy : 44.29\n",
            "iteration : 350, loss : 1.4667, accuracy : 46.18\n",
            "epoch :   1, training loss : 1.4241, training accuracy : 47.81, test loss : 1.1884, test accuracy : 58.42\n",
            "\n",
            "Epoch: 2\n",
            "iteration :  50, loss : 1.0251, accuracy : 63.84\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-cc4225c6e06c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-0515b8342e9c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, net, criterion, trainloader, scheduler)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# main body\n",
        "config = {\n",
        "    'lr': 0.01,\n",
        "    'momentum': 0.9,\n",
        "    'weight_decay': 5e-4\n",
        "}\n",
        "\n",
        "\n",
        "net = ResNet18().to('cuda')\n",
        "criterion = nn.CrossEntropyLoss().to('cuda')\n",
        "optimizer = optim.SGD(net.parameters(), lr=config['lr'],\n",
        "                      momentum=config['momentum'], weight_decay=config['weight_decay'])\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "\n",
        "for epoch in range(1, 200):\n",
        "    train_loss, train_acc = train(epoch, net, criterion, trainloader, scheduler)\n",
        "    test_loss, test_acc = test(epoch, net, criterion, testloader)\n",
        "    \n",
        "    print((\"Epoch : %3d, training loss : %0.4f, training accuracy : %2.2f, test loss \" + \\\n",
        "      \": %0.4f, test accuracy : %2.2f\") % (epoch, train_loss, train_acc, test_loss, test_acc))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "5e-4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3dnchRvgsIh",
        "outputId": "192c5271-8d64-4f5c-f047-c09375cc578f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0005"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "1e-2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-7CTXbhgsn3",
        "outputId": "092221a8-4d80-4c48-939f-655dcf5a0aee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.01"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jpkJiS0QguTc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "DL_GroupProject_LabelSmooth.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6c36c4eadda3424abdeacd3783a1c43b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bb2d457ec6784ab0b52dcfc7073696a2",
              "IPY_MODEL_c50aff42b73b4b6d8141c60dc96b7775",
              "IPY_MODEL_fa0a6677abcf4896a98e98cd2e62c949"
            ],
            "layout": "IPY_MODEL_6533f4a89d054c6ab1d3e8d6031e4f17"
          }
        },
        "bb2d457ec6784ab0b52dcfc7073696a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87ff2bb3b61447238bb0ff9a0ea52ba5",
            "placeholder": "",
            "style": "IPY_MODEL_a0641be4884b4074a828c71961bb9101",
            "value": ""
          }
        },
        "c50aff42b73b4b6d8141c60dc96b7775": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9417521d49204099b144c4c9378cc05e",
            "max": 169001437,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3b72d049dfec41c6b2900d5d85213e7e",
            "value": 169001437
          }
        },
        "fa0a6677abcf4896a98e98cd2e62c949": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39fe80cc93f24c6a8d964934f467db92",
            "placeholder": "",
            "style": "IPY_MODEL_7e589638b6cf444e920d0d753a680ca4",
            "value": " 169001984/? [00:08&lt;00:00, 23903051.50it/s]"
          }
        },
        "6533f4a89d054c6ab1d3e8d6031e4f17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87ff2bb3b61447238bb0ff9a0ea52ba5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0641be4884b4074a828c71961bb9101": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9417521d49204099b144c4c9378cc05e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b72d049dfec41c6b2900d5d85213e7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "39fe80cc93f24c6a8d964934f467db92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e589638b6cf444e920d0d753a680ca4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}